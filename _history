{"entries":[{"timestamp":1740253156124,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.blocks","patch":[{"diffs":[[0,"http"],[-1,"s"],[0,"://"],[-1,"developers.google.com/blockly/xml\"><variables></variables>"],[1,"www.w3.org/1999/xhtml\">\n  "],[0,"<blo"]],"start1":12,"start2":12,"length1":70,"length2":37},{"diffs":[[0,"art\""],[-1," x=\"20\" y=\"20\""],[0,"></b"]],"start1":67,"start2":67,"length1":22,"length2":8},{"diffs":[[0,"</block>"],[1,"\n  "],[0,"<block t"]],"start1":72,"start2":72,"length1":16,"length2":19},{"diffs":[[0,"ver\""],[-1," x=\"225\" y=\"20\""],[0,"></b"]],"start1":107,"start2":107,"length1":23,"length2":8},{"diffs":[[0,"</block>"],[1,"\n"],[0,"</xml>"]],"start1":112,"start2":112,"length1":14,"length2":15}]},{"type":"edited","filename":"pxt.json","patch":[{"diffs":[[0,".md\""],[-1,",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\""],[1,"\n    ],\n    \"additionalFilePaths\": []"],[0,"\n}\n"]],"start1":221,"start2":221,"length1":64,"length2":44}]},{"type":"added","filename":"main.py","value":"from microbit import *\nimport Math  # Use Micro:bit's Math module for randomness\n\n# Maze representation (1 = wall, 0 = path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Player start position\nplayer_x, player_y = 1, 1\ngoal_x, goal_y = 3, 4\n\n# Learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 0.3  # Exploration chance\n\n# Possible moves (Up, Down, Left, Right)\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Function to choose an action\ndef choose_action():\n    if Math.random() < epsilon:  # Random move for exploration\n        action_index = int(Math.random() * 4)  # Pick a random index between 0-3\n        return action_index\n    else:\n        return 0  # Placeholder for now, can be improved with learned Q-values\n\n# Training loop\nfor episode in range(10):  # Run multiple learning episodes\n    player_x, player_y = 1, 1  # Reset position\n    while (player_x, player_y) != (goal_x, goal_y):  # Until goal is reached\n        action = choose_action()\n        new_x = player_x + move_x[action]\n        new_y = player_y + move_y[action]\n\n        # Check if the move is valid\n        if maze[new_y][new_x] == 1:  # Hit a wall\n            reward = -5\n        elif (new_x, new_y) == (goal_x, goal_y):  # Reached goal\n            reward = 10\n        else:\n            reward = -1  # Normal movement\n\n        # Move the player only if it's a valid move\n        if maze[new_y][new_x] == 0:\n            player_x, player_y = new_x, new_y\n\n        # Update display to show movement\n        display.clear()\n        display.set_pixel(player_x, player_y, 9)  # Show player position\n        sleep(500)\n\n# Display success message when done\ndisplay.show(Image.HAPPY)\n"}]},{"timestamp":1740253532048,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"let action: number;\nlet new_x: number;\nlet new_y: number;\nlet reward: number;\n//  Use Micro:bit's Math module for randomness\n//  Maze representation (1 = wall, 0 = path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Player start position\nlet [player_x, player_y] = [1, 1]\nlet [goal_x, goal_y] = [3, 4]\n//  Learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 0.3\n//  Exploration chance\n//  Possible moves (Up, Down, Left, Right)\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\n//  Function to choose an action\nfunction choose_action(): number {\n    let action_index: number;\n    if (Math.random() < epsilon) {\n        //  Random move for exploration\n        action_index = Math.trunc(Math.random() * 4)\n        //  Pick a random index between 0-3\n        return action_index\n    } else {\n        return 0\n    }\n    \n}\n\n//  Placeholder for now, can be improved with learned Q-values\n//  Training loop\nfor (let episode = 0; episode < 10; episode++) {\n    //  Run multiple learning episodes\n    let [player_x, player_y] = [1, 1]\n    //  Reset position\n    while ([player_x, player_y] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        action = choose_action()\n        new_x = player_x + move_x[action]\n        new_y = player_y + move_y[action]\n        //  Check if the move is valid\n        if (maze[new_y][new_x] == 1) {\n            //  Hit a wall\n            reward = -5\n        } else if ([new_x, new_y] == [goal_x, goal_y]) {\n            //  Reached goal\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        //  Normal movement\n        //  Move the player only if it's a valid move\n        if (maze[new_y][new_x] == 0) {\n            let [player_x, player_y] = [new_x, new_y]\n        }\n        \n        //  Update display to show movement\n        basic.clearScreen()\n        //  Clear previous display\n        led.plot(player_x, player_y)\n        //  Show player position\n        basic.pause(500)\n    }\n}\n//  Wait for a short time\n//  Display success message when done\nbasic.showIcon(IconNames.Happy)"],[0,"\n"]],"start1":0,"start2":0,"length1":2166,"length2":1}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"basic.clear_screen"],[1,"display.clear"],[0,"()"],[1,"\n"],[0,"  "],[-1,"# Clear previous display\n        led.plot"],[1,"      display.set_pixel"],[0,"(pla"]],"start1":1593,"start2":1593,"length1":71,"length2":49},{"diffs":[[0,"player_y"],[1,", 9"],[0,")  # Sho"]],"start1":1649,"start2":1649,"length1":16,"length2":19},{"diffs":[[0,"    "],[-1,"basic.pause(500)  # Wait for a short time"],[1,"sleep(500)"],[0,"\n\n# "]],"start1":1690,"start2":1690,"length1":49,"length2":18},{"diffs":[[0,"one\n"],[-1,"basic.show_icon(IconNames"],[1,"display.show(Image"],[0,".HAPPY)"],[1,"\n"]],"start1":1738,"start2":1738,"length1":36,"length2":30}]}]},{"timestamp":1740253793201,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,", 1]\n]\n\n"],[-1,"\n"],[0,"# Player"]],"start1":233,"start2":233,"length1":17,"length2":16}]}]},{"timestamp":1740261439364,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,", 1]\n]\n\n"],[-1,"di"],[0,"\n# Playe"]],"start1":233,"start2":233,"length1":18,"length2":16}]}]},{"timestamp":1740261440431,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\n\ndi"],[-1,"splay maze"],[0,"\n# P"]],"start1":239,"start2":239,"length1":18,"length2":8}]}]},{"timestamp":1740261451895,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,", 1]\n]\n\n"],[-1,"\n"],[0,"display "]],"start1":233,"start2":233,"length1":17,"length2":16}]}]},{"timestamp":1740264901764,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," 1]\n]\n\n\n"],[-1,"\n"],[0,"display "]],"start1":234,"start2":234,"length1":17,"length2":16}]}]},{"timestamp":1740264911745,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"1]]\n"],[-1,"function display_maze() {\n    basic.clearScreen()\n    //  Clear previous display\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            if (maze[y][x] == 1) {\n                //  If it's a wall, turn on the LED\n                led.plot(x, y)\n            }\n            \n        }\n    }\n}\n\n//  Display the maze\ndisplay_maze()\n"],[0,"//  "]],"start1":263,"start2":263,"length1":361,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\n]\n\n"],[-1,"def display_maze():\n    basic.clear_screen()  # Clear previous display\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:  # If it's a wall, turn on the LED\n                led.plot(x, y)\n\n# Display the maze\ndisplay_maze()\n"],[1,"\n\ndisplay maze"],[0,"\n# P"]],"start1":237,"start2":237,"length1":262,"length2":22}]}]},{"timestamp":1740265045800,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"],\n    ["],[-1,"0"],[1,"1"],[0,", 0, 0, "]],"start1":152,"start2":152,"length1":17,"length2":17},{"diffs":[[0,"plot"],[-1,"_brightness"],[0,"(x, y"],[-1,", 10"],[0,")\n\n#"]],"start1":449,"start2":449,"length1":28,"length2":13}]}]},{"timestamp":1740266407523,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"1, 1], ["],[-1,"0"],[1,"1"],[0,", 0, 0, "]],"start1":192,"start2":192,"length1":17,"length2":17},{"diffs":[[0,"plot"],[-1,"Brightness"],[0,"(x, y"],[-1,", 10"],[0,")\n  "]],"start1":527,"start2":527,"length1":27,"length2":13}]}]},{"timestamp":1740266947939,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"o show movement\n"],[1,"        basic.clearScreen()\n        //  Clear previous display\n"],[0,"        led.plot"]],"start1":2255,"start2":2255,"length1":32,"length2":95}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"        "],[1,"basic.clear_screen()  # Clear previous display"],[0,"\n       "]],"start1":1859,"start2":1859,"length1":16,"length2":62}]}]},{"timestamp":1740266951693,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[1,"from microbit import *\nimport Math  # Use Micro:bit's Math module for randomness\n\n# Maze representation (1 = wall, 0 = path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [0, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\ndef display_maze():\n    basic.clear_screen()  # Clear previous display\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:  # If it's a wall, turn on the LED\n                led.plot_brightness(x, y, 10)\n\n# Display the maze\ndisplay_maze()\n\n# Player start position\nplayer_x, player_y = 1, 1\ngoal_x, goal_y = 3, 4\n\n# Learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 0.3  # Exploration chance\n\n# Possible moves (Up, Down, Left, Right)\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Function to choose an action\ndef choose_action():\n    if Math.random() < epsilon:  # Random move for exploration\n        action_index = int(Math.random() * 4)  # Pick a random index between 0-3\n        return action_index\n    else:\n        return 0  # Placeholder for now, can be improved with learned Q-values\n\n# Training loop\nfor episode in range(10):  # Run multiple learning episodes\n    player_x, player_y = 1, 1  # Reset position\n    while (player_x, player_y) != (goal_x, goal_y):  # Until goal is reached\n        action = choose_action()\n        new_x = player_x + move_x[action]\n        new_y = player_y + move_y[action]\n\n        # Check if the move is valid\n        if maze[new_y][new_x] == 1:  # Hit a wall\n            reward = -5\n        elif (new_x, new_y) == (goal_x, goal_y):  # Reached goal\n            reward = 10\n        else:\n            reward = -1  # Normal movement\n\n        # Move the player only if it's a valid move\n        if maze[new_y][new_x] == 0:\n            player_x, player_y = new_x, new_y\n\n        # Update display to show movement\n        \n        led.plot(player_x, player_y)  # Show player position\n        basic.pause(500)  # Wait for a short time\n\n# Display success message when done\nbasic.show_icon(IconNames.HAPPY)"]],"start1":0,"start2":0,"length1":0,"length2":2048}]}]},{"timestamp":1740334754439,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"let action: number;\nlet new_x: number;\nlet new_y: number;\nlet reward: number;\n//  Use Micro:bit's Math module for randomness\n//  Maze representation (1 = wall, 0 = path)\nlet maze = [[1, 1, 1, 1, 1], [0, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\nfunction display_maze() {\n    basic.clearScreen()\n    //  Clear previous display\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            if (maze[y][x] == 1) {\n                //  If it's a wall, turn on the LED\n                led.plotBrightness(x, y, 10)\n            }\n            \n        }\n    }\n}\n\n//  Display the maze\ndisplay_maze()\n//  Player start position\nlet [player_x, player_y] = [1, 1]\nlet [goal_x, goal_y] = [3, 4]\n//  Learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 0.3\n//  Exploration chance\n//  Possible moves (Up, Down, Left, Right)\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\n//  Function to choose an action\nfunction choose_action(): number {\n    let action_index: number;\n    if (Math.random() < epsilon) {\n        //  Random move for exploration\n        action_index = Math.trunc(Math.random() * 4)\n        //  Pick a random index between 0-3\n        return action_index\n    } else {\n        return 0\n    }\n    \n}\n\n//  Placeholder for now, can be improved with learned Q-values\n//  Training loop\nfor (let episode = 0; episode < 10; episode++) {\n    //  Run multiple learning episodes\n    let [player_x, player_y] = [1, 1]\n    //  Reset position\n    while ([player_x, player_y] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        action = choose_action()\n        new_x = player_x + move_x[action]\n        new_y = player_y + move_y[action]\n        //  Check if the move is valid\n        if (maze[new_y][new_x] == 1) {\n            //  Hit a wall\n            reward = -5\n        } else if ([new_x, new_y] == [goal_x, goal_y]) {\n            //  Reached goal\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        //  Normal movement\n        //  Move the player only if it's a valid move\n        if (maze[new_y][new_x] == 0) {\n            let [player_x, player_y] = [new_x, new_y]\n        }\n        \n        //  Update display to show movement\n        led.plot(player_x, player_y)\n        //  Show player position\n        basic.pause(500)\n    }\n}\n//  Wait for a short time\n//  Display success message when done\nbasic.showIcon(IconNames.Happy)"],[0,"\n"]],"start1":0,"start2":0,"length1":1,"length2":2470}]}]},{"timestamp":1740334754536,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[-1,"from microbit import *\n\n# 5x5 Q-table initialized to 0\nQ_table = {}\nfor y in range(5):\n    for x in range(5):\n        Q_table[(x, y)] = [0, 0, 0, 0]  # [Up, Down, Left, Right]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Q-learning parameters\nalpha = 0.1   # Learning rate\ngamma = 0.9   # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\ngoal_x, goal_y = 3, 4  # Goal position\n\n# Initialize robot position\nrobot_x, robot_y = 1, 1  # Start at (1,1)\n\n# Run multiple training episodes\nfor episode in range(100):  # 100 training episodes\n    robot_x, robot_y = 1, 1  # Reset position each episode\n\n    while (robot_x, robot_y) != (goal_x, goal_y):  # Until goal is reached\n        # Choose action (exploration or exploitation)\n        # Exploration or Exploitation\n        if random.random() < epsilon:\n            action_index = random.randint(0, 3)  # explore\n        else:\n            # exploit (choose best action manually)\n            q_values = Q_table[(x, y)]\n            action_index = 0\n            max_q = q_values[0]\n            for i in range(1, len(q_values)):\n                if q_values[i] > max_q:\n                    max_q = q_values[i]\n                    action_index = i\n\n\n        # Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        # Check if move is valid\n        if maze[new_y][new_x] == 1:  # Hit a wall\n            reward = -5\n            new_x, new_y = robot_x, robot_y  # Stay in place\n        elif (new_x, new_y) == (goal_x, goal_y):  # Reached goal\n            reward = 10\n        else:\n            reward = -1  # Normal movement\n\n        # Q-value update using the formula:\n        old_q_value = Q_table[(robot_x, robot_y)][action_index]\n        max_future_q = max(Q_table[(new_x, new_y)])  # Best future Q-value\n\n        Q_table[(robot_x, robot_y)][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n\n        # Move the robot to the new position\n        robot_x, robot_y = new_x, new_y\n\n    # Reduce epsilon after each episode (exponential decay)\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    print(f\"Episode {episode+1} completed | Epsilon: {round(epsilon, 4)}\")\n\n"]],"start1":0,"start2":0,"length1":2562,"length2":0}]}]},{"timestamp":1740335347869,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"# Exploration or Exploitation\n        if Math"],[1,"if random"],[0,".ran"]],"start1":1124,"start2":1124,"length1":53,"length2":17},{"diffs":[[0,"index = "],[1,"random."],[0,"randint("]],"start1":1177,"start2":1177,"length1":16,"length2":23},{"diffs":[[0,"lore"],[-1," (0, 1, 2, or 3)"],[0,"\n   "]],"start1":1212,"start2":1212,"length1":24,"length2":8},{"diffs":[[0,"e(1,"],[-1," 4):  # Directly use 4 instead of"],[0," len"]],"start1":1408,"start2":1408,"length1":41,"length2":8},{"diffs":[[0,"_values)"],[1,"):"],[0,"\n       "]],"start1":1418,"start2":1418,"length1":16,"length2":18}]}]},{"timestamp":1740335717877,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"let y: number;\nlet x: number;\nlet action_index: number;\nlet q_values: any;\nlet max_q: any;\nlet i: number;\nlet new_x: number;\nlet new_y: number;\nlet reward: number;\nlet old_q_value: number;\n//  5x5 Q-table initialized to 0\nlet Q_table =  {\n\t\n}\n\nfor (y = 0; y < 5; y++) {\n    for (x = 0; x < 5; x++) {\n        Q_table[[x, y]] = [0, 0, 0, 0]\n    }\n}\n//  [Up, Down, Left, Right]\n//  Define possible moves\nlet actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\n//  Define the maze (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay = 0.98\n//  Multiply epsilon by this after each episode\nlet [goal_x, goal_y] = [3, 4]\n//  Goal position\n//  Initialize robot position\nlet [robot_x, robot_y] = [1, 1]\n//  Start at (1,1)\n//  Run multiple training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    //  100 training episodes\n    let [robot_x, robot_y] = [1, 1]\n    //  Reset position each episode\n    while ([robot_x, robot_y] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        //  Choose action (exploration or exploitation)\n        //  Exploration or Exploitation\n        //  Exploration or Exploitation\n        if (Math.random() < epsilon) {\n            action_index = randint(0, 3)\n        } else {\n            //  explore (0, 1, 2, or 3)\n            //  exploit (choose best action manually)\n            q_values = Q_table[[x, y]]\n            action_index = 0\n            max_q = q_values[0]\n            for (i = 1; i < 4; i++) {\n                //  Directly use 4 instead of len(q_values)\n                if (q_values[i] > max_q) {\n                    max_q = q_values[i]\n                    action_index = i\n                }\n                \n            }\n        }\n        \n        //  Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n        //  Check if move is valid\n        if (maze[new_y][new_x] == 1) {\n            //  Hit a wall\n            reward = -5\n            let [new_x, new_y] = [robot_x, robot_y]\n        } else if ([new_x, new_y] == [goal_x, goal_y]) {\n            //  Stay in place\n            //  Reached goal\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        //  Normal movement\n        //  Q-value update using the formula:\n        old_q_value = Q_table[[robot_x, robot_y]][action_index]\n    }\n}\n//  Manually find max future Q-value for the next state\nlet next_q_values = Q_table[[new_x, new_y]]\nlet max_future_q = next_q_values[0]\nfor (i = 1; i < 4; i++) {\n    if (next_q_values[i] > max_future_q) {\n        max_future_q = next_q_values[i]\n        Q_table[[robot_x, robot_y]][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n        //  Move the robot to the new position\n        let [robot_x, robot_y] = [new_x, new_y]\n    }\n    \n    //  Reduce epsilon after each episode (exponential decay)\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n    console.log(\"Episode {episode+1} completed | Epsilon: {round(epsilon, 4)}\")\n}"],[0,"\n"]],"start1":0,"start2":0,"length1":3346,"length2":1}]}]},{"timestamp":1740336251049,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"lue\n"],[-1,"        next_q_values = Q_table[(new_x, new_y)]\n        max_future_q = next_q_values[0]\n        for i in range(1, 4):\n            if next_q_values[i] > max_future_q:\n                max_future_q = next_q_values[i]\n                "],[0,"\n   "]],"start1":2246,"start2":2246,"length1":238,"length2":8}]}]},{"timestamp":1740336556887,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\n       "],[-1,"#"],[1," "],[0,"max_futu"]],"start1":2174,"start2":2174,"length1":17,"length2":17},{"diffs":[[0,"lues[i]\n"],[1,"                "],[0,"\n       "]],"start1":2456,"start2":2456,"length1":16,"length2":32}]}]},{"timestamp":1740339407024,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"er;\n"],[-1,"let next_q_values: any;\nlet max_future_q: number;\n"],[0,"//  "]],"start1":185,"start2":185,"length1":58,"length2":8},{"diffs":[[0,"    "],[-1,"    //"],[1,"}\n}\n//  Manually find"],[0," max"],[-1,"_"],[1," "],[0,"future"],[-1,"_q = max(Q_table[(new_x, new_y)])  # Best future Q-value\n       "],[1," Q-value for the next state\nlet"],[0," nex"]],"start1":2675,"start2":2675,"length1":89,"length2":71},{"diffs":[[0,"new_y]]\n"],[-1,"       "],[1,"let"],[0," max_fut"]],"start1":2775,"start2":2775,"length1":23,"length2":19},{"diffs":[[0,"lues[0]\n"],[-1,"        "],[0,"for (i ="]],"start1":2811,"start2":2811,"length1":24,"length2":16},{"diffs":[[0,"< 4; i++) {\n"],[-1,"        "],[0,"    if (next"]],"start1":2833,"start2":2833,"length1":32,"length2":24},{"diffs":[[0,") {\n        "],[-1,"      "],[-1,"  "],[0,"max_future_q"]],"start1":2884,"start2":2884,"length1":32,"length2":24},{"diffs":[[0,"[i]\n"],[-1,"            }\n            \n        }\n"],[0,"    "]],"start1":2924,"start2":2924,"length1":45,"length2":8},{"diffs":[[0," new_y]\n"],[-1,""],[0,"    }\n"],[1,"    \n"],[0,"    //  "]],"start1":3122,"start2":3122,"length1":22,"length2":27}]}]},{"timestamp":1740339407096,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"        "],[-1,"#"],[0,"Q_table["]],"start1":110,"start2":110,"length1":17,"length2":16}]}]},{"timestamp":1740340258913,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"nge(5):\n        "],[1,"#"],[0,"Q_table[(x, y)] "]],"start1":102,"start2":102,"length1":32,"length2":33}]}]},{"timestamp":1740340272193,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\n       "],[-1," "],[0,"#max_fut"]],"start1":2174,"start2":2174,"length1":17,"length2":16}]}]},{"timestamp":1740341276098,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"lue\n"],[-1,"        # Find max future Q manually (next state)\n"],[0,"    "]],"start1":2247,"start2":2247,"length1":58,"length2":8},{"diffs":[[0,"_y)]"],[-1,"  # Corrected to tuple"],[0,"\n   "]],"start1":2294,"start2":2294,"length1":30,"length2":8},{"diffs":[[0,"ues[i]\n\n"],[-1,"\n"],[0,"        "]],"start1":2458,"start2":2458,"length1":17,"length2":16}]}]},{"timestamp":1740341476553,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"lue\n"],[-1,"        //  Find max future Q manually (next state)\n"],[0,"    "]],"start1":2799,"start2":2799,"length1":60,"length2":8},{"diffs":[[0,"y]]\n"],[-1,"        //  Corrected to tuple\n"],[0,"    "]],"start1":2847,"start2":2847,"length1":39,"length2":8}]}]},{"timestamp":1740342147055,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," state)\n"],[1,"        "],[0,"next_q_v"]],"start1":2293,"start2":2293,"length1":16,"length2":24},{"diffs":[[0,"o tuple\n"],[1,"        "],[0,"max_futu"]],"start1":2363,"start2":2363,"length1":16,"length2":24},{"diffs":[[0,"lues[0]\n"],[1,"        "],[0,"for i in"]],"start1":2403,"start2":2403,"length1":16,"length2":24},{"diffs":[[0,"(1, 4):\n"],[1,"        "],[0,"    if n"]],"start1":2433,"start2":2433,"length1":16,"length2":24},{"diffs":[[0,"x_future_q:\n"],[1,"        "],[0,"        max_"]],"start1":2477,"start2":2477,"length1":24,"length2":32}]}]},{"timestamp":1740342147243,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"Q-value\n"],[1,"        //  Find max future Q manually (next state)\n"],[0,"        "]],"start1":2795,"start2":2795,"length1":16,"length2":68},{"diffs":[[0,"_x, new_y]]\n"],[1,"        //  Corrected to tuple\n"],[0,"        max_"]],"start1":2891,"start2":2891,"length1":24,"length2":55}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"        "],[1,"# Find max future Q manually (next state)\n"],[0,"next_q_v"]],"start1":2251,"start2":2251,"length1":16,"length2":58},{"diffs":[[0," new_y)]"],[-1,"\n"],[0,"  "],[-1,"      "],[1,"# Corrected to tuple\n"],[0,"max_futu"]],"start1":2332,"start2":2332,"length1":25,"length2":39},{"diffs":[[0,"lues[0]\n"],[-1,"        "],[0,"for i in"]],"start1":2387,"start2":2387,"length1":24,"length2":16},{"diffs":[[0,"(1, 4):\n"],[-1,"        "],[0,"    if n"]],"start1":2409,"start2":2409,"length1":24,"length2":16},{"diffs":[[0,"_q:\n        "],[-1,"      "],[-1,"  "],[0,"max_future_q"]],"start1":2453,"start2":2453,"length1":32,"length2":24},{"diffs":[[0,"lues[i]\n"],[-1,"                "],[1,"\n"],[0,"\n       "]],"start1":2489,"start2":2489,"length1":32,"length2":17}]}]},{"timestamp":1740342551829,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"let y: number;\nlet x: number;\n"],[0,"let action_index"]],"start1":0,"start2":0,"length1":16,"length2":46},{"diffs":[[0,"}\n\nfor ("],[-1,"let "],[0,"y = 0; y"]],"start1":291,"start2":291,"length1":20,"length2":16},{"diffs":[[0,"   for ("],[-1,"let "],[0,"x = 0; x"]],"start1":321,"start2":321,"length1":20,"length2":16},{"diffs":[[0,"ble["],[-1,"\"\" + x + \",\" + (\"\" + y)"],[1,"[x, y]"],[0,"] = "]],"start1":362,"start2":362,"length1":31,"length2":14},{"diffs":[[0,"//  "],[-1,"Epsilon decay"],[1,"Multiply epsilon by this"],[0," aft"]],"start1":917,"start2":917,"length1":21,"length2":32},{"diffs":[[0,"osition\n"],[1,"//  Initialize robot position\nlet [robot_x, robot_y] = [1, 1]\n//  Start at (1,1)\n"],[0,"//  Run "]],"start1":1005,"start2":1005,"length1":16,"length2":97},{"diffs":[[0,"pisode++) {\n"],[1,"    //  100 training episodes\n"],[0,"    let [rob"]],"start1":1167,"start2":1167,"length1":24,"length2":54},{"diffs":[[0,"  Reset "],[1,"position "],[0,"each epi"]],"start1":1251,"start2":1251,"length1":16,"length2":25},{"diffs":[[0,"l_x, goal_y]) {\n"],[1,"        //  Until goal is reached\n        //  Choose action (exploration or exploitation)\n        //  Exploration or Exploitation\n"],[0,"        //  Expl"]],"start1":1318,"start2":1318,"length1":32,"length2":162},{"diffs":[[0," explore"],[1," (0, 1, 2, or 3)"],[0,"\n       "]],"start1":1616,"start2":1616,"length1":16,"length2":32},{"diffs":[[0,"ble["],[-1,"\"\" + robot_x + \",\" + (\"\" + robot_y)"],[1,"[x, y]"],[0,"]\n  "]],"start1":1722,"start2":1722,"length1":43,"length2":14},{"diffs":[[0,"; i < 4; i++) {\n"],[1,"                //  Directly use 4 instead of len(q_values)\n"],[0,"                "]],"start1":1817,"start2":1817,"length1":32,"length2":92},{"diffs":[[0,"_x] == 1) {\n"],[1,"            //  Hit a wall\n"],[0,"            "]],"start1":2272,"start2":2272,"length1":24,"length2":51},{"diffs":[[0,"l_x, goal_y]) {\n"],[1,"            //  Stay in place\n            //  Reached goal\n"],[0,"            rewa"]],"start1":2428,"start2":2428,"length1":32,"length2":91},{"diffs":[[0,"//  "],[-1,"Q-value update"],[1,"Normal movement\n        //  Q-value update using the formula:"],[0,"\n   "]],"start1":2595,"start2":2595,"length1":22,"length2":69},{"diffs":[[0,"ble["],[-1,"\"\" + "],[1,"["],[0,"robot_x"],[-1," + \",\" + (\"\" +"],[1,","],[0," robot_y"],[-1,")"],[1,"]"],[0,"][ac"]],"start1":2687,"start2":2687,"length1":43,"length2":26},{"diffs":[[0,"    "],[-1,"next_q_values = Q_table[\"\" + new_x + \",\" + (\"\" +"],[1,"// max_future_q = max(Q_table[(new_x, new_y)])  # Best future Q-value\n        next_q_values = Q_table[[new_x,"],[0," new_y"],[-1,")"],[1,"]"],[0,"]\n  "]],"start1":2729,"start2":2729,"length1":63,"length2":124},{"diffs":[[0,"ble["],[-1,"\"\" + "],[1,"["],[0,"robot_x"],[-1," + \",\" + (\"\" +"],[1,","],[0," robot_y"],[-1,")"],[1,"]"],[0,"][ac"]],"start1":3073,"start2":3073,"length1":43,"length2":26},{"diffs":[[0,"  Move t"],[-1,"o"],[1,"he robot to the"],[0," new pos"]],"start1":3178,"start2":3178,"length1":17,"length2":31},{"diffs":[[0,"//  "],[-1,"Decay"],[1,"Reduce"],[0," eps"]],"start1":3273,"start2":3273,"length1":13,"length2":14},{"diffs":[[0,"each episode"],[1," (exponential decay)"],[0,"\n    epsilon"]],"start1":3298,"start2":3298,"length1":24,"length2":44},{"diffs":[[0,"on_min)\n"],[1,"    console.log(\"Episode {episode+1} completed | Epsilon: {round(epsilon, 4)}\")\n"],[0,"}\n"]],"start1":3384,"start2":3384,"length1":10,"length2":90}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ble["],[-1,"str(x) + \",\" + str("],[1,"(x, "],[0,"y)] "]],"start1":122,"start2":122,"length1":27,"length2":12},{"diffs":[[0,"  # "],[-1,"Epsilon decay"],[1,"Multiply epsilon by this"],[0," aft"]],"start1":648,"start2":648,"length1":21,"length2":32},{"diffs":[[0,"sition\n\n"],[1,"# Initialize robot position\nrobot_x, robot_y = 1, 1  # Start at (1,1)\n\n"],[0,"# Run mu"]],"start1":729,"start2":729,"length1":16,"length2":87},{"diffs":[[0,"ge(100):"],[1,"  # 100 training episodes"],[0,"\n    rob"]],"start1":859,"start2":859,"length1":16,"length2":41},{"diffs":[[0,"# Reset "],[1,"position "],[0,"each epi"]],"start1":922,"start2":922,"length1":16,"length2":25},{"diffs":[[0,"goal_x, goal_y):"],[1,"  # Until goal is reached\n        # Choose action (exploration or exploitation)\n        # Exploration or Exploitation"],[0,"\n        # Explo"]],"start1":986,"start2":986,"length1":32,"length2":149},{"diffs":[[0," explore"],[1," (0, 1, 2, or 3)"],[0,"\n       "]],"start1":1237,"start2":1237,"length1":16,"length2":32},{"diffs":[[0,"ble["],[-1,"str(robot_x) + \",\" + str(robot_"],[1,"(x, "],[0,"y)]\n"]],"start1":1355,"start2":1355,"length1":39,"length2":12},{"diffs":[[0," in range(1, 4):"],[1,"  # Directly use 4 instead of len(q_values)"],[0,"\n               "]],"start1":1445,"start2":1445,"length1":32,"length2":75},{"diffs":[[0,"ex = i\n\n"],[1,"\n"],[0,"        "]],"start1":1615,"start2":1615,"length1":16,"length2":17},{"diffs":[[0,"x] == 1:"],[1,"  # Hit a wall"],[0,"\n       "]],"start1":1812,"start2":1812,"length1":16,"length2":30},{"diffs":[[0," robot_y"],[1,"  # Stay in place"],[0,"\n       "]],"start1":1894,"start2":1894,"length1":16,"length2":33},{"diffs":[[0,"goal_y):"],[1,"  # Reached goal"],[0,"\n       "]],"start1":1960,"start2":1960,"length1":16,"length2":32},{"diffs":[[0,"ard = -1"],[1,"  # Normal movement"],[0,"\n\n      "]],"start1":2038,"start2":2038,"length1":16,"length2":35},{"diffs":[[0,"e update"],[1," using the formula:"],[0,"\n       "]],"start1":2083,"start2":2083,"length1":16,"length2":35},{"diffs":[[0,"ble["],[-1,"str"],[0,"(robot_x"],[-1,") + \",\" + str("],[1,", "],[0,"robo"]],"start1":2137,"start2":2137,"length1":33,"length2":18},{"diffs":[[0,"][action_index]\n"],[1,"        #max_future_q = max(Q_table[(new_x, new_y)])  # Best future Q-value"],[0,"\n        next_q_"]],"start1":2159,"start2":2159,"length1":32,"length2":107},{"diffs":[[0,"ble["],[-1,"str"],[0,"(new_x"],[-1,") + \",\" + str("],[1,", "],[0,"new_"]],"start1":2279,"start2":2279,"length1":31,"length2":16},{"diffs":[[0,"lues[i]\n"],[1,"                "],[0,"\n       "]],"start1":2457,"start2":2457,"length1":16,"length2":32},{"diffs":[[0,"ble["],[-1,"str"],[0,"(robot_x"],[-1,") + \",\" + str("],[1,", "],[0,"robo"]],"start1":2494,"start2":2494,"length1":33,"length2":18},{"diffs":[[0,"# Move t"],[-1,"o"],[1,"he robot to the"],[0," new pos"]],"start1":2598,"start2":2598,"length1":17,"length2":31},{"diffs":[[0,"  # "],[-1,"Decay"],[1,"Reduce"],[0," eps"]],"start1":2678,"start2":2678,"length1":13,"length2":14},{"diffs":[[0," episode"],[1," (exponential decay)"],[0,"\n    eps"]],"start1":2707,"start2":2707,"length1":16,"length2":36},{"diffs":[[0,"int("],[1,"f"],[0,"\"Episode"],[-1,":\","],[0," "],[1,"{"],[0,"epis"]],"start1":2799,"start2":2799,"length1":20,"length2":19},{"diffs":[[0,"de+1"],[-1,", \""],[1,"} completed | "],[0,"Epsilon:"],[-1,"\","],[0," "],[1,"{"],[0,"roun"]],"start1":2819,"start2":2819,"length1":22,"length2":32},{"diffs":[[0,"ilon, 4)"],[1,"}\""],[0,")\n"],[1,"\n"]],"start1":2856,"start2":2856,"length1":10,"length2":13}]}]},{"timestamp":1740343116153,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ilon:\", "],[-1,"\""],[0,"round(ep"]],"start1":2413,"start2":2413,"length1":17,"length2":16}]}]},{"timestamp":1740374728306,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ilon, 4)"],[-1,"\""],[0,")\n"]],"start1":2431,"start2":2431,"length1":11,"length2":10}]}]},{"timestamp":1740374730714,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"Q_table["],[1,"\"\" + "],[0,"robot_x "]],"start1":1436,"start2":1436,"length1":16,"length2":21},{"diffs":[[0," + \",\" +"],[1," (\"\" +"],[0," robot_y"],[1,")"],[0,"]\n      "]],"start1":1456,"start2":1456,"length1":24,"length2":31}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"Q_table["],[1,"str("],[0,"robot_x"],[1,")"],[0," + \",\" +"]],"start1":1117,"start2":1117,"length1":23,"length2":28},{"diffs":[[0,"+ \",\" + "],[1,"str("],[0,"robot_y"],[1,")"],[0,"]\n      "]],"start1":1138,"start2":1138,"length1":23,"length2":28},{"diffs":[[0,"n_min)\n\n"],[1,"    print(\"Episode:\", episode+1, \"Epsilon:\", \"round(epsilon, 4)\")\n"]],"start1":2368,"start2":2368,"length1":8,"length2":74}]}]},{"timestamp":1740375300825,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"alues = Q_table["],[-1,"\"\" + "],[0,"robot_x + \",\" + "]],"start1":1428,"start2":1428,"length1":37,"length2":32},{"diffs":[[0," + \",\" +"],[-1," (\"\" +"],[0," robot_y"],[-1,")"],[0,"]\n      "]],"start1":1451,"start2":1451,"length1":31,"length2":24}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"s = Q_table["],[-1,"str("],[0,"robot_x"],[-1,")"],[0," + \",\" + str"]],"start1":1113,"start2":1113,"length1":36,"length2":31},{"diffs":[[0,"+ \",\" + "],[-1,"str("],[0,"robot_y"],[-1,")"],[0,"]\n      "]],"start1":1133,"start2":1133,"length1":28,"length2":23}]}]},{"timestamp":1740375468356,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[1,"from microbit import *\n\n# 5x5 Q-table initialized to 0\nQ_table = {}\nfor y in range(5):\n    for x in range(5):\n        Q_table[str(x) + \",\" + str(y)] = [0, 0, 0, 0]  # [Up, Down, Left, Right]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Q-learning parameters\nalpha = 0.1   # Learning rate\ngamma = 0.9   # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Epsilon decay after each episode\n\ngoal_x, goal_y = 3, 4  # Goal position\n\n# Run multiple training episodes\nfor episode in range(100):\n    robot_x, robot_y = 1, 1  # Reset each episode\n\n    while (robot_x, robot_y) != (goal_x, goal_y):\n        # Exploration or Exploitation\n        if Math.random() < epsilon:\n            action_index = randint(0, 3)  # explore\n        else:\n            # exploit (choose best action manually)\n            q_values = Q_table[str(robot_x) + \",\" + str(robot_y)]\n            action_index = 0\n            max_q = q_values[0]\n            for i in range(1, 4):\n                if q_values[i] > max_q:\n                    max_q = q_values[i]\n                    action_index = i\n\n        # Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        # Check if move is valid\n        if maze[new_y][new_x] == 1:\n            reward = -5\n            new_x, new_y = robot_x, robot_y\n        elif (new_x, new_y) == (goal_x, goal_y):\n            reward = 10\n        else:\n            reward = -1\n\n        # Q-value update\n        old_q_value = Q_table[str(robot_x) + \",\" + str(robot_y)][action_index]\n\n        next_q_values = Q_table[str(new_x) + \",\" + str(new_y)]\n        max_future_q = next_q_values[0]\n        for i in range(1, 4):\n            if next_q_values[i] > max_future_q:\n                max_future_q = next_q_values[i]\n\n        Q_table[str(robot_x) + \",\" + str(robot_y)][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n\n        # Move to new position\n        robot_x, robot_y = new_x, new_y\n\n    # Decay epsilon after each episode\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)"],[0,"\n\n"]],"start1":0,"start2":0,"length1":2,"length2":2376}]}]},{"timestamp":1740377161692,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"let action_index: number;\nlet q_values: any;\nlet max_q: any;\nlet i: number;\nlet new_x: number;\nlet new_y: number;\nlet reward: number;\nlet old_q_value: number;\nlet next_q_values: any;\nlet max_future_q: number;\n//  5x5 Q-table initialized to 0\nlet Q_table =  {\n\t\n}\n\nfor (let y = 0; y < 5; y++) {\n    for (let x = 0; x < 5; x++) {\n        Q_table[\"\" + x + \",\" + (\"\" + y)] = [0, 0, 0, 0]\n    }\n}\n//  [Up, Down, Left, Right]\n//  Define possible moves\nlet actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\n//  Define the maze (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay = 0.98\n//  Epsilon decay after each episode\nlet [goal_x, goal_y] = [3, 4]\n//  Goal position\n//  Run multiple training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    let [robot_x, robot_y] = [1, 1]\n    //  Reset each episode\n    while ([robot_x, robot_y] != [goal_x, goal_y]) {\n        //  Exploration or Exploitation\n        if (Math.random() < epsilon) {\n            action_index = randint(0, 3)\n        } else {\n            //  explore\n            //  exploit (choose best action manually)\n            q_values = Q_table[\"\" + robot_x + \",\" + (\"\" + robot_y)]\n            action_index = 0\n            max_q = q_values[0]\n            for (i = 1; i < 4; i++) {\n                if (q_values[i] > max_q) {\n                    max_q = q_values[i]\n                    action_index = i\n                }\n                \n            }\n        }\n        \n        //  Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n        //  Check if move is valid\n        if (maze[new_y][new_x] == 1) {\n            reward = -5\n            let [new_x, new_y] = [robot_x, robot_y]\n        } else if ([new_x, new_y] == [goal_x, goal_y]) {\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        //  Q-value update\n        old_q_value = Q_table[\"\" + robot_x + \",\" + (\"\" + robot_y)][action_index]\n        next_q_values = Q_table[\"\" + new_x + \",\" + (\"\" + new_y)]\n        max_future_q = next_q_values[0]\n        for (i = 1; i < 4; i++) {\n            if (next_q_values[i] > max_future_q) {\n                max_future_q = next_q_values[i]\n            }\n            \n        }\n        Q_table[\"\" + robot_x + \",\" + (\"\" + robot_y)][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n        //  Move to new position\n        let [robot_x, robot_y] = [new_x, new_y]\n    }\n    //  Decay epsilon after each episode\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n}"],[0,"\n"]],"start1":0,"start2":0,"length1":1,"length2":2886}]}]},{"timestamp":1740377162106,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[-1,"from microbit import *\n\n\n# Explicit Q-table initialization\nQ_table = {}\n\n# Helper function to create a key\ndef get_key(x, y):\n    return \"{}:{}\".format(x, y)\n\n# Initialize Q-table explicitly\nfor y in range(5):\n    for x in range(5):\n        Q_table[get_key(x, y)] = [0, 0, 0, 0]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Q-learning parameters\nalpha = 0.1\ngamma = 0.9\nepsilon = 1.0\nepsilon_min = 0.05\nepsilon_decay = 0.98\n\ngoal_x, goal_y = 3, 4\n\n# Training episodes\nfor episode in range(100):\n    robot_x, robot_y = 1, 1\n\n    while (robot_x, robot_y) != (goal_x, goal_y):\n        # Exploration or Exploitation\n        if Math.random() < epsilon:\n            action_index = randint(0, 3)\n        else:\n            key = get_key(robot_x, robot_y)\n            q_values = Q_table[key]\n            action_index = 0\n            max_q = q_values[0]\n            for i in range(1, 4):\n                if q_values[i] > max_q:\n                    max_q = q_values[i]\n                    action_index = i\n\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        if maze[new_y][new_x] == 1:\n            reward = -5\n            new_x, new_y = robot_x, robot_y\n        elif (new_x, new_y) == (goal_x, goal_y):\n            reward = 10\n        else:\n            reward = -1\n\n        old_key = get_key(robot_x, robot_y)\n        new_key = get_key(new_x, new_y)\n\n        old_q_value = Q_table[old_key][action_index]\n\n        next_q_values = Q_table[new_key]\n        max_future_q = next_q_values[0]\n        for i in range(1, 4):\n            if next_q_values[i] > max_future_q:\n                max_future_q = next_q_values[i]\n\n        # Update Q-table explicitly\n        Q_table[old_key][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n\n        robot_x, robot_y = new_x, new_y\n\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    print(\"Episode:\", episode+1, \"Epsilon:\", round(epsilon, 4))\n"],[0,"\n\n"]],"start1":0,"start2":0,"length1":2138,"length2":2}]}]},{"timestamp":1740377204268,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"n)\n\n    "],[-1,"#"],[0,"print(\"E"]],"start1":2068,"start2":2068,"length1":17,"length2":16}]}]},{"timestamp":1740668899808,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"n)\n\n"],[-1,"    \n"],[0,"    "]],"start1":2068,"start2":2068,"length1":13,"length2":8}]}]},{"timestamp":1740668916479,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"let action_index: number;\nlet key: number;\nlet q_values: any;\nlet max_q: any;\nlet i: number;\nlet new_x: number;\nlet new_y: number;\nlet reward: number;\nlet old_key: number;\nlet new_key: number;\nlet old_q_value: number;\nlet next_q_values: any;\nlet max_future_q: number;\nlet printstr: any;\n//  Explicit Q-table initialization\nlet Q_table =  {\n\t\n}\n\n//  Helper function to create a key\nfunction get_key(x: number, y: number): number {\n    return \"\" + x + \":\" + (\"\" + y)\n}\n\n//  Initialize Q-table explicitly\nfor (let y = 0; y < 5; y++) {\n    for (let x = 0; x < 5; x++) {\n        Q_table[get_key(x, y)] = [0, 0, 0, 0]\n    }\n}\n//  Define possible moves\nlet actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Q-learning parameters\nlet alpha = 0.1\nlet gamma = 0.9\nlet epsilon = 1.0\nlet epsilon_min = 0.05\nlet epsilon_decay = 0.98\nlet [goal_x, goal_y] = [3, 4]\n//  Training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    let [robot_x, robot_y] = [1, 1]\n    while ([robot_x, robot_y] != [goal_x, goal_y]) {\n        //  Exploration or Exploitation\n        if (Math.random() < epsilon) {\n            action_index = randint(0, 3)\n        } else {\n            key = get_key(robot_x, robot_y)\n            q_values = Q_table[key]\n            action_index = 0\n            max_q = q_values[0]\n            for (i = 1; i < 4; i++) {\n                if (q_values[i] > max_q) {\n                    max_q = q_values[i]\n                    action_index = i\n                }\n                \n            }\n        }\n        \n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n        if (maze[new_y][new_x] == 1) {\n            reward = -5\n            let [new_x, new_y] = [robot_x, robot_y]\n        } else if ([new_x, new_y] == [goal_x, goal_y]) {\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        old_key = get_key(robot_x, robot_y)\n        new_key = get_key(new_x, new_y)\n        old_q_value = Q_table[old_key][action_index]\n        next_q_values = Q_table[new_key]\n        max_future_q = next_q_values[0]\n        for (i = 1; i < 4; i++) {\n            if (next_q_values[i] > max_future_q) {\n                max_future_q = next_q_values[i]\n            }\n            \n        }\n        //  Update Q-table explicitly\n        Q_table[old_key][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n        let [robot_x, robot_y] = [new_x, new_y]\n    }\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n    printstr = \"Episode: \" + (\"\" + (episode + 1)) + \"Epsilon: \" + (\"\" + epsilon)\n    console.log(printstr)\n}"],[0,"\n"]],"start1":0,"start2":0,"length1":2758,"length2":1}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"urn "],[-1,"str(x) + \":\" + str("],[1,"\"{}:{}\".format(x, "],[0,"y)\n\n"],[-1,"\n"],[0,"# In"]],"start1":133,"start2":133,"length1":32,"length2":30},{"diffs":[[0,"    "],[-1,"printstr = \"Episode: \" + str("],[1,"\n    #print(\"Episode:\", "],[0,"epis"]],"start1":2072,"start2":2072,"length1":37,"length2":32},{"diffs":[[0,"de+1"],[-1,") +"],[1,","],[0," \"Ep"]],"start1":2105,"start2":2105,"length1":11,"length2":9},{"diffs":[[0,"lon:"],[-1," \" + str(epsilon)\n    print(printstr"],[1,"\", round(epsilon, 4)"],[0,")\n\n\n"]],"start1":2116,"start2":2116,"length1":44,"length2":28}]}]},{"timestamp":1740669310798,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[1,"from microbit import *\n\n\n# Explicit Q-table initialization\nQ_table = {}\n\n# Helper function to create a key\ndef get_key(x, y):\n    return str(x) + \":\" + str(y)\n\n\n# Initialize Q-table explicitly\nfor y in range(5):\n    for x in range(5):\n        Q_table[get_key(x, y)] = [0, 0, 0, 0]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Q-learning parameters\nalpha = 0.1\ngamma = 0.9\nepsilon = 1.0\nepsilon_min = 0.05\nepsilon_decay = 0.98\n\ngoal_x, goal_y = 3, 4\n\n# Training episodes\nfor episode in range(100):\n    robot_x, robot_y = 1, 1\n\n    while (robot_x, robot_y) != (goal_x, goal_y):\n        # Exploration or Exploitation\n        if Math.random() < epsilon:\n            action_index = randint(0, 3)\n        else:\n            key = get_key(robot_x, robot_y)\n            q_values = Q_table[key]\n            action_index = 0\n            max_q = q_values[0]\n            for i in range(1, 4):\n                if q_values[i] > max_q:\n                    max_q = q_values[i]\n                    action_index = i\n\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        if maze[new_y][new_x] == 1:\n            reward = -5\n            new_x, new_y = robot_x, robot_y\n        elif (new_x, new_y) == (goal_x, goal_y):\n            reward = 10\n        else:\n            reward = -1\n\n        old_key = get_key(robot_x, robot_y)\n        new_key = get_key(new_x, new_y)\n\n        old_q_value = Q_table[old_key][action_index]\n\n        next_q_values = Q_table[new_key]\n        max_future_q = next_q_values[0]\n        for i in range(1, 4):\n            if next_q_values[i] > max_future_q:\n                max_future_q = next_q_values[i]\n\n        # Update Q-table explicitly\n        Q_table[old_key][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n\n        robot_x, robot_y = new_x, new_y\n\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    printstr = \"Episode: \" + str(episode+1) + \"Epsilon: \" + str(epsilon)\n    print(printstr)\n"],[0,"\n\n"]],"start1":0,"start2":0,"length1":2,"length2":2169}]}]},{"timestamp":1740670249006,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"let action_index: number;\nlet key: number;\nlet q_values: any;\nlet max_q: any;\nlet i: number;\nlet new_x: number;\nlet new_y: number;\nlet reward: number;\nlet old_key: number;\nlet new_key: number;\nlet old_q_value: number;\nlet next_q_values: any;\nlet max_future_q: number;\nlet printstr: any;\n//  Explicit Q-table initialization\nlet Q_table =  {\n\t\n}\n\n//  Helper function to create a key\nfunction get_key(x: number, y: number): number {\n    return \"\" + x + \":\" + (\"\" + y)\n}\n\n//  Initialize Q-table explicitly\nfor (let y = 0; y < 5; y++) {\n    for (let x = 0; x < 5; x++) {\n        Q_table[get_key(x, y)] = [0, 0, 0, 0]\n    }\n}\n//  Define possible moves\nlet actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Q-learning parameters\nlet alpha = 0.1\nlet gamma = 0.9\nlet epsilon = 1.0\nlet epsilon_min = 0.05\nlet epsilon_decay = 0.98\nlet [goal_x, goal_y] = [3, 4]\n//  Training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    let [robot_x, robot_y] = [1, 1]\n    while ([robot_x, robot_y] != [goal_x, goal_y]) {\n        //  Exploration or Exploitation\n        if (Math.random() < epsilon) {\n            action_index = randint(0, 3)\n        } else {\n            key = get_key(robot_x, robot_y)\n            q_values = Q_table[key]\n            action_index = 0\n            max_q = q_values[0]\n            for (i = 1; i < 4; i++) {\n                if (q_values[i] > max_q) {\n                    max_q = q_values[i]\n                    action_index = i\n                }\n                \n            }\n        }\n        \n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n        if (maze[new_y][new_x] == 1) {\n            reward = -5\n            let [new_x, new_y] = [robot_x, robot_y]\n        } else if ([new_x, new_y] == [goal_x, goal_y]) {\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        old_key = get_key(robot_x, robot_y)\n        new_key = get_key(new_x, new_y)\n        old_q_value = Q_table[old_key][action_index]\n        next_q_values = Q_table[new_key]\n        max_future_q = next_q_values[0]\n        for (i = 1; i < 4; i++) {\n            if (next_q_values[i] > max_future_q) {\n                max_future_q = next_q_values[i]\n            }\n            \n        }\n        //  Update Q-table explicitly\n        Q_table[old_key][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n        let [robot_x, robot_y] = [new_x, new_y]\n    }\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n    printstr = \"Episode: \" + (\"\" + (episode + 1)) + \"Epsilon: \" + (\"\" + epsilon)\n    console.log(printstr)\n}"],[0,"\n"]],"start1":0,"start2":0,"length1":1,"length2":2758}]}]},{"timestamp":1740670249077,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"let action_index: number;\nlet q_values: any;\nlet max_q: any;\nlet i: number;\nlet new_x: number;\nlet new_y: number;\nlet reward: number;\nlet old_q_value: number;\nlet next_q_values: any;\nlet max_future_q: number;\n//  Define Q-table as a 5x5 grid of lists (compatible with MakeCode Python)\nlet Q_table =  {TODO: ListComp} \n//  Define possible moves\nlet actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\n//  Define the maze (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay = 0.98\n//  Multiply epsilon by this after each episode\nlet [goal_x, goal_y] = [3, 4]\n//  Goal position\n//  Run multiple training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    let [robot_x, robot_y] = [1, 1]\n    //  Reset position each episode\n    while ([robot_x, robot_y] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        //  Exploration vs Exploitation\n        if (Math.random() < epsilon) {\n            action_index = randint(0, 3)\n        } else {\n            //  Explore\n            //  Exploit (choose best action)\n            q_values = Q_table[robot_y][robot_x]\n            //  Access Q-values directly\n            action_index = 0\n            max_q = q_values[0]\n            for (i = 1; i < 4; i++) {\n                if (q_values[i] > max_q) {\n                    max_q = q_values[i]\n                    action_index = i\n                }\n                \n            }\n        }\n        \n        //  Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n        //  Check if move is valid\n        if (maze[new_y][new_x] == 1) {\n            //  Hit a wall\n            reward = -5\n            let [new_x, new_y] = [robot_x, robot_y]\n        } else if ([new_x, new_y] == [goal_x, goal_y]) {\n            //  Stay in place\n            //  Reached goal\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        //  Normal movement\n        //  Q-value update\n        old_q_value = Q_table[robot_y][robot_x][action_index]\n        next_q_values = Q_table[new_y][new_x]\n        max_future_q = next_q_values[0]\n        for (i = 1; i < 4; i++) {\n            if (next_q_values[i] > max_future_q) {\n                max_future_q = next_q_values[i]\n            }\n            \n        }\n        Q_table[robot_y][robot_x][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n        //  Move to new position\n        let [robot_x, robot_y] = [new_x, new_y]\n    }\n    //  Reduce epsilon after each episode (exponential decay)\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n}"],[0,"\n"]],"start1":0,"start2":0,"length1":2929,"length2":1}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[-1,"from microbit import *\n\n\n# Define Q-table as a 5x5 grid of lists (compatible with MakeCode Python)\nQ_table = [[[0, 0, 0, 0] for _ in range(5)] for _ in range(5)]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\ngoal_x, goal_y = 3, 4  # Goal position\n\n# Run multiple training episodes\nfor episode in range(100):\n    robot_x, robot_y = 1, 1  # Reset position each episode\n\n    while (robot_x, robot_y) != (goal_x, goal_y):  # Until goal is reached\n        # Exploration vs Exploitation\n        if Math.random() < epsilon:\n            action_index = randint(0, 3)  # Explore\n        else:\n            # Exploit (choose best action)\n            q_values = Q_table[robot_y][robot_x]  # Access Q-values directly\n            action_index = 0\n            max_q = q_values[0]\n            for i in range(1, 4):\n                if q_values[i] > max_q:\n                    max_q = q_values[i]\n                    action_index = i\n\n        # Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        # Check if move is valid\n        if maze[new_y][new_x] == 1:  # Hit a wall\n            reward = -5\n            new_x, new_y = robot_x, robot_y  # Stay in place\n        elif (new_x, new_y) == (goal_x, goal_y):  # Reached goal\n            reward = 10\n        else:\n            reward = -1  # Normal movement\n\n        # Q-value update\n        old_q_value = Q_table[robot_y][robot_x][action_index]\n        next_q_values = Q_table[new_y][new_x]\n        max_future_q = next_q_values[0]\n\n        for i in range(1, 4):\n            if next_q_values[i] > max_future_q:\n                max_future_q = next_q_values[i]\n\n        Q_table[robot_y][robot_x][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n\n        # Move to new position\n        robot_x, robot_y = new_x, new_y\n\n    # Reduce epsilon after each episode (exponential decay)\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    #print(\"Episode:\", episode+1, \"Epsilon:\", round(epsilon, 4))\n"],[0,"\n\n"]],"start1":0,"start2":0,"length1":2495,"length2":2}]}]},{"timestamp":1740670286326,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[1,"from microbit import *\n\n\n# Define Q-table as a 5x5 grid of lists (compatible with MakeCode Python)\nQ_table = [[[0, 0, 0, 0] for _ in range(5)] for _ in range(5)]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\ngoal_x, goal_y = 3, 4  # Goal position\n\n# Run multiple training episodes\nfor episode in range(100):\n    robot_x, robot_y = 1, 1  # Reset position each episode\n\n    while (robot_x, robot_y) != (goal_x, goal_y):  # Until goal is reached\n        # Exploration vs Exploitation\n        if Math.random() < epsilon:\n            action_index = randint(0, 3)  # Explore\n        else:\n            # Exploit (choose best action)\n            q_values = Q_table[robot_y][robot_x]  # Access Q-values directly\n            action_index = 0\n            max_q = q_values[0]\n            for i in range(1, 4):\n                if q_values[i] > max_q:\n                    max_q = q_values[i]\n                    action_index = i\n\n        # Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        # Check if move is valid\n        if maze[new_y][new_x] == 1:  # Hit a wall\n            reward = -5\n            new_x, new_y = robot_x, robot_y  # Stay in place\n        elif (new_x, new_y) == (goal_x, goal_y):  # Reached goal\n            reward = 10\n        else:\n            reward = -1  # Normal movement\n\n        # Q-value update\n        old_q_value = Q_table[robot_y][robot_x][action_index]\n        next_q_values = Q_table[new_y][new_x]\n        max_future_q = next_q_values[0]\n\n        for i in range(1, 4):\n            if next_q_values[i] > max_future_q:\n                max_future_q = next_q_values[i]\n\n        Q_table[robot_y][robot_x][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n\n        # Move to new position\n        robot_x, robot_y = new_x, new_y\n\n    # Reduce epsilon after each episode (exponential decay)\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    #print(\"Episode:\", episode+1, \"Epsilon:\", round(epsilon, 4))"],[0,"\n\n\n"]],"start1":0,"start2":0,"length1":3,"length2":2495}]}]},{"timestamp":1740695682948,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"let action_index: number;\nlet q_values: any;\nlet max_q: any;\nlet i: number;\nlet new_x: number;\nlet new_y: number;\nlet reward: number;\nlet old_q_value: number;\nlet next_q_values: any;\nlet max_future_q: number;\n//  Define Q-table as a 5x5 grid of lists (compatible with MakeCode Python)\nlet Q_table =  {TODO: ListComp} \n//  Define possible moves\nlet actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\n//  Define the maze (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay = 0.98\n//  Multiply epsilon by this after each episode\nlet [goal_x, goal_y] = [3, 4]\n//  Goal position\n//  Run multiple training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    let [robot_x, robot_y] = [1, 1]\n    //  Reset position each episode\n    while ([robot_x, robot_y] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        //  Exploration vs Exploitation\n        if (Math.random() < epsilon) {\n            action_index = randint(0, 3)\n        } else {\n            //  Explore\n            //  Exploit (choose best action)\n            q_values = Q_table[robot_y][robot_x]\n            //  Access Q-values directly\n            action_index = 0\n            max_q = q_values[0]\n            for (i = 1; i < 4; i++) {\n                if (q_values[i] > max_q) {\n                    max_q = q_values[i]\n                    action_index = i\n                }\n                \n            }\n        }\n        \n        //  Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n        //  Check if move is valid\n        if (maze[new_y][new_x] == 1) {\n            //  Hit a wall\n            reward = -5\n            let [new_x, new_y] = [robot_x, robot_y]\n        } else if ([new_x, new_y] == [goal_x, goal_y]) {\n            //  Stay in place\n            //  Reached goal\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        //  Normal movement\n        //  Q-value update\n        old_q_value = Q_table[robot_y][robot_x][action_index]\n        next_q_values = Q_table[new_y][new_x]\n        max_future_q = next_q_values[0]\n        for (i = 1; i < 4; i++) {\n            if (next_q_values[i] > max_future_q) {\n                max_future_q = next_q_values[i]\n            }\n            \n        }\n        Q_table[robot_y][robot_x][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n        //  Move to new position\n        let [robot_x, robot_y] = [new_x, new_y]\n    }\n    //  Reduce epsilon after each episode (exponential decay)\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n}"],[0,"\n"]],"start1":0,"start2":0,"length1":1,"length2":2929}]}]},{"timestamp":1740695683002,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[-1,"from microbit import *\n\n\n# Define Q-table as a 5x5 grid where each entry is a list of 4 Q-values: [Up, Down, Left, Right]\nQtable = [\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]]\n]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Agent starting position\nAx_0 = 0\n# Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\ngoal_x, goal_y = 3, 4  # Goal position\n\n# Run multiple training episodes\nfor episode in range(100):  # 100 training episodes\n    robot_x = 0\n    robot_y = 0  # Reset position each episode\n\n    while (robot_x, robot_y) != (goal_x, goal_y):  # Until goal is reached\n        # Exploration vs Exploitation\n        if Math.random() < epsilon:\n            action_index = randint(0, 3)  # Explore (0,1,2,3)\n        else:\n            qvals = Qtable[1][2]\n            q_values = Qtable[robot_y][robot_x]  # Get Q-values\n            action_index = q_values.index(max(q_values))  # Choose best action\n\n        # Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        # Check if move is valid\n        if maze[new_y][new_x] == 1:  # Hit a wall\n            reward = -5\n            new_x, new_y = robot_x, robot_y  # Stay in place\n        elif (new_x, new_y) == (goal_x, goal_y):  # Reached goal\n            reward = 10\n        else:\n            reward = -1  # Normal movement\n\n        # Q-value update\n        old_q_value = Qtable[robot_y][robot_x][action_index]\n        max_future_q = max(Qtable[new_y][new_x])  # Best future Q-value\n        new_q_value = old_q_value + alpha * (reward + gamma * max_future_q - old_q_value)\n\n        # Update Q-table\n        Qtable[robot_y][robot_x][action_index] = round(new_q_value, 2)  # Store rounded Q-value\n\n        # Move to new position\n        robot_x, robot_y = new_x, new_y\n\n    # Reduce epsilon after each episode (exponential decay)\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    #print(\"Episode:\", episode+1, \"Epsilon:\", round(epsilon, 4))"],[1,"\n\n\n"]],"start1":0,"start2":0,"length1":2651,"length2":3}]}]},{"timestamp":1740696281384,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"= 0\n"],[-1,"Ay_0 = randint(1, 3)\n\n"],[0,"# Q-"]],"start1":744,"start2":744,"length1":30,"length2":8},{"diffs":[[0,"    "],[-1,"Ax = Ax_0 \n    A"],[1,"robot_x = 0\n    robot_"],[0,"y = "],[-1,"Ay_"],[0,"0  #"]],"start1":1118,"start2":1118,"length1":31,"length2":34},{"diffs":[[0,"set "],[-1,"agent starting "],[0,"posi"]],"start1":1155,"start2":1155,"length1":23,"length2":8},{"diffs":[[0,"le ("],[-1,"Ax, A"],[1,"robot_x, robot_"],[0,"y) !"]],"start1":1189,"start2":1189,"length1":13,"length2":23},{"diffs":[[0,"    "],[-1,"Qvalues = Qtable[Ax][Ay]"],[1,"qvals = Qtable[1][2]\n            q_values = Qtable[robot_y][robot_x]  # Get Q-values"],[0,"\n   "]],"start1":1415,"start2":1415,"length1":32,"length2":92},{"diffs":[[0,"        "],[-1,"#"],[0,"action_i"]],"start1":1508,"start2":1508,"length1":17,"length2":16},{"diffs":[[0,"index = "],[-1,"Q"],[1,"q_"],[0,"values.i"]],"start1":1523,"start2":1523,"length1":17,"length2":18},{"diffs":[[0,"dex(max("],[-1,"Q"],[1,"q_"],[0,"values))"]],"start1":1542,"start2":1542,"length1":17,"length2":18},{"diffs":[[0,"ion\n"],[-1,"            while i   "],[0,"\n   "]],"start1":1579,"start2":1579,"length1":30,"length2":8},{"diffs":[[0,"new_x = "],[-1,"A"],[1,"robot_"],[0,"x + move"]],"start1":1625,"start2":1625,"length1":17,"length2":22},{"diffs":[[0,"new_y = "],[-1,"A"],[1,"robot_"],[0,"y + move"]],"start1":1672,"start2":1672,"length1":17,"length2":22}]}]},{"timestamp":1740696879516,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"maxQvalue = Qvalues[1]\n            for x in Qvalues:\n                if x > maxQvalue:\n                    maxQvalue = x\n                   "],[1,"while i"],[0,"   \n"]],"start1":1554,"start2":1554,"length1":148,"length2":15}]}]},{"timestamp":1740697206810,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"Qvalues["],[-1,"0"],[1,"1"],[0,"]\n      "]],"start1":1570,"start2":1570,"length1":17,"length2":17},{"diffs":[[0,"        "],[1,"  "],[0,"if x > m"]],"start1":1617,"start2":1617,"length1":16,"length2":18},{"diffs":[[0,"Qvalue:\n"],[1,"    "],[0,"        "]],"start1":1637,"start2":1637,"length1":16,"length2":20},{"diffs":[[0,"    "],[-1,"Qindex = Qvalues.index(maxQvalue)\n"],[1,"          "],[0,"\n   "]],"start1":1687,"start2":1687,"length1":42,"length2":18},{"diffs":[[0," move_x["],[-1,"Q"],[1,"action_"],[0,"index]\n "]],"start1":1755,"start2":1755,"length1":17,"length2":23},{"diffs":[[0," move_y["],[-1,"Q"],[1,"action_"],[0,"index]\n\n"]],"start1":1797,"start2":1797,"length1":17,"length2":23}]}]},{"timestamp":1740697733381,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    new_"],[-1,"A"],[0,"x = Ax +"]],"start1":1757,"start2":1757,"length1":17,"length2":16},{"diffs":[[0,"    new_"],[-1,"A"],[0,"y = Ay +"]],"start1":1793,"start2":1793,"length1":17,"length2":16},{"diffs":[[0,"aze[new_"],[-1,"A"],[0,"y][new_"],[-1,"A"],[0,"x] == 1:"]],"start1":1871,"start2":1871,"length1":25,"length2":23},{"diffs":[[0,"new_"],[-1,"A"],[0,"x, new_"],[-1,"A = Ax, A"],[1,"y = robot_x, robot_"],[0,"y  #"]],"start1":1945,"start2":1945,"length1":25,"length2":34},{"diffs":[[0,"new_"],[-1,"A"],[0,"x, new_"],[-1,"A"],[0,"y) ="]],"start1":2008,"start2":2008,"length1":17,"length2":15},{"diffs":[[0,"ble["],[-1,"Ax][Ay][Q"],[1,"robot_y][robot_x][action_"],[0,"inde"]],"start1":2191,"start2":2191,"length1":17,"length2":33},{"diffs":[[0,"new_"],[-1,"A"],[0,"y][new_"],[-1,"A"],[0,"x]) "]],"start1":2261,"start2":2261,"length1":17,"length2":15},{"diffs":[[0,"ndex] = "],[1,"round("],[0,"new_q_va"]],"start1":2456,"start2":2456,"length1":16,"length2":22},{"diffs":[[0,"_q_value"],[1,", 2)"],[0,"  # Stor"]],"start1":2473,"start2":2473,"length1":16,"length2":20}]}]},{"timestamp":1740698117947,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ex]\n"],[-1,"                    #action_index = Qvalues.index(max(Qvalues))  # Choose best action\n                    maxQvalue = Qvalues[0]\n                    for x in Qvalues:\n                      if x > maxQvalue:\n                        maxQvalue = x\n                    Qindex = Qvalues.index(maxQvalue)\n"],[0,"    "]],"start1":2204,"start2":2204,"length1":307,"length2":8}]}]},{"timestamp":1740698716228,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"  else:\n"],[1,"            Qvalues = Qtable[Ax][Ay]\n            #action_index = Qvalues.index(max(Qvalues))  # Choose best action\n            maxQvalue = Qvalues[0]\n            for x in Qvalues:\n              if x > maxQvalue:\n                maxQvalue = x"],[0,"\n"],[-1,""],[0,"        "]],"start1":1423,"start2":1423,"length1":17,"length2":258},{"diffs":[[0,"Ax][Ay][Qindex]\n"],[1,"            "],[0,"        #action_"]],"start1":2192,"start2":2192,"length1":32,"length2":44},{"diffs":[[0,"    "],[-1,"max_future_q = Qtable[0][0][0]\n"],[1,"            maxQvalue = Qvalues[0]\n            "],[0,"    "]],"start1":2298,"start2":2298,"length1":39,"length2":55},{"diffs":[[0,"es:\n            "],[1," "],[1,"         "],[0,"if x > maxQvalue"]],"start1":2371,"start2":2371,"length1":32,"length2":42},{"diffs":[[0,"                "],[1,"  "],[1,"      "],[0,"maxQvalue = x\n  "]],"start1":2415,"start2":2415,"length1":32,"length2":40},{"diffs":[[0,"  maxQvalue = x\n"],[1,"            "],[0,"        Qindex ="]],"start1":2437,"start2":2437,"length1":32,"length2":44},{"diffs":[[0,"        "],[-1,"#"],[0,"max_futu"]],"start1":2507,"start2":2507,"length1":17,"length2":16},{"diffs":[[0,"ture_q ="],[-1,"    "],[0," max(Qta"]],"start1":2521,"start2":2521,"length1":20,"length2":16},{"diffs":[[0,"n_min)\n\n"],[1,""],[0,"    #pri"]],"start1":2965,"start2":2965,"length1":16,"length2":16},{"diffs":[[0," 4))"],[-1,"\n    def Find_MaxQvalue(x,y):\n        Qvalues = Qtable[x][y]\n        #action_index = Qvalues.index(max(Qvalues))  # Choose best action\n        maxQvalue = Qvalues[0]\n            for v in Qvalues:\n                if v > maxQvalue:\n                    maxQvalue = v\n        return maxQ"]],"start1":3033,"start2":3033,"length1":287,"length2":4}]}]},{"timestamp":1740699315911,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"let Ax: number;\nlet Ay: number;\nlet action_index: number;\nlet Qvalues: number[];\nlet maxQvalue: number;\nlet Qindex: number;\nlet new_Ax: number;\nlet new_Ay: number;\nlet reward: number;\nlet old_q_value: number;\nlet max_future_q: number;\nlet new_q_value: number;\n//  Define Q-table as a 5x5 grid where each entry is a list of 4 Q-values: [Up, Down, Left, Right]\nlet Qtable = [[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n//  Define possible moves\nlet actions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nlet move_x = [0, 0, -1, 1]\nlet move_y = [-1, 1, 0, 0]\n//  Define the maze (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Agent starting position\nlet Ax_0 = 0\nlet Ay_0 = randint(1, 3)\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay = 0.98\n//  Multiply epsilon by this after each episode\nlet [goal_x, goal_y] = [3, 4]\n//  Goal position\n//  Run multiple training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    //  100 training episodes\n    Ax = Ax_0\n    Ay = Ay_0\n    //  Reset agent starting position each episode\n    while ([Ax, Ay] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        //  Exploration vs Exploitation\n        if (Math.random() < epsilon) {\n            action_index = randint(0, 3)\n        } else {\n            //  Explore (0,1,2,3)\n            Qvalues = Qtable[Ax][Ay]\n            maxQvalue = Find_MaxQvalue(Qvalues)\n            // call to function\n            Qindex = _py.py_array_index(Qvalues, maxQvalue)\n        }\n        \n        //  Determine new position\n        new_Ax = Ax + move_x[Qindex]\n        new_Ay = Ay + move_y[Qindex]\n        //  Check if move is valid\n        if (maze[new_Ay][new_Ax] == 1) {\n            //  Hit a wall\n            reward = -5\n            let [new_Ax, new_A] = [Ax, Ay]\n        } else if ([new_Ax, new_Ay] == [goal_x, goal_y]) {\n            //  Stay in place\n            //  Reached goal\n            reward = 10\n        } else {\n            reward = -1\n        }\n        \n        //  Normal movement\n        //  Q-value update\n        old_q_value = Qtable[Ax][Ay][Qindex]\n        // action_index = Qvalues.index(max(Qvalues))  # Choose best action\n        max_future_q = Qtable[0][0][0]\n        for (let x of Qvalues) {\n            if (x > maxQvalue) {\n                maxQvalue = x\n            }\n            \n        }\n        Qindex = _py.py_array_index(Qvalues, maxQvalue)\n        // max_future_q =     max(Qtable[new_Ay][new_Ax])  # Best future Q-value\n        new_q_value = old_q_value + alpha * (reward + gamma * max_future_q - old_q_value)\n        //  Update Q-table\n        Qtable[Ax][Ay][Qindex] = new_q_value\n        //  Store rounded Q-value\n        //  Move to new position\n        let [Ax, Ay] = [new_Ax, new_Ay]\n    }\n    //  Reduce epsilon after each episode (exponential decay)\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n}\n// print(\"Episode:\", episode+1, \"Epsilon:\", round(epsilon, 4))\n//  function to find max Qvalue within a single element of the Qtable\nfunction Find_MaxQvalue(Qvalues: number[]): number {\n    // action_index = Qvalues.index(max(Qvalues))  # Choose best action\n    let maxQ = Qvalues[0]\n    for (let v of Qvalues) {\n        if (v > maxQvalue) {\n            maxQ = v\n        }\n        \n    }\n    return maxQ\n}\n"],[0,"\n"]],"start1":0,"start2":0,"length1":3798,"length2":1}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"se:\n"],[-1,"            Qvalues = Qtable[Ax][Ay]\n            maxQvalue = Find_MaxQvalue(Qvalues)  #call to function"],[0,"\n   "]],"start1":1427,"start2":1427,"length1":111,"length2":8},{"diffs":[[0,"     Qtable["],[-1,"Ax][Ay][Q"],[1,"robot_y][robot_x][action_"],[0,"index] = new"]],"start1":2406,"start2":2406,"length1":33,"length2":49},{"diffs":[[0,"    "],[-1,"Ax, A"],[1,"robot_x, robot_"],[0,"y = new_"],[-1,"A"],[0,"x, new_"],[-1,"A"],[0,"y\n\n "]],"start1":2525,"start2":2525,"length1":30,"length2":38},{"diffs":[[0,"    "],[-1,"\n    # function to find m"],[1,"def Find_M"],[0,"ax"],[-1," "],[0,"Qvalue"],[-1," within a single element of the Qtable\n    \ndef Find_MaxQvalue(Qvalues):\n"],[1,"(x,y):\n        Qvalues = Qtable[x][y]\n    "],[0,"    "]],"start1":2744,"start2":2744,"length1":115,"length2":68},{"diffs":[[0,"ion\n    "],[-1,"maxQ"],[1,"    maxQvalue"],[0," = Qvalu"]],"start1":2874,"start2":2874,"length1":20,"length2":29},{"diffs":[[0," Qvalues[0]\n"],[1,"        "],[0,"    for v in"]],"start1":2897,"start2":2897,"length1":24,"length2":32},{"diffs":[[0,"        "],[1,"        "],[0,"if v > m"]],"start1":2939,"start2":2939,"length1":16,"length2":24},{"diffs":[[0,"    "],[-1,"maxQ"],[1,"        maxQvalue"],[0," = v\n"],[1,"    "],[0,"    "]],"start1":2981,"start2":2981,"length1":17,"length2":34}]}]},{"timestamp":1740699732138,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"ure_q = "],[-1,"Find_MaxQvalue_Matrix()"],[1,"Qtable[0][0][0]"],[0,"\n       "]],"start1":2697,"start2":2697,"length1":39,"length2":31},{"diffs":[[0,"\n}\n\n"],[-1,"function Find_MaxQvalue_Matrix(): number {\n    let maxQ = Qtable[0][0][0]\n    for (let v of Qvalues) {\n        if (v > maxQvalue) {\n            maxQ = v\n        }\n        \n    }\n}\n\n"]],"start1":3794,"start2":3794,"length1":185,"length2":4}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ion\n"],[-1,"        \n"],[0,"    "]],"start1":2140,"start2":2140,"length1":17,"length2":8},{"diffs":[[0,"ure_q = "],[-1,"Find_MaxQvalue_Matrix()"],[1,"Qtable[0][0][0]"],[0,"\n       "]],"start1":2159,"start2":2159,"length1":39,"length2":31},{"diffs":[[0,"maxQ"],[-1,"\n\ndef Find_MaxQvalue_Matrix():\n    maxQ = Qtable[0][0][0]\n    for v in Qvalues:\n            if v > maxQvalue:\n                maxQ = v\n                for i in range(rows):\n                    for j in range(cols):\n                        print(matrix[i][j], end=\" \")\n                    print()"]],"start1":3106,"start2":3106,"length1":299,"length2":4}]}]},{"timestamp":1740700329630,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," Qtable\n"],[-1,"//  "],[0,"function"]],"start1":3524,"start2":3524,"length1":20,"length2":16},{"diffs":[[0,"ion "],[-1,"to find the max Qvalue within single element of the Qtable\nfun"],[1,"Find_MaxQvalue(Qvalues: number[]): number {\n    // a"],[0,"ction"],[-1," Find_Max"],[1,"_index = "],[0,"Qvalue"],[-1,"(Qvalues: number[]): number {"],[1,"s.index(max(Qvalues))  # Choose best action"],[0,"\n   "]],"start1":3537,"start2":3537,"length1":119,"length2":123},{"diffs":[[0,"\n}\n\n"],[-1,"//  function to find the max Qvalue within the whole Qtable\nfunction Find_MaxQvalue_Matrix(): number {\n    let maxQ_element: number;\n    let maxQ_table = Qtable[0][0][0]\n    for (let i = 0; i < 5; i++) {\n        for (let j = 0; j < 5; j++) {\n            maxQ_element = Find_MaxQvalue(Qtable[i][j])\n            if (maxQ_element"],[1,"function Find_MaxQvalue_Matrix(): number {\n    let maxQ = Qtable[0][0][0]\n    for (let v of Qvalues) {\n        if (v"],[0," > maxQ"],[-1,"_tabl"],[1,"valu"],[0,"e) {"]],"start1":3802,"start2":3802,"length1":346,"length2":135},{"diffs":[[0,"    "],[-1,"    maxQ_table = maxQ_element\n            }\n            \n        }\n    }\n    return maxQ_table"],[1,"maxQ = v\n        }\n        \n    }"],[0,"\n}\n\n"]],"start1":3946,"start2":3946,"length1":102,"length2":41}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"   \n"],[-1,"# function to find the m"],[1,"def Find_M"],[0,"ax"],[-1," "],[0,"Qvalue"],[-1," within single element of the Qtable\ndef Find_MaxQvalue(Qvalues):"],[1,"(Qvalues):\n    #action_index = Qvalues.index(max(Qvalues))  # Choose best action"],[0,"\n   "]],"start1":2918,"start2":2918,"length1":106,"length2":106},{"diffs":[[0,"xQ\n\n"],[-1,"# function to find the max Qvalue within the whole Qtable\n"],[0,"def "]],"start1":3125,"start2":3125,"length1":66,"length2":8},{"diffs":[[0,"    maxQ"],[-1,"_table"],[0," = Qtabl"]],"start1":3158,"start2":3158,"length1":22,"length2":16},{"diffs":[[0,"for "],[-1,"i"],[1,"v"],[0," in "],[-1,"range(5)"],[1,"Qvalues"],[0,":\n  "]],"start1":3189,"start2":3189,"length1":21,"length2":20},{"diffs":[[0,"    "],[-1,"for j in range(5):\n            maxQ_element =  Find_MaxQvalue(Qtable[i][j])\n            if maxQ_element > maxQ_table:\n                maxQ_table = maxQ_element\n    return maxQ_table\n"],[1,"    if v > maxQvalue:\n                maxQ = v\n                for i in range(rows):\n                    for j in range(cols):\n                        print(matrix[i][j], end=\" \")\n                    print()"]],"start1":3211,"start2":3211,"length1":186,"length2":211}]}]},{"timestamp":1740700716482,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"alue"],[-1,"(Qtable[new_Ax][new_Ay]"],[1,"_Matrix("],[0,")\n  "]],"start1":2715,"start2":2715,"length1":31,"length2":16},{"diffs":[[0,"maxQ\n}\n\n"],[1,"//  function to find the max Qvalue within the whole Qtable\nfunction Find_MaxQvalue_Matrix(): number {\n    let maxQ_element: number;\n    let maxQ_table = Qtable[0][0][0]\n    for (let i = 0; i < 5; i++) {\n        for (let j = 0; j < 5; j++) {\n            maxQ_element = Find_MaxQvalue(Qtable[i][j])\n            if (maxQ_element > maxQ_table) {\n                maxQ_table = maxQ_element\n            }\n            \n        }\n    }\n    return maxQ_table\n}\n\n"]],"start1":3798,"start2":3798,"length1":8,"length2":461}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"alue"],[-1,"(Qtable[new_Ax][new_Ay]"],[1,"_Matrix("],[0,")\n  "]],"start1":2186,"start2":2186,"length1":31,"length2":16},{"diffs":[[0,"n maxQ\n\n"],[1,"# function to find the max Qvalue within the whole Qtable\ndef Find_MaxQvalue_Matrix():\n    maxQ_table = Qtable[0][0][0]\n    for i in range(5):\n        for j in range(5):\n            maxQ_element =  Find_MaxQvalue(Qtable[i][j])\n            if maxQ_element > maxQ_table:\n                maxQ_table = maxQ_element\n    return maxQ_table\n"]],"start1":3121,"start2":3121,"length1":8,"length2":341}]}]},{"timestamp":1740701093079,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"(1, 3)\n\n"],[-1,"\n"],[0,"# Q-lear"]],"start1":762,"start2":762,"length1":17,"length2":16}]}]},{"timestamp":1740702044796,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"t(1, 3)\n"],[-1,"Ax"],[0,"\n\n# Q-le"]],"start1":761,"start2":761,"length1":18,"length2":16}]}]},{"timestamp":1740702046486,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"Qvalue: number;\n"],[1,"let Qindex: number;\n"],[0,"let new_Ax: numb"]],"start1":88,"start2":88,"length1":32,"length2":52},{"diffs":[[0,"ion\n"],[-1,"//  Function to find the max Q-value in a state\nfunction Find_MaxQvalue(Qvalues: number[]): number {\n    let maxQ = Qvalues[0]\n    for (let v of Qvalues) {\n        if (v > maxQ) {\n            maxQ = v\n        }\n        \n    }\n    return maxQ\n}\n\n"],[0,"//  "]],"start1":1383,"start2":1383,"length1":253,"length2":8},{"diffs":[[0,"es)\n            "],[-1,"action_"],[1,"// call to function\n            Q"],[0,"index = _py.py_a"]],"start1":1910,"start2":1910,"length1":39,"length2":65},{"diffs":[[0," move_x["],[-1,"action_"],[1,"Q"],[0,"index]\n "]],"start1":2081,"start2":2081,"length1":23,"length2":17},{"diffs":[[0," move_y["],[-1,"action_"],[1,"Q"],[0,"index]\n "]],"start1":2118,"start2":2118,"length1":23,"length2":17},{"diffs":[[0,"x, new_A"],[-1,"y"],[0,"] = [Ax,"]],"start1":2283,"start2":2283,"length1":17,"length2":16},{"diffs":[[0,"Ay]["],[-1,"action_index]\n        max_future_q = Find_MaxQvalue(Qtable[new_Ax][new_Ay])"],[1,"Qindex]\n        // action_index = Qvalues.index(max(Qvalues))  # Choose best action\n        max_future_q = Find_MaxQvalue(Qtable[new_Ax][new_Ay])\n        for (let x of Qvalues) {\n            if (x > maxQvalue) {\n                maxQvalue = x\n            }\n            \n        }\n        Qindex = _py.py_array_index(Qvalues, maxQvalue)\n        // max_future_q =     max(Qtable[new_Ay][new_Ax])  # Best future Q-value"],[0,"\n   "]],"start1":2594,"start2":2594,"length1":83,"length2":423},{"diffs":[[0,"Ax][Ay]["],[-1,"action_"],[1,"Q"],[0,"index] ="]],"start1":3146,"start2":3146,"length1":23,"length2":17},{"diffs":[[0,"new_q_value\n"],[1,"        //  Store rounded Q-value\n"],[0,"        //  "]],"start1":3164,"start2":3164,"length1":24,"length2":58},{"diffs":[[0,"_min)\n}\n"],[1,"// print(\"Episode:\", episode+1, \"Epsilon:\", round(epsilon, 4))\n//  function to find max Qvalue within a single element of the Qtable\n//  function to find the max Qvalue within single element of the Qtable\nfunction Find_MaxQvalue(Qvalues: number[]): number {\n    let maxQ = Qvalues[0]\n    for (let v of Qvalues) {\n        if (v > maxQvalue) {\n            maxQ = v\n        }\n        \n    }\n    return maxQ\n}\n\n"]],"start1":3406,"start2":3406,"length1":8,"length2":415}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"port *\n\n"],[1,"\n"],[0,"# Define"]],"start1":16,"start2":16,"length1":16,"length2":17},{"diffs":[[0,"nt(1, 3)"],[1,"\nAx"],[0,"\n\n# Q-le"]],"start1":760,"start2":760,"length1":16,"length2":19},{"diffs":[[0,"on\n\n"],[-1,"# Function to find the max Q-value in a state\ndef Find_MaxQvalue(Qvalues):\n    maxQ = Qvalues[0]\n    for v in Qvalues:\n        if v > maxQ:\n            maxQ = v\n    return maxQ\n\n"],[0,"# Ru"]],"start1":1054,"start2":1054,"length1":186,"length2":8},{"diffs":[[0,"x = Ax_0"],[1," "],[0,"\n    Ay "]],"start1":1148,"start2":1148,"length1":16,"length2":17},{"diffs":[[0,"Qvalues)"],[1,"  #call to function"],[0,"\n       "]],"start1":1510,"start2":1510,"length1":16,"length2":35},{"diffs":[[0,"ion\n            "],[-1,"action_"],[1,"Q"],[0,"index = Qvalues."]],"start1":1534,"start2":1534,"length1":39,"length2":33},{"diffs":[[0," move_x["],[-1,"action_"],[1,"Q"],[0,"index]\n "]],"start1":1639,"start2":1639,"length1":23,"length2":17},{"diffs":[[0," move_y["],[-1,"action_"],[1,"Q"],[0,"index]\n\n"]],"start1":1676,"start2":1676,"length1":23,"length2":17},{"diffs":[[0,"ew_Ax, new_A"],[-1,"y"],[0," = Ax, Ay  #"]],"start1":1815,"start2":1815,"length1":25,"length2":24},{"diffs":[[0,"Ay]["],[-1,"action_index]\n        max_future_q = Find_MaxQvalue(Qtable[new_Ax][new_Ay])\n"],[1,"Qindex]\n        #action_index = Qvalues.index(max(Qvalues))  # Choose best action\n        \n        max_future_q = Find_MaxQvalue(Qtable[new_Ax][new_Ay])\n        for x in Qvalues:\n            if x > maxQvalue:\n                maxQvalue = x\n        Qindex = Qvalues.index(maxQvalue)\n        #max_future_q =     max(Qtable[new_Ay][new_Ax])  # Best future Q-value"],[0,"\n   "]],"start1":2061,"start2":2061,"length1":84,"length2":367},{"diffs":[[0,"Ax][Ay]["],[-1,"action_"],[1,"Q"],[0,"index] ="]],"start1":2556,"start2":2556,"length1":23,"length2":17},{"diffs":[[0,"_q_value"],[1,"  # Store rounded Q-value"],[0,"\n\n      "]],"start1":2577,"start2":2577,"length1":16,"length2":41},{"diffs":[[0,"on, 4))\n"],[1,"    \n    # function to find max Qvalue within a single element of the Qtable\n    \n# function to find the max Qvalue within single element of the Qtable\ndef Find_MaxQvalue(Qvalues):\n    maxQ = Qvalues[0]\n    for v in Qvalues:\n        if v > maxQvalue:\n            maxQ = v\n    return maxQ\n\n"]],"start1":2850,"start2":2850,"length1":8,"length2":297}]}]},{"timestamp":1740702469071,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"rt *"],[-1,"\nimport random  # Import random module"],[0,"\n\n# "]],"start1":18,"start2":18,"length1":46,"length2":8},{"diffs":[[0,"0 = "],[-1,"1"],[1,"0"],[0,"\nAy_0 = "],[-1,"random."],[0,"rand"]],"start1":741,"start2":741,"length1":24,"length2":17},{"diffs":[[0,"     if "],[-1,"random"],[1,"Math"],[0,".random("]],"start1":1498,"start2":1498,"length1":22,"length2":20},{"diffs":[[0,"lon:"],[-1,"  # Use random module"],[0,"\n   "]],"start1":1526,"start2":1526,"length1":29,"length2":8},{"diffs":[[0,"x = "],[-1,"random."],[0,"rand"]],"start1":1554,"start2":1554,"length1":15,"length2":8},{"diffs":[[0,"le[A"],[-1,"y][Ax]  # Correct indexing row-first"],[1,"x][Ay]"],[0,"\n   "]],"start1":1633,"start2":1633,"length1":44,"length2":14},{"diffs":[[0,"value = Qtable[A"],[-1,"y][Ax"],[1,"x][Ay"],[0,"][action_index]\n"]],"start1":2215,"start2":2215,"length1":37,"length2":37},{"diffs":[[0,"le[new_A"],[-1,"y"],[1,"x"],[0,"][new_A"],[-1,"x"],[1,"y"],[0,"])\n\n    "]],"start1":2294,"start2":2294,"length1":25,"length2":25},{"diffs":[[0,"le[A"],[-1,"y][Ax"],[1,"x][Ay"],[0,"][ac"]],"start1":2443,"start2":2443,"length1":13,"length2":13},{"diffs":[[0,"n)\n\n    "],[1,"#"],[0,"print(\"E"]],"start1":2660,"start2":2660,"length1":16,"length2":17}]}]},{"timestamp":1740711550425,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\nAy_0 = "],[1,"random."],[0,"randint("]],"start1":784,"start2":784,"length1":16,"length2":23}]}]},{"timestamp":1740711559957,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," Ax_0 = "],[-1,"1"],[1,"0"],[0,"\nlet Ay_"]],"start1":1013,"start2":1013,"length1":17,"length2":17},{"diffs":[[0,") {\n"],[-1,"            //  Use random module\n"],[0,"    "]],"start1":1958,"start2":1958,"length1":42,"length2":8},{"diffs":[[0,"le[A"],[-1,"y][Ax"],[1,"x][Ay"],[0,"]\n"],[-1,"            //  Correct indexing row-first\n"],[0,"    "]],"start1":2080,"start2":2080,"length1":58,"length2":15},{"diffs":[[0,"value = Qtable[A"],[-1,"y][Ax"],[1,"x][Ay"],[0,"][action_index]\n"]],"start1":2787,"start2":2787,"length1":37,"length2":37},{"diffs":[[0,"le[new_A"],[-1,"y"],[1,"x"],[0,"][new_A"],[-1,"x"],[1,"y"],[0,"])\n     "]],"start1":2866,"start2":2866,"length1":25,"length2":25},{"diffs":[[0,"le[A"],[-1,"y][Ax"],[1,"x][Ay"],[0,"][ac"]],"start1":3015,"start2":3015,"length1":13,"length2":13}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"mport *\n"],[1,"import random  # Import random module"],[0,"\n\n# Defi"]],"start1":15,"start2":15,"length1":16,"length2":53},{"diffs":[[0," if "],[-1,"Math"],[1,"random"],[0,".ran"]],"start1":1540,"start2":1540,"length1":12,"length2":14},{"diffs":[[0,"index = "],[1,"random."],[0,"randint("]],"start1":1611,"start2":1611,"length1":16,"length2":23},{"diffs":[[0,"n)\n\n    "],[-1,"#"],[0,"print(\"E"]],"start1":2758,"start2":2758,"length1":17,"length2":16}]}]},{"timestamp":1740711630627,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"t *\n"],[-1,"import random  # Import random module for randint and random.random()"],[0,"\n"],[1,""],[0,"\n# D"]],"start1":19,"start2":19,"length1":78,"length2":9},{"diffs":[[0,"\nAy_0 = "],[-1,"random."],[0,"randint("]],"start1":747,"start2":747,"length1":23,"length2":16},{"diffs":[[0,", 3)"],[-1,"  # Use random.randint"],[0,"\n\n# "]],"start1":764,"start2":764,"length1":30,"length2":8},{"diffs":[[0,"     if "],[-1,"random"],[1,"Math"],[0,".random("]],"start1":1499,"start2":1499,"length1":22,"length2":20},{"diffs":[[0,"e random"],[-1,".random()"],[1," module"],[0,"\n       "]],"start1":1537,"start2":1537,"length1":25,"length2":23},{"diffs":[[0,"x = "],[-1,"random."],[0,"rand"]],"start1":1576,"start2":1576,"length1":15,"length2":8},{"diffs":[[0," if "],[-1,"(new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4 or "],[0,"maze"]],"start1":1957,"start2":1957,"length1":65,"length2":8},{"diffs":[[0,"Ax] == 1"],[-1,")"],[0,":  # Hit"]],"start1":1978,"start2":1978,"length1":17,"length2":16},{"diffs":[[0,"wall"],[-1," or out of bounds"],[0,"\n   "]],"start1":1997,"start2":1997,"length1":25,"length2":8},{"diffs":[[0,"   #"],[-1," Uncomment to debug\n    # "],[0,"prin"]],"start1":2717,"start2":2717,"length1":34,"length2":8},{"diffs":[[0,"lon, 4))"],[1,"\n"]],"start1":2772,"start2":2772,"length1":8,"length2":9}]}]},{"timestamp":1740715795144,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"import *"],[1,"\nimport random  # Import random module for randint and random.random()"],[0,"\n\n# Defi"]],"start1":14,"start2":14,"length1":16,"length2":86},{"diffs":[[0,"\nAy_0 = "],[1,"random."],[0,"randint("]],"start1":816,"start2":816,"length1":16,"length2":23},{"diffs":[[0,"rand"],[-1,"int directly from microbi"],[1,"om.randin"],[0,"t\n\n#"]],"start1":852,"start2":852,"length1":33,"length2":17},{"diffs":[[0,"     if "],[1,"random."],[0,"random()"]],"start1":1597,"start2":1597,"length1":16,"length2":23},{"diffs":[[0,"ndom"],[-1,"() from microbit"],[1,".random()"],[0,"\n   "]],"start1":1641,"start2":1641,"length1":24,"length2":17},{"diffs":[[0,"index = "],[1,"random."],[0,"randint("]],"start1":1674,"start2":1674,"length1":16,"length2":23},{"diffs":[[0,"  # "],[-1,"display.scroll(\"Ep:\" + str(episode+1) + \" E:\" + str("],[1,"print(\"Episode:\", episode+1, \"Epsilon:\", "],[0,"roun"]],"start1":2927,"start2":2927,"length1":60,"length2":49},{"diffs":[[0,"lon, 4))"],[-1,")"]],"start1":2982,"start2":2982,"length1":9,"length2":8}]}]},{"timestamp":1740716022083,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," 3)\n"],[-1,"//  Use randint directly from microbit\n"],[0,"//  "]],"start1":1044,"start2":1044,"length1":47,"length2":8},{"diffs":[[0,"ndom"],[-1,"() from microbit"],[1," module"],[0,"\n   "]],"start1":1984,"start2":1984,"length1":24,"length2":15},{"diffs":[[0,"if ("],[-1,"new_Ax < 0 || new_Ax > 4 || new_Ay < 0 || new_Ay > 4 || "],[0,"maze"]],"start1":2465,"start2":2465,"length1":64,"length2":8},{"diffs":[[0,"wall"],[-1," or out of bounds"],[0,"\n   "]],"start1":2520,"start2":2520,"length1":25,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," if "],[-1,"Math."],[0,"rand"]],"start1":1540,"start2":1540,"length1":13,"length2":8}]}]},{"timestamp":1740716305890,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"nt(1, 3)"],[1,"  # Use randint directly from microbit"],[0,"\n\n# Q-le"]],"start1":759,"start2":759,"length1":16,"length2":54},{"diffs":[[0,"al_x"],[1,","],[0," "],[-1,"= 3\n"],[0,"goal_y ="],[1," 3,"],[0," 4  "]],"start1":1054,"start2":1054,"length1":21,"length2":21},{"diffs":[[0,"Ax_0"],[-1,"  # Declare Ax at the start of the episode\n    Ay = Ay_0  # Declare Ay at the start of the"],[1,"\n    Ay = Ay_0  # Reset agent starting position each"],[0," epi"]],"start1":1364,"start2":1364,"length1":98,"length2":60},{"diffs":[[0,"episode\n"],[-1,"    "],[0,"\n    whi"]],"start1":1421,"start2":1421,"length1":20,"length2":16},{"diffs":[[0,"hed\n"],[-1,"        action_index = 0  # Declare action_index before use\n        Qvalues = []      # Declare Qvalues if needed (though not strictly necessary here)\n        new_Ax = 0        # Declare new_Ax before use\n        new_Ay = 0        # Declare new_Ay before use\n        reward = 0        # Declare reward before use\n\n"],[0,"    "]],"start1":1491,"start2":1491,"length1":322,"length2":8},{"diffs":[[0,"     if "],[1,"Math."],[0,"random()"]],"start1":1536,"start2":1536,"length1":16,"length2":21},{"diffs":[[0,"epsilon:"],[1,"  # Use random() from microbit"],[0,"\n       "]],"start1":1560,"start2":1560,"length1":16,"length2":46},{"diffs":[[0,"[Ay][Ax]"],[1,"  # Correct indexing row-first"],[0,"\n       "]],"start1":1703,"start2":1703,"length1":16,"length2":46},{"diffs":[[0,"] == 1):"],[1,"  # Hit a wall or out of bounds"],[0,"\n       "]],"start1":2083,"start2":2083,"length1":16,"length2":47},{"diffs":[[0,"w_Ax"],[-1," = Ax\n           "],[1,","],[0," new"]],"start1":2161,"start2":2161,"length1":25,"length2":9},{"diffs":[[0,"Ax, new_Ay ="],[1," Ax,"],[0," Ay  # Stay "]],"start1":2163,"start2":2163,"length1":24,"length2":28},{"diffs":[[0,"goal_y):"],[1,"  # Reached goal"],[0,"\n       "]],"start1":2242,"start2":2242,"length1":16,"length2":32},{"diffs":[[0,"      Ax"],[1,", Ay"],[0," = new_A"]],"start1":2689,"start2":2689,"length1":16,"length2":20},{"diffs":[[0,"w_Ax"],[-1,"\n        Ay ="],[1,","],[0," new"]],"start1":2706,"start2":2706,"length1":21,"length2":9},{"diffs":[[0,"(episode"],[-1," + "],[1,"+"],[0,"1) + \" E"]],"start1":2894,"start2":2894,"length1":19,"length2":17}]}]},{"timestamp":1740717587425,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," if "],[-1,"Math."],[0,"rand"]],"start1":1858,"start2":1858,"length1":13,"length2":8}]}]},{"timestamp":1740717607691,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"mber[];\n"],[1,"let maxQvalue: number;\n"],[0,"let new_"]],"start1":73,"start2":73,"length1":16,"length2":39},{"diffs":[[0,"number;\n"],[-1,"let maxQvalue: number;\n"],[0,"let old_"]],"start1":156,"start2":156,"length1":39,"length2":16},{"diffs":[[0,"t(1, 3)\n"],[1,"//  Use randint directly from microbit\n"],[0,"//  Q-le"]],"start1":1040,"start2":1040,"length1":16,"length2":55},{"diffs":[[0,"let "],[1,"["],[0,"goal_x"],[-1," = 3\nlet"],[1,","],[0," goal_y"],[1,"]"],[0," = "],[-1,"4"],[1,"[3, 4]"],[0,"\n// "]],"start1":1358,"start2":1358,"length1":33,"length2":33},{"diffs":[[0,"    "],[-1,"//  Declare Ax at the start of the episode\n    Ay = Ay_0\n    //  Declare Ay at the start of the episode\n    while ([Ax, Ay] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        action_index = 0\n        //  Declare action_index before use\n        Qvalues = []\n        //  Declare Qvalues if needed (though not strictly necessary here)\n        new_Ax = 0\n        //  Declare new_Ax before use\n        new_Ay = 0\n        //  Declare new_Ay before use\n        reward = 0\n        //  Declare reward before use"],[1,"Ay = Ay_0\n    //  Reset agent starting position each episode\n    while ([Ax, Ay] != [goal_x, goal_y]) {\n        //  Until goal is reached"],[0,"\n   "]],"start1":1780,"start2":1780,"length1":524,"length2":145},{"diffs":[[0," epsilon) {\n"],[1,"            //  Use random() from microbit\n"],[0,"            "]],"start1":1989,"start2":1989,"length1":24,"length2":67},{"diffs":[[0,"Ay][Ax]\n"],[1,"            //  Correct indexing row-first\n"],[0,"        "]],"start1":2165,"start2":2165,"length1":16,"length2":59},{"diffs":[[0,"    "],[-1,"reward = -5"],[1,"//  Hit a wall or out of bounds"],[0,"\n   "]],"start1":2610,"start2":2610,"length1":19,"length2":39},{"diffs":[[0,"        "],[-1,"new_Ax = Ax"],[1,"reward = -5"],[0,"\n       "]],"start1":2650,"start2":2650,"length1":27,"length2":27},{"diffs":[[0,"        "],[1," let [new_Ax,"],[0," new_Ay"],[1,"]"],[0," ="],[1," [Ax,"],[0," Ay"],[1,"]"],[0,"\n       "]],"start1":2673,"start2":2673,"length1":28,"length2":48},{"diffs":[[0,"n place\n"],[1,"            //  Reached goal\n"],[0,"        "]],"start1":2795,"start2":2795,"length1":16,"length2":45},{"diffs":[[0,"    "],[-1,"Ax"],[1,"let [Ax, Ay]"],[0," = "],[1,"["],[0,"new_Ax"],[-1,"\n        Ay ="],[1,","],[0," new_Ay"],[1,"]"],[0,"\n   "]],"start1":3289,"start2":3289,"length1":39,"length2":39},{"diffs":[[0,"in)\n"],[-1,"    //  Uncomment to debug\n    console.log(\"Episode: \" + (\"\" + (episode + 1)) + \" Epsilon: \" + (\"\" + Math.round(epsilon * 1000) / 1000))\n"],[0,"}\n"]],"start1":3450,"start2":3450,"length1":143,"length2":6}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"print(\"Episode: "],[1,"# display.scroll(\"Ep:"],[0,"\" + "]],"start1":3093,"start2":3093,"length1":24,"length2":29},{"diffs":[[0," \" E"],[-1,"psilon: "],[1,":"],[0,"\" + str("],[-1,"Math."],[0,"roun"]],"start1":3140,"start2":3140,"length1":29,"length2":17},{"diffs":[[0,"ilon"],[-1," * 1000)/1000"],[1,", 4)"],[0,"))"]],"start1":3162,"start2":3162,"length1":19,"length2":10}]}]},{"timestamp":1740717806666,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"er;\nlet "],[-1,"new_A"],[1,"action_inde"],[0,"x: numbe"]],"start1":28,"start2":28,"length1":21,"length2":27},{"diffs":[[0,"let "],[-1,"new_Ay"],[1,"Qvalues"],[0,": number"],[1,"[]"],[0,";\nlet "],[-1,"reward"],[1,"new_Ax"],[0,": nu"]],"start1":58,"start2":58,"length1":34,"length2":37},{"diffs":[[0,"let "],[-1,"valid_moves: any[];\nlet temp_x"],[1,"new_Ay: number;\nlet reward"],[0,": nu"]],"start1":101,"start2":101,"length1":38,"length2":34},{"diffs":[[0,"er;\nlet "],[-1,"temp_y"],[1,"maxQvalue"],[0,": number"]],"start1":137,"start2":137,"length1":22,"length2":25},{"diffs":[[0,"er;\nlet "],[-1,"next_q_value"],[1,"max_future_q"],[0,": number"]],"start1":185,"start2":185,"length1":28,"length2":28},{"diffs":[[0,"id w"],[-1,"ith a single Q-value per state"],[1,"here each entry is a list of 4 Q-values: [Up, Down, Left, Right]"],[0,"\nlet"]],"start1":270,"start2":270,"length1":38,"length2":72},{"diffs":[[0,"ble = [["],[1,"["],[0,"0, 0, 0,"],[1," 0], [0, 0,"],[0," 0, 0], "]],"start1":346,"start2":346,"length1":24,"length2":36},{"diffs":[[0,"0, 0], [0, 0, 0,"],[1," 0], [0, 0,"],[0," 0, 0], [0, 0, 0"]],"start1":375,"start2":375,"length1":32,"length2":43},{"diffs":[[0,"0, 0], [0, 0, 0,"],[1," 0]], [[0, 0,"],[0," 0, 0], [0, 0, 0"]],"start1":403,"start2":403,"length1":32,"length2":45},{"diffs":[[0,"], [0, 0, 0,"],[1," 0], [0, 0,"],[0," 0, 0], [0, "]],"start1":437,"start2":437,"length1":24,"length2":35},{"diffs":[[0,"0, 0"],[-1,", 0]]\n//  Define possible moves"],[1,"], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n//  Define possible moves\nlet actions = [\"Up\", \"Down\", \"Left\", \"Right\"]"],[0,"\nlet"]],"start1":475,"start2":475,"length1":39,"length2":313},{"diffs":[[0," 1]\n"],[-1,"//  Up, Down, Left, Right\n"],[0,"let "]],"start1":808,"start2":808,"length1":34,"length2":8},{"diffs":[[0,"the "],[-1,"best adjacent Q-valu"],[1,"max Q-value in a stat"],[0,"e\nfu"]],"start1":1388,"start2":1388,"length1":28,"length2":29},{"diffs":[[0,"ion "],[-1,"f"],[1,"F"],[0,"ind_"],[-1,"best_neighbor(x: number, y: number): number[] {\n    let new_x: number;\n    let new_y: number;\n    let q: number;\n    let best_q = -9999\n    //  large negative value as initial value\n    let best_x = x\n    let best_y = y\n    for (let i = 0; i < 4; i++) {\n        //  Check all 4 directions\n        new_x = x + move_x[i]\n        new_y = y + move_y[i]\n        if (0 <= new_x && new_x <= 4 && (0 <= new_y && new_y <= 4) && maze[new_y][new_x] == 0) {\n            //  Valid move\n            q = Qtable[new_y][new_x]\n            if (q > best_q) {\n                best_q = q\n                best_x = new_x\n                best_y = new_y\n            }\n            "],[1,"MaxQvalue(Qvalues: number[]): number {\n    let maxQ = Qvalues[0]\n    for (let v of Qvalues) {\n        if (v > maxQ) {\n            maxQ = v"],[0,"\n   "]],"start1":1420,"start2":1420,"length1":668,"length2":151},{"diffs":[[0,"urn "],[-1,"[best_x, best_y, best_q]"],[1,"maxQ"],[0,"\n}\n\n"]],"start1":1600,"start2":1600,"length1":32,"length2":12},{"diffs":[[0,"reached\n"],[1,"        action_index = 0\n        //  Declare action_index before use\n        Qvalues = []\n        //  Declare Qvalues if needed (though not strictly necessary here)\n"],[0,"        "]],"start1":1918,"start2":1918,"length1":16,"length2":181},{"diffs":[[0,"    "],[-1,"//  Random valid move\n            valid_moves = []\n            for (let i = 0; i < 4; i++) {\n                temp_x = Ax + move_x[i]\n                temp_y = Ay + move_y[i]\n                if (0 <= temp_x && temp_"],[1,"action_index = randint(0, 3)\n        } else {\n            //  Explore (0,1,2,3)\n            Qvalues = Qtable[Ay][Ax]\n            maxQvalue = Find_MaxQvalue(Qvalues)\n            action_inde"],[0,"x "],[-1,"<"],[0,"= "],[-1,"4 && (0 <= temp_y && temp_y <= 4) && maze[temp_y][temp_x] == 0) {\n                    valid_moves.push([temp_x, temp_y])\n                }\n      "],[1,"_py.py_array_index(Qvalues, maxQvalue)\n        }\n"],[0,"        "],[-1,"  "],[0,"\n   "]],"start1":2349,"start2":2349,"length1":381,"length2":257},{"diffs":[[0,"    "],[-1,"    }\n            if (valid_moves) {\n                //  If there are valid moves\n                let [new_Ax, new_Ay] = valid_moves[randint(0, valid_moves.length - 1)]\n            } else {\n                let [new_Ax,"],[1,"//  Determine new position\n        new_Ax = Ax + move_x[action_index]\n       "],[0," new_Ay"],[-1,"]"],[0," = "],[-1,"[Ax, Ay]\n            }\n            \n        } else {\n            //  Stay put if no valid moves\n            //  Greedy: Pick the neighbor with the highest Q-value"],[1,"Ay + move_y[action_index]\n        //  Check if move is valid"],[0,"\n   "]],"start1":2607,"start2":2607,"length1":399,"length2":155},{"diffs":[[0,"    "],[-1,"    let [new_Ax, new_Ay, _] = find_best_neighbor(Ax, Ay)\n        }"],[1,"if (new_Ax < 0 || new_Ax > 4 || new_Ay < 0 || new_Ay > 4 || maze[new_Ay][new_Ax] == 1) {"],[0,"\n   "]],"start1":2763,"start2":2763,"length1":74,"length2":96},{"diffs":[[0,"    "],[-1,"\n   "],[0,"    "],[-1," //  Determine "],[0,"reward"],[1," = -5"],[0,"\n   "]],"start1":2860,"start2":2860,"length1":37,"length2":23},{"diffs":[[0,"    "],[-1,"if ([new_Ax, new_Ay] == [goal_x, goal_y]) {"],[1,"    new_Ax = Ax"],[0,"\n   "]],"start1":2884,"start2":2884,"length1":51,"length2":23},{"diffs":[[0,"    "],[-1,"reward = 10"],[1,"new_Ay = Ay"],[0,"\n   "]],"start1":2912,"start2":2912,"length1":19,"length2":19},{"diffs":[[0,"if ("],[-1,"maze"],[0,"[new_A"],[-1,"y]["],[1,"x, "],[0,"new_A"],[-1,"x"],[1,"y"],[0,"] == "],[-1,"1"],[1,"[goal_x, goal_y]"],[0,") {\n"]],"start1":2943,"start2":2943,"length1":33,"length2":44},{"diffs":[[0,"/  S"],[-1,"houldn't happen due to valid move check, but included for safety\n            reward = -5\n            let [new_Ax, new_Ay] = [Ax, Ay]"],[1,"tay in place\n            reward = 10"],[0,"\n   "]],"start1":3000,"start2":3000,"length1":140,"length2":44},{"diffs":[[0,"//  "],[-1,"Q-value update (update current state based on next st"],[1,"Normal movement\n        //  Q-value upd"],[0,"ate"],[-1,")"],[0,"\n   "]],"start1":3109,"start2":3109,"length1":65,"length2":50},{"diffs":[[0,"[Ax]"],[-1,"\n        next_q_"],[1,"[action_index]\n        max_future_q = Find_MaxQ"],[0,"value"],[-1," = "],[1,"("],[0,"Qtab"]],"start1":3188,"start2":3188,"length1":32,"length2":61},{"diffs":[[0,"[new_Ax]"],[1,")"],[0,"\n       "]],"start1":3259,"start2":3259,"length1":16,"length2":17},{"diffs":[[0,"a * "],[-1,"next_q_value"],[1,"max_future_q"],[0," - o"]],"start1":3327,"start2":3327,"length1":20,"length2":20},{"diffs":[[0,"[Ay][Ax]"],[1,"[action_index]"],[0," = new_q"]],"start1":3400,"start2":3400,"length1":16,"length2":30},{"diffs":[[0,"    //  "],[-1,"for"],[1,"Uncomment to"],[0," debug\n "]],"start1":3639,"start2":3639,"length1":19,"length2":28},{"diffs":[[0,".log(\"Ep"],[-1,":"],[1,"isode: "],[0,"\" + (\"\" "]],"start1":3677,"start2":3677,"length1":17,"length2":23},{"diffs":[[0,")) + \" E"],[-1,":"],[1,"psilon: "],[0,"\" + (\"\" "]],"start1":3714,"start2":3714,"length1":17,"length2":24}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"id w"],[-1,"ith a single Q-value per state"],[1,"here each entry is a list of 4 Q-values: [Up, Down, Left, Right]"],[0,"\nQta"]],"start1":52,"start2":52,"length1":38,"length2":72},{"diffs":[[0,"    "],[1,"["],[0,"[0,"],[-1," 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0"],[1,"0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]"],[0,"],\n    "],[1,"["],[0,"[0,"],[-1," 0, 0, 0, 0"],[1,"0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]"],[0,"],\n    "],[1,"["],[0,"[0,"],[-1," 0, 0, 0, 0"],[1,"0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]"],[0,"]\n]\n"]],"start1":132,"start2":132,"length1":106,"length2":306},{"diffs":[[0,"ves\n"],[-1,"move_x = [0, 0, -1, 1]  # "],[1,"actions = [\""],[0,"Up"],[1,"\""],[0,", "],[1,"\""],[0,"Down"],[1,"\""],[0,", "],[1,"\""],[0,"Left"],[1,"\""],[0,", "],[1,"\""],[0,"Right"],[1,"\"]\nmove_x = [0, 0, -1, 1]"],[0,"\nmov"]],"start1":459,"start2":459,"length1":55,"length2":72},{"diffs":[[0,"the "],[-1,"best adjacent Q-value\ndef find_best_neighbor(x, y):\n    best_q = -9999  # large negative value as initial "],[1,"max Q-value in a state\ndef Find_MaxQvalue(Q"],[0,"value"],[1,"s):"],[0,"\n    "],[-1,"best_x = x\n    best_y = y"],[1,"maxQ = Qvalues[0]"],[0,"\n   "]],"start1":1073,"start2":1073,"length1":149,"length2":81},{"diffs":[[0,"for "],[-1,"i"],[1,"v"],[0," in "],[-1,"range(4):  # Check all 4 directions\n        new_x = x + move_x[i]\n        new_y = y + move_y[i]\n        if (0 <= new_x <= 4 and 0 <= new_y <= 4 and maze[new_y][new_x] == 0):  # Valid move\n            q = Qtable[new_y][new_x]\n            if q > best_q:\n                best_q = q\n                best_x = new_x\n                best_y = new_y\n    return best_x, best_y, best_q"],[1,"Qvalues:\n        if v > maxQ:\n            maxQ = v\n    return maxQ"],[0,"\n\n# "]],"start1":1155,"start2":1155,"length1":387,"length2":79},{"diffs":[[0,"reached\n"],[1,"        action_index = 0  # Declare action_index before use\n        Qvalues = []      # Declare Qvalues if needed (though not strictly necessary here)\n"],[0,"        "]],"start1":1491,"start2":1491,"length1":16,"length2":167},{"diffs":[[0,"new_Ax = 0  "],[1,"      "],[0,"# Declare ne"]],"start1":1658,"start2":1658,"length1":24,"length2":30},{"diffs":[[0,"new_Ay = 0  "],[1,"      "],[0,"# Declare ne"]],"start1":1712,"start2":1712,"length1":24,"length2":30},{"diffs":[[0,"reward = 0  "],[1,"      "],[0,"# Declare re"]],"start1":1766,"start2":1766,"length1":24,"length2":30},{"diffs":[[0,"    "],[-1,"# Random valid move\n            valid_moves = []\n            for i in range(4):\n                temp_x = Ax + move_x[i]\n                temp_y = Ay + move_y[i]\n                if (0 <= temp_x <= 4 and 0 <= temp_y <= 4 and maze[temp_y][temp_x] == 0):\n                    valid_moves.append((temp_x, temp_y))\n            if valid_moves:  # If there are valid moves\n   "],[1,"action_index = randint(0, 3)  # Explore (0,1,2,3)\n        else:\n            Qvalues = Qtable[Ay][Ax]\n            maxQvalue = Find_MaxQvalue(Qvalues)\n            action_index = Qvalues.index(maxQvalue)\n\n        # Determine new position\n"],[0,"        "],[-1,"     "],[0,"new_Ax"],[-1,", new_Ay = valid_moves[randint(0, len(valid_moves) - 1)]\n            else:\n                new_Ax, new_Ay = Ax, Ay  # Stay put if no valid moves\n        else:\n            # Greedy: Pick the neighbor with the highest Q-value"],[1," = Ax + move_x[action_index]\n        new_Ay = Ay + move_y[action_index]\n\n        # Check if move is valid"],[0,"\n   "]],"start1":1895,"start2":1895,"length1":616,"length2":362},{"diffs":[[0,"    "],[-1,"    new_Ax, new_Ay, _ = find_best_neighbor(Ax, Ay)\n\n        # Determine"],[1,"if (new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4 or maze[new_Ay][new_Ax] == 1):\n           "],[0," reward"],[1," = -5"],[0,"\n   "]],"start1":2258,"start2":2258,"length1":86,"length2":119},{"diffs":[[0,"    "],[-1,"if ("],[1,"    "],[0,"new_Ax"],[-1,", new_Ay) == (goal_x, goal_y):"],[1," = Ax"],[0,"\n   "]],"start1":2378,"start2":2378,"length1":48,"length2":23},{"diffs":[[0,"    "],[-1,"reward = 10"],[1,"new_Ay = Ay  # Stay in place"],[0,"\n   "]],"start1":2406,"start2":2406,"length1":19,"length2":36},{"diffs":[[0,"lif "],[-1,"maze["],[1,"("],[0,"new_A"],[-1,"y]["],[1,"x, "],[0,"new_A"],[-1,"x]"],[1,"y)"],[0," == "],[-1,"1:  # Shouldn't happen due to valid move check, but included for safety\n            reward = -5\n            new_Ax, new_Ay = Ax, Ay"],[1,"(goal_x, goal_y):\n            reward = 10"],[0,"\n   "]],"start1":2448,"start2":2448,"length1":163,"length2":69},{"diffs":[[0,"ard = -1"],[1,"  # Normal movement"],[0,"\n\n      "]],"start1":2543,"start2":2543,"length1":16,"length2":35},{"diffs":[[0,"date"],[-1," (update current state based on next state)\n        old_q_value = Qtable[Ay][Ax]\n        next_q_"],[1,"\n        old_q_value = Qtable[Ay][Ax][action_index]\n        max_future_q = Find_MaxQ"],[0,"value"],[-1," = "],[1,"("],[0,"Qtab"]],"start1":2592,"start2":2592,"length1":112,"length2":98},{"diffs":[[0,"[new_Ax]"],[1,")\n"],[0,"\n       "]],"start1":2700,"start2":2700,"length1":16,"length2":18},{"diffs":[[0,"a * "],[-1,"next_q_value"],[1,"max_future_q"],[0," - o"]],"start1":2769,"start2":2769,"length1":20,"length2":20},{"diffs":[[0,"[Ay][Ax]"],[1,"[action_index]"],[0," = new_q"]],"start1":2841,"start2":2841,"length1":16,"length2":30},{"diffs":[[0,"\n\n    # "],[-1,"for"],[1,"Uncomment to"],[0," debug\n "]],"start1":3066,"start2":3066,"length1":19,"length2":28},{"diffs":[[0,"rint(\"Ep"],[-1,":"],[1,"isode: "],[0,"\" + str("]],"start1":3098,"start2":3098,"length1":17,"length2":23},{"diffs":[[0,"1) + \" E"],[-1,":"],[1,"psilon: "],[0,"\" + str("]],"start1":3131,"start2":3131,"length1":17,"length2":24},{"diffs":[[0,"(epsilon"],[-1,"*"],[1," * "],[0,"1000)/10"]],"start1":3165,"start2":3165,"length1":17,"length2":19}]}]},{"timestamp":1740718524439,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"1, 1]\n]\n"],[-1,"bas"],[0,"\n# Agent"]],"start1":452,"start2":452,"length1":19,"length2":16}]}]},{"timestamp":1740719399655,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"1]\n]\nbas"],[-1,"i"],[0,"\n# Agent"]],"start1":455,"start2":455,"length1":17,"length2":16}]}]},{"timestamp":1740719399746,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"let Ax: number;\nlet Ay: number;\nlet new_Ax: number;\nlet new_Ay: number;\nlet reward: number;\nlet valid_moves: any[];\nlet temp_x: number;\nlet temp_y: number;\nlet old_q_value: number;\nlet next_q_value: number;\nlet new_q_value: number;\n//  Define Q-table as a 5x5 grid with a single Q-value per state\nlet Qtable = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\n//  Define possible moves\nlet move_x = [0, 0, -1, 1]\n//  Up, Down, Left, Right\nlet move_y = [-1, 1, 0, 0]\n//  Define the maze (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n//  Agent starting position\nlet Ax_0 = 1\nlet Ay_0 = randint(1, 3)\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay = 0.98\n//  Multiply epsilon by this after each episode\nlet goal_x = 3\nlet goal_y = 4\n//  Goal position\n//  Function to find the best adjacent Q-value\nfunction find_best_neighbor(x: number, y: number): number[] {\n    let new_x: number;\n    let new_y: number;\n    let q: number;\n    let best_q = -9999\n    //  large negative value as initial value\n    let best_x = x\n    let best_y = y\n    for (let i = 0; i < 4; i++) {\n        //  Check all 4 directions\n        new_x = x + move_x[i]\n        new_y = y + move_y[i]\n        if (0 <= new_x && new_x <= 4 && (0 <= new_y && new_y <= 4) && maze[new_y][new_x] == 0) {\n            //  Valid move\n            q = Qtable[new_y][new_x]\n            if (q > best_q) {\n                best_q = q\n                best_x = new_x\n                best_y = new_y\n            }\n            \n        }\n        \n    }\n    return [best_x, best_y, best_q]\n}\n\n//  Run multiple training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    //  100 training episodes\n    Ax = Ax_0\n    //  Declare Ax at the start of the episode\n    Ay = Ay_0\n    //  Declare Ay at the start of the episode\n    while ([Ax, Ay] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        new_Ax = 0\n        //  Declare new_Ax before use\n        new_Ay = 0\n        //  Declare new_Ay before use\n        reward = 0\n        //  Declare reward before use\n        //  Exploration vs Exploitation\n        if (Math.random() < epsilon) {\n            //  Random valid move\n            valid_moves = []\n            for (let i = 0; i < 4; i++) {\n                temp_x = Ax + move_x[i]\n                temp_y = Ay + move_y[i]\n                if (0 <= temp_x && temp_x <= 4 && (0 <= temp_y && temp_y <= 4) && maze[temp_y][temp_x] == 0) {\n                    valid_moves.push([temp_x, temp_y])\n                }\n                \n            }\n            if (valid_moves) {\n                //  If there are valid moves\n                let [new_Ax, new_Ay] = valid_moves[randint(0, valid_moves.length - 1)]\n            } else {\n                let [new_Ax, new_Ay] = [Ax, Ay]\n            }\n            \n        } else {\n            //  Stay put if no valid moves\n            //  Greedy: Pick the neighbor with the highest Q-value\n            let [new_Ax, new_Ay, _] = find_best_neighbor(Ax, Ay)\n        }\n        \n        //  Determine reward\n        if ([new_Ax, new_Ay] == [goal_x, goal_y]) {\n            reward = 10\n        } else if (maze[new_Ay][new_Ax] == 1) {\n            //  Shouldn't happen due to valid move check, but included for safety\n            reward = -5\n            let [new_Ax, new_Ay] = [Ax, Ay]\n        } else {\n            reward = -1\n        }\n        \n        //  Q-value update (update current state based on next state)\n        old_q_value = Qtable[Ay][Ax]\n        next_q_value = Qtable[new_Ay][new_Ax]\n        new_q_value = old_q_value + alpha * (reward + gamma * next_q_value - old_q_value)\n        //  Update Q-table\n        Qtable[Ay][Ax] = new_q_value\n        //  Move to new position\n        Ax = new_Ax\n        Ay = new_Ay\n    }\n    //  Reduce epsilon after each episode (exponential decay)\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n    //  for debug\n    console.log(\"Ep:\" + (\"\" + (episode + 1)) + \" E:\" + (\"\" + Math.round(epsilon * 1000) / 1000))\n}"],[0,"\n"]],"start1":0,"start2":0,"length1":1,"length2":4234}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"1, 1]\n]\n"],[1,"basi"],[0,"\n"],[-1,""],[0,"# Agent "]],"start1":452,"start2":452,"length1":17,"length2":21},{"diffs":[[0,"on\n\n"],[-1,"# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            brightness = 2 if maze[y][x] == 1 else 1  # Walls = 2, paths = 1\n            display.setPixel(x, y, brightness)  # Use setPixel for MakeCode\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    # Clear the old position (restore maze brightness)\n    display.setPixel(old_x, old_y, 1 if maze[old_y][old_x] == 0 else 2)\n    # Set the new position to full brightness\n    display.setPixel(new_x, new_y, 9)\n    sleep(100)  # Brief delay to make movement visible (adjust as needed)\n\n"],[0,"# Fu"]],"start1":803,"start2":803,"length1":669,"length2":8},{"diffs":[[0," = -"],[-1,"float('inf')  # Negative infinity"],[1,"9999  # large negative value"],[0," as "]],"start1":892,"start2":892,"length1":41,"length2":36},{"diffs":[[0,"_q\n\n"],[-1,"# Show the maze once at the start\nshow_maze()\nsleep(500)  # Brief pause to view the maze before agent appears\n\n"],[0,"# Ru"]],"start1":1357,"start2":1357,"length1":119,"length2":8},{"diffs":[[0,"   \n"],[-1,"    # Set initial agent position on the display\n    display.setPixel(Ax, Ay, 9)\n    sleep(100)  # Brief delay to show starting position\n    \n"],[0,"    "]],"start1":1559,"start2":1559,"length1":149,"length2":8},{"diffs":[[0,"     if "],[1,"Math."],[0,"random()"]],"start1":1814,"start2":1814,"length1":16,"length2":21},{"diffs":[[0," # S"],[-1,"afety check"],[1,"houldn't happen due to valid move check, but included for safety"],[0,"\n   "]],"start1":2668,"start2":2668,"length1":19,"length2":72},{"diffs":[[0,"e update"],[1," (update current state based on next state)"],[0,"\n       "]],"start1":2852,"start2":2852,"length1":16,"length2":59},{"diffs":[[0,"ue\n\n"],[-1,"        # Update agent position on display\n        update_agent_position(Ax, Ay, new_Ax, new_Ay)\n\n"],[0,"    "]],"start1":3137,"start2":3137,"length1":106,"length2":8},{"diffs":[[0,"  # "],[-1,"Uncomment to debug\n    # display.scroll"],[1,"for debug\n    print"],[0,"(\"Ep"]],"start1":3332,"start2":3332,"length1":47,"length2":27},{"diffs":[[0,"\" + str("],[1,"Math."],[0,"round(ep"]],"start1":3387,"start2":3387,"length1":16,"length2":21},{"diffs":[[0,"ilon"],[-1,", 4)))\n\n# Optional: Show completion\n"],[1,"*1000)/1000))"]],"start1":3409,"start2":3409,"length1":40,"length2":17}]}]},{"timestamp":1740719850482,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"led.plot_brightness(0, 0, 255)"],[1,"display.setPixel"],[0,"(x, "]],"start1":1007,"start2":1007,"length1":38,"length2":24}]}]},{"timestamp":1740720527531,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," 0, 255)"],[1,"(x, y, brightness)"],[0,"  # Use "]],"start1":1033,"start2":1033,"length1":16,"length2":34}]}]},{"timestamp":1740720535472,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.blocks","patch":[{"diffs":[[0,"es><"],[-1,"variable id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</variable></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement"],[1,"/variables><block type=\"pxt-on-start\" x=\"20\" y=\"20\"></block><block type=\"device_forever\" x=\"225\" y=\"20\""],[0,"></b"]],"start1":63,"start2":63,"length1":3978,"length2":111}]},{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"//  Define Q-table as a 5x5 grid with a single Q-value per state\nlet Qtable = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [0, 0, 1, 0, 0], [1, 0, 1, 0, 1], [1, 1, 1, 1, 1]]\nshow_maze()\nfunction show_maze() {\n    for (let x = 0; x < 5; x++) {\n        for (let y = 0; y < 5; y++) {\n            if (maze[y][x] == 1) {\n                led.plotBrightness(y, x, 1)\n            }\n            \n        }\n    }\n}\n"],[0,"\n"]],"start1":0,"start2":0,"length1":493,"length2":1}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[1,"from microbit import *\n\n"],[0,"# Define"]],"start1":0,"start2":0,"length1":8,"length2":32},{"diffs":[[0,"able = ["],[1,"\n    "],[0,"[0, 0, 0"]],"start1":89,"start2":89,"length1":16,"length2":21},{"diffs":[[0,", 0]"],[1,"\n"],[0,"]\n\n"],[1,"# Define possible moves\nmove_x = [0, 0, -1, 1]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\n"],[0,"maze = ["],[1,"\n    "],[0,"[1, "]],"start1":197,"start2":197,"length1":19,"length2":165},{"diffs":[[0,"0, 1],\n    ["],[-1,"0"],[1,"1"],[0,", 0, 1, 0, 0"]],"start1":389,"start2":389,"length1":25,"length2":25},{"diffs":[[0,"1, 0, 1, 0, "],[-1,"0"],[1,"1"],[0,"],\n    [1, 0"]],"start1":401,"start2":401,"length1":25,"length2":25},{"diffs":[[0,"   [1, 0, 1, 0, "],[-1,"1"],[1,"0"],[0,"],\n    [1, 1, 1,"]],"start1":418,"start2":418,"length1":33,"length2":33},{"diffs":[[0,", 1]"],[1,"\n"],[0,"]\n\n"],[-1,"show_maze("],[1,"# Agent starting position\nAx_0 = 1\nAy_0 = randint(1, 3)\n\n# Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\ngoal_x = 3\ngoal_y = 4  # Goal position\n\n# Function to display the static maze (called once"],[0,")\nde"]],"start1":453,"start2":453,"length1":21,"length2":404},{"diffs":[[0,"    for "],[-1,"x"],[1,"y"],[0," in rang"]],"start1":872,"start2":872,"length1":17,"length2":17},{"diffs":[[0,"5):\n        for "],[-1,"y"],[1,"x"],[0," in range(5):\n  "]],"start1":891,"start2":891,"length1":33,"length2":33},{"diffs":[[0,"        "],[1,"brightness = 2 "],[0,"if maze["]],"start1":926,"start2":926,"length1":16,"length2":31},{"diffs":[[0,"== 1"],[-1,":\n                led.plot_brightness(y, x, 1)  # Use setPixel for MakeCode"],[1," else 1  # Walls = 2, paths = 1\n            led.plot_brightness(0, 0, 255)  # Use setPixel for MakeCode\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    # Clear the old position (restore maze brightness)\n    display.setPixel(old_x, old_y, 1 if maze[old_y][old_x] == 0 else 2)\n    # Set the new position to full brightness\n    display.setPixel(new_x, new_y, 9)\n    sleep(100)  # Brief delay to make movement visible (adjust as needed)\n\n# Function to find the best adjacent Q-value\ndef find_best_neighbor(x, y):\n    best_q = -float('inf')  # Negative infinity as initial value\n    best_x = x\n    best_y = y\n    for i in range(4):  # Check all 4 directions\n        new_x = x + move_x[i]\n        new_y = y + move_y[i]\n        if (0 <= new_x <= 4 and 0 <= new_y <= 4 and maze[new_y][new_x] == 0):  # Valid move\n            q = Qtable[new_y][new_x]\n            if q > best_q:\n                best_q = q\n                best_x = new_x\n                best_y = new_y\n    return best_x, best_y, best_q\n\n# Show the maze once at the start\nshow_maze()\nsleep(500)  # Brief pause to view the maze before agent appears\n\n# Run multiple training episodes\nfor episode in range(100):  # 100 training episodes\n    Ax = Ax_0  # Declare Ax at the start of the episode\n    Ay = Ay_0  # Declare Ay at the start of the episode\n    \n    # Set initial agent position on the display\n    display.setPixel(Ax, Ay, 9)\n    sleep(100)  # Brief delay to show starting position\n    \n    while (Ax, Ay) != (goal_x, goal_y):  # Until goal is reached\n        new_Ax = 0  # Declare new_Ax before use\n        new_Ay = 0  # Declare new_Ay before use\n        reward = 0  # Declare reward before use\n\n        # Exploration vs Exploitation\n        if random() < epsilon:\n            # Random valid move\n            valid_moves = []\n            for i in range(4):\n                temp_x = Ax + move_x[i]\n                temp_y = Ay + move_y[i]\n                if (0 <= temp_x <= 4 and 0 <= temp_y <= 4 and maze[temp_y][temp_x] == 0):\n                    valid_moves.append((temp_x, temp_y))\n            if valid_moves:  # If there are valid moves\n                new_Ax, new_Ay = valid_moves[randint(0, len(valid_moves) - 1)]\n            else:\n                new_Ax, new_Ay = Ax, Ay  # Stay put if no valid moves\n        else:\n            # Greedy: Pick the neighbor with the highest Q-value\n            new_Ax, new_Ay, _ = find_best_neighbor(Ax, Ay)\n\n        # Determine reward\n        if (new_Ax, new_Ay) == (goal_x, goal_y):\n            reward = 10\n        elif maze[new_Ay][new_Ax] == 1:  # Safety check\n            reward = -5\n            new_Ax, new_Ay = Ax, Ay\n        else:\n            reward = -1\n\n        # Q-value update\n        old_q_value = Qtable[Ay][Ax]\n        next_q_value = Qtable[new_Ay][new_Ax]\n        new_q_value = old_q_value + alpha * (reward + gamma * next_q_value - old_q_value)\n\n        # Update Q-table\n        Qtable[Ay][Ax] = new_q_value\n\n        # Update agent position on display\n        update_agent_position(Ax, Ay, new_Ax, new_Ay)\n\n        # Move to new position\n        Ax = new_Ax\n        Ay = new_Ay\n\n    # Reduce epsilon after each episode (exponential decay)\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    # Uncomment to debug\n    # display.scroll(\"Ep:\" + str(episode + 1) + \" E:\" + str(round(epsilon, 4)))\n\n# Optional: Show completion"],[0,"\n"]],"start1":963,"start2":963,"length1":80,"length2":3408}]}]},{"timestamp":1740721130560,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"let Ax: number;\nlet Ay: number;\nlet new_Ax: number;\nlet new_Ay: number;\nlet reward: number;\nlet valid_moves: any[];\nlet i: number;\nlet temp_x: number;\nlet temp_y: number;\nlet old_q_value: number;\nlet next_q_value: number;\nlet new_q_value: number;\n"],[0,"//  "]],"start1":0,"start2":0,"length1":251,"length2":4},{"diffs":[[0,"1]]\n"],[-1,"//  Define possible moves\nlet move_x = [0, 0, -1, 1]\n//  Up, Down, Left, Right\nlet move_y = [-1, 1, 0, 0]\n//  Agent starting position\nlet Ax_0 = 1\nlet Ay_0 = randint(1, 3)\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay = 0.98\n//  Multiply epsilon by this after each episode\nlet goal_x = 3\nlet goal_y = 4\n//  Goal position\nshow_maze()\npause(500)\n//  Brief pause to view the maze before agent appears\n//  Run multiple training episodes\nfor (let episode = 0; episode < 100; episode++) {\n    //  100 training episodes\n    Ax = Ax_0\n    //  Declare Ax at the start of the episode\n    Ay = Ay_0\n    //  Declare Ay at the start of the episode\n    //  Set initial agent position on the display\n    led.plotBrightness(Ax, Ay, 255)\n    pause(100)\n    //  Brief delay to show starting position\n    while ([Ax, Ay] != [goal_x, goal_y]) {\n        //  Until goal is reached\n        new_Ax = 0\n        //  Declare new_Ax before use\n        new_Ay = 0\n        //  Declare new_Ay before use\n        reward = 0\n        //  Declare reward before use\n        //  Exploration vs Exploitation\n        if (Math.random() < epsilon) {\n            //  Random valid move\n            valid_moves = []\n            for (i = 0; i < 4; i++) {\n                temp_x = Ax + move_x[i]\n                temp_y = Ay + move_y[i]\n                if (0 <= temp_x && temp_x <= 4 && (0 <= temp_y && temp_y <= 4) && maze[temp_y][temp_x] == 0) {\n                    valid_moves.push([temp_x, temp_y])\n                }\n                \n            }\n            if (valid_moves) {\n                //  If there are valid moves\n                let [new_Ax, new_Ay] = valid_moves[randint(0, valid_moves.length - 1)]\n            } else {\n                let [new_Ax, new_Ay] = [Ax, Ay]\n            }\n            \n        } else {\n            //  Stay put if no valid moves\n            //  Greedy: Pick the neighbor with the highest Q-value\n            let [new_Ax, new_Ay, _] = find_best_neighbor(Ax, Ay)\n        }\n        \n        //  Determine reward\n        if ([new_Ax, new_Ay] == [goal_x, goal_y]) {\n            reward = 10\n        } else if (maze[new_Ay][new_Ax] == 1) {\n            //  Safety check\n            reward = -5\n            let [new_Ax, new_Ay] = [Ax, Ay]\n        } else {\n            reward = -1\n        }\n        \n        //  Q-value update\n        old_q_value = Qtable[Ay][Ax]\n        next_q_value = Qtable[new_Ay][new_Ax]\n        new_q_value = old_q_value + alpha * (reward + gamma * next_q_value - old_q_value)\n        //  Update Q-table\n        Qtable[Ay][Ax] = new_q_value\n        //  Update agent position on display\n        update_agent_position(Ax, Ay, new_Ax, new_Ay)\n        //  Move to new position\n        Ax = new_Ax\n        Ay = new_Ay\n    }\n    //  Reduce epsilon after each episode (exponential decay)\n    epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n}\n//  Function to update only the agent's positionupdate_agent_position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    //  Clear the old position (restore maze brightness)\n    led.unplot(old_x, old_y)\n    //  Set the new position to full brightness\n    led.plotBrightness(new_x, new_y, 255)\n}\n\n// Function to statically show the maze \nfunction show_maze() {\n    for (let x = 0; x < 5; x++) {\n        for (let y = 0; y < 5; y++) {\n            if (maze[y][x] == 1) {\n                // note python array elements as [row][column]\n                led.plotBrightness(x, y, 1)\n            }\n            \n        }\n    }\n}\n\n// note micro:bit led element as column,row\n//  Function to find the best adjacent Q-value\nfunction find_best_neighbor(x: number, y: number): number[] {\n    let new_x: number;\n    let new_y: number;\n    let q: number;\n    let best_q = -9999\n    //  Negative infinity as initial value\n    let best_x = x\n    let best_y = y\n    for (let i = 0; i < 4; i++) {\n        //  Check all 4 directions\n        new_x = x + move_x[i]\n        new_y = y + move_y[i]\n        if (0 <= new_x && new_x <= 4 && (0 <= new_y && new_y <= 4) && maze[new_y][new_x] == 0) {\n            //  Valid move\n            q = Qtable[new_y][new_x]\n            if (q > best_q) {\n                best_q = q\n                best_x = new_x\n                best_y = new_y\n            }\n            \n        }\n        \n    }\n    return [best_x, best_y, best_q]"],[1,"show_maze()\nfunction show_maze() {\n    for (let x = 0; x < 5; x++) {\n        for (let y = 0; y < 5; y++) {\n            if (maze[y][x] == 1) {\n                led.plotBrightness(y, x, 1)\n            }\n            \n        }\n    }"],[0,"\n}\n\n"]],"start1":257,"start2":257,"length1":4514,"length2":236}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"]]\n\n"],[-1,"# Define possible moves\nmove_x = [0, 0, -1, 1]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Agent starting position\nAx_0 = 1\nAy_0 = randint(1, 3)\n\n# Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\ngoal_x = 3\ngoal_y = 4  # Goal position\n\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears\n\n\n# Run multiple training episodes\nfor episode in range(100):  # 100 training episodes\n    Ax = Ax_0  # Declare Ax at the start of the episode\n    Ay = Ay_0  # Declare Ay at the start of the episode\n    \n    # Set initial agent position on the display\n    led.plot_brightness(Ax, Ay, 255)\n    pause(100)  # Brief delay to show starting position\n    \n    while (Ax, Ay) != (goal_x, goal_y):  # Until goal is reached\n        new_Ax = 0  # Declare new_Ax before use\n        new_Ay = 0  # Declare new_Ay before use\n        reward = 0  # Declare reward before use\n\n        # Exploration vs Exploitation\n        if Math.random() < epsilon:\n            # Random valid move\n            valid_moves = []\n            for i in range(4):\n                temp_x = Ax + move_x[i]\n                temp_y = Ay + move_y[i]\n                if (0 <= temp_x <= 4 and 0 <= temp_y <= 4 and maze[temp_y][temp_x] == 0):\n                    valid_moves.append((temp_x, temp_y))\n            if valid_moves:  # If there are valid moves\n                new_Ax, new_Ay = valid_moves[randint(0, len(valid_moves) - 1)]\n            else:\n                new_Ax, new_Ay = Ax, Ay  # Stay put if no valid moves\n        else:\n            # Greedy: Pick the neighbor with the highest Q-value\n            new_Ax, new_Ay, _ = find_best_neighbor(Ax, Ay)\n\n        # Determine reward\n        if (new_Ax, new_Ay) == (goal_x, goal_y):\n            reward = 10\n        elif maze[new_Ay][new_Ax] == 1:  # Safety check\n            reward = -5\n            new_Ax, new_Ay = Ax, Ay\n        else:\n            reward = -1\n\n        # Q-value update\n        old_q_value = Qtable[Ay][Ax]\n        next_q_value = Qtable[new_Ay][new_Ax]\n        new_q_value = old_q_value + alpha * (reward + gamma * next_q_value - old_q_value)\n\n        # Update Q-table\n        Qtable[Ay][Ax] = new_q_value\n\n        # Update agent position on display\n        update_agent_position(Ax, Ay, new_Ax, new_Ay)\n\n        # Move to new position\n        Ax = new_Ax\n        Ay = new_Ay\n\n    # Reduce epsilon after each episode (exponential decay)\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n\n# Function to update only the agent's positionupdate_agent_position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    # Clear the old position (restore maze brightness)\n    led.unplot(old_x, old_y)\n    # Set the new position to full brightness\n    led.plot_brightness(new_x, new_y, 255)\n\n#Function to statically show the maze "],[1,"show_maze()"],[0,"\ndef"]],"start1":281,"start2":281,"length1":2977,"length2":19},{"diffs":[[0,"= 1:"],[-1,"  #note python array elements as [row][column]"],[0,"\n   "]],"start1":391,"start2":391,"length1":54,"length2":8},{"diffs":[[0,"ess("],[-1,"x, y"],[1,"y, x"],[0,", 1)  #"],[-1,"note micro:bit led element as column,row\n\n# Function to find the best adjacent Q-value\ndef find_best_neighbor(x, y):\n    best_q = -9999  # Negative infinity as initial value\n    best_x = x\n    best_y = y\n    for i in range(4):  # Check all 4 directions\n        new_x = x + move_x[i]\n        new_y = y + move_y[i]\n        if (0 <= new_x <= 4 and 0 <= new_y <= 4 and maze[new_y][new_x] == 0):  # Valid move\n            q = Qtable[new_y][new_x]\n            if q > best_q:\n                best_q = q\n                best_x = new_x\n                best_y = new_y\n    return best_x, best_y, best_q"],[1," Use setPixel for MakeCode"],[0,"\n"]],"start1":428,"start2":428,"length1":607,"length2":42}]}]},{"timestamp":1740721853787,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"y, 255)\n"],[1,"\n"],[0,"#Functio"]],"start1":3207,"start2":3207,"length1":16,"length2":17}]}]},{"timestamp":1740721854298,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"y, 255)\n"],[-1,"    \n"],[0,"#Functio"]],"start1":3207,"start2":3207,"length1":21,"length2":16}]}]},{"timestamp":1740721863610,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"        if len(valid_moves) > 0:  # Explicitly check length\n                                    move_index = randint(0, len("],[1,"if valid_moves:  # If there are "],[0,"valid"],[-1,"_"],[1," "],[0,"moves"],[-1,") - 1)"],[0,"\n   "]],"start1":1760,"start2":1760,"length1":149,"length2":51},{"diffs":[[0,"    "],[-1,"                    selected_move = valid_moves[move_index]\n                                    new_Ax = selected_move[0]\n                                    new_Ay = selected_move[1"],[1,"new_Ax, new_Ay = valid_moves[randint(0, len(valid_moves) - 1)"],[0,"]\n  "]],"start1":1820,"start2":1820,"length1":190,"length2":69},{"diffs":[[0,"y, 255)\n"],[1,"    "],[0,"\n#Functi"]],"start1":3207,"start2":3207,"length1":16,"length2":20}]}]},{"timestamp":1740723112361,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"        "],[1,"    "],[0,"if len(v"]],"start1":1760,"start2":1760,"length1":16,"length2":20}]}]},{"timestamp":1740723115236,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"ves: any[];\n"],[1,"let i: number;\n"],[0,"let temp_x: "]],"start1":104,"start2":104,"length1":24,"length2":39},{"diffs":[[0,"er;\n"],[-1,"let move_index: number;\nlet selected_move: any;\nlet next_pos: number[];\n"],[0,"let "]],"start1":167,"start2":167,"length1":80,"length2":8},{"diffs":[[0," 0, 0]]\n"],[1,"let maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [0, 0, 1, 0, 0], [1, 0, 1, 0, 1], [1, 1, 1, 1, 1]]\n"],[0,"//  Defi"]],"start1":403,"start2":403,"length1":16,"length2":113},{"diffs":[[0," 0]\n"],[-1,"//  Define the maze (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 0, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 0], [1, 1, 1, 1, 1]]\n"],[0,"//  "]],"start1":610,"start2":610,"length1":151,"length2":8},{"diffs":[[0,"ion\n"],[-1,"//  Function to display the static maze (called once)\nfunction show_maze() {\n    let brightness: number;\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            brightness = maze[y][x] == 1 ? 50 : 20\n            //  Walls = 50, paths = 20 (0-255 scale)\n            led.plotBrightness(x, y, brightness)\n        }\n    }\n}\n\n//  Function to update only the agent's position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    let brightness = maze[old_y][old_x] == 0 ? 20 : 50\n    led.plotBrightness(old_x, old_y, brightness)\n    led.plotBrightness(new_x, new_y, 255)\n    pause(100)\n}\n\n//  Brief delay to make movement visible\n//  Function to choose the next position (simplified, no unused return values)\nfunction choose_next_position(x: number, y: number): number[] {\n    let nx: number;\n    let ny: number;\n    let q: number;\n    let best_q = -1000\n    //  Large negative number instead of -float('inf')\n    let next_x = x\n    let next_y = y\n    for (let i = 0; i < 4; i++) {\n        nx = x + move_x[i]\n        ny = y + move_y[i]\n        if (0 <= nx && nx <= 4 && (0 <= ny && ny <= 4) && maze[ny][nx] == 0) {\n            q = Qtable[ny][nx]\n            if (q > best_q) {\n                best_q = q\n                next_x = nx\n                next_y = ny\n            }\n            \n        }\n        \n    }\n    return [next_x, next_y]\n}\n\n//  Return as a list to avoid tuple unpacking issues\n//  Show the maze once at the start\n"],[0,"show"]],"start1":995,"start2":995,"length1":1505,"length2":8},{"diffs":[[0,"the episode\n"],[1,"    //  Set initial agent position on the display\n"],[0,"    led.plot"]],"start1":1301,"start2":1301,"length1":24,"length2":74},{"diffs":[[0," epsilon) {\n"],[1,"            //  Random valid move\n"],[0,"            "]],"start1":1775,"start2":1775,"length1":24,"length2":58},{"diffs":[[0,"   for ("],[-1,"let "],[0,"i = 0; i"]],"start1":1859,"start2":1859,"length1":20,"length2":16},{"diffs":[[0,"oves"],[-1,".length > 0"],[0,") {\n"]],"start1":2206,"start2":2206,"length1":19,"length2":8},{"diffs":[[0,"    "],[-1,"move_index = randint(0,"],[1,"//  If there are"],[0," valid"],[-1,"_"],[1," "],[0,"moves"],[-1,".length - 1)"],[0,"\n   "]],"start1":2226,"start2":2226,"length1":55,"length2":36},{"diffs":[[0,"    "],[-1,"selected_move = valid_moves[move_index]\n                new_Ax = selected_move[0]\n                new_Ay = selected_move[1"],[1,"let [new_Ax, new_Ay] = valid_moves[randint(0, valid_moves.length - 1)"],[0,"]\n  "]],"start1":2271,"start2":2271,"length1":130,"length2":77},{"diffs":[[0,"    "],[1,"let ["],[0,"new_Ax"],[-1," = Ax\n               "],[1,","],[0," new_Ay"],[1,"]"],[0," ="],[1," [Ax,"],[0," Ay"],[1,"]"],[0,"\n   "]],"start1":2379,"start2":2379,"length1":47,"length2":39},{"diffs":[[0,"//  "],[-1,"Greedy: Use the simplified function\n            next_pos = choose_next_position(Ax, Ay)"],[1,"Stay put if no valid moves\n            //  Greedy: Pick the neighbor with the highest Q-value"],[0,"\n   "]],"start1":2471,"start2":2471,"length1":95,"length2":101},{"diffs":[[0,"    "],[1,"let ["],[0,"new_Ax"],[-1," = next_pos[0]\n            new_Ay = next_pos[1]"],[1,", new_Ay, _] = find_best_neighbor(Ax, Ay)"],[0,"\n   "]],"start1":2577,"start2":2577,"length1":61,"length2":60},{"diffs":[[0,"    "],[-1,"reward = -5"],[1,"//  Safety check"],[0,"\n   "]],"start1":2814,"start2":2814,"length1":19,"length2":24},{"diffs":[[0,"    "],[-1,"new_Ax = Ax"],[1,"reward = -5"],[0,"\n   "]],"start1":2843,"start2":2843,"length1":19,"length2":19},{"diffs":[[0,"    "],[1," let [new_Ax,"],[0," new_Ay"],[1,"]"],[0," ="],[1," [Ax,"],[0," Ay"],[1,"]"],[0,"\n   "]],"start1":2866,"start2":2866,"length1":20,"length2":40},{"diffs":[[0,"//  "],[-1,"Uncomment to debug\n//  print(\"Ep:\" + str(episode + 1) + \" E:\" + str(round(epsilon, 4)))\n//  Optional: Show completion\nconsole.log(\"Done\")"],[1,"Function to update only the agent's positionupdate_agent_position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    //  Clear the old position (restore maze brightness)\n    led.unplot(old_x, old_y)\n    //  Set the new position to full brightness\n    led.plotBrightness(new_x, new_y, 255)\n}\n\n// Function to statically show the maze \nfunction show_maze() {\n    for (let x = 0; x < 5; x++) {\n        for (let y = 0; y < 5; y++) {\n            if (maze[y][x] == 1) {\n                // note python array elements as [row][column]\n                led.plotBrightness(x, y, 1)\n            }\n            \n        }\n    }\n}\n\n// note micro:bit led element as column,row\n//  Function to find the best adjacent Q-value\nfunction find_best_neighbor(x: number, y: number): number[] {\n    let new_x: number;\n    let new_y: number;\n    let q: number;\n    let best_q = -9999\n    //  Negative infinity as initial value\n    let best_x = x\n    let best_y = y\n    for (let i = 0; i < 4; i++) {\n        //  Check all 4 directions\n        new_x = x + move_x[i]\n        new_y = y + move_y[i]\n        if (0 <= new_x && new_x <= 4 && (0 <= new_y && new_y <= 4) && maze[new_y][new_x] == 0) {\n            //  Valid move\n            q = Qtable[new_y][new_x]\n            if (q > best_q) {\n                best_q = q\n                best_x = new_x\n                best_y = new_y\n            }\n            \n        }\n        \n    }\n    return [best_x, best_y, best_q]\n}\n"],[0,"\n"]],"start1":3530,"start2":3530,"length1":142,"length2":1488}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[-1,"from microbit import *\n\n"],[0,"# De"]],"start1":0,"start2":0,"length1":28,"length2":4},{"diffs":[[0,"e\nQtable = ["],[1,"[0, 0, 0, 0, 0],"],[0,"\n    [0, 0, "]],"start1":61,"start2":61,"length1":24,"length2":40},{"diffs":[[0,", 0]"],[-1,",\n    [0, 0, 0, 0, 0]\n]\n\n# Define possible moves\nmove_x"],[1,"]\n\nmaze"],[0," = ["],[-1,"0, 0"],[1,"[1, 1"],[0,", "],[-1,"-"],[0,"1, 1"],[-1,"]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = ["],[1,", 1],\n    [1, 0, 0, 0, 1],"],[0,"\n    ["],[-1,"1, 1"],[1,"0, 0"],[0,", 1, "],[-1,"1, 1"],[1,"0, 0"],[0,"],\n "]],"start1":168,"start2":168,"length1":200,"length2":75},{"diffs":[[0," 0],\n    [1, 0, "],[-1,"0"],[1,"1"],[0,", 0, 1],\n    [1,"]],"start1":237,"start2":237,"length1":33,"length2":33},{"diffs":[[0,"[1, "],[-1,"0"],[1,"1, 1"],[0,", 1, "],[-1,"0, 1],\n    [1"],[1,"1]]\n\n# Define possible moves\nmove_x = [0"],[0,", 0, "],[1,"-"],[0,"1, "],[-1,"0, 0],\n   "],[1,"1]  # Up, Down, Left, Right\nmove_y ="],[0," ["],[1,"-"],[0,"1, 1, "],[-1,"1, 1, 1]\n"],[1,"0, 0"],[0,"]\n\n#"]],"start1":267,"start2":267,"length1":62,"length2":115},{"diffs":[[0,"on\n\n"],[-1,"# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            brightness = 50 if maze[y][x] == 1 else 20  # Walls = 50, paths = 20 (0-255 scale)\n            led.plot_brightness(x, y, brightness)\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    brightness = 20 if maze[old_y][old_x] == 0 else 50\n    led.plot_brightness(old_x, old_y, brightness)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(100)  # Brief delay to make movement visible\n\n# Function to choose the next position (simplified, no unused return values)\ndef choose_next_position(x, y):\n    best_q = -1000  # Large negative number instead of -float('inf')\n    next_x = x\n    next_y = y\n    for i in range(4):\n        nx = x + move_x[i]\n        ny = y + move_y[i]\n        if (0 <= nx <= 4 and 0 <= ny <= 4 and maze[ny][nx] == 0):\n            q = Qtable[ny][nx]\n            if q > best_q:\n                best_q = q\n                next_x = nx\n                next_y = ny\n    return [next_x, next_y]  # Return as a list to avoid tuple unpacking issues\n\n# Show the maze once at the start\n"],[0,"show"]],"start1":719,"start2":719,"length1":1186,"length2":8},{"diffs":[[0,"ppears\n\n"],[1,"\n"],[0,"# Run mu"]],"start1":792,"start2":792,"length1":16,"length2":17},{"diffs":[[0,"pisode\n    \n"],[1,"    # Set initial agent position on the display\n"],[0,"    led.plot"]],"start1":991,"start2":991,"length1":24,"length2":72},{"diffs":[[0,"psilon:\n"],[1,"            # Random valid move\n"],[0,"        "]],"start1":1425,"start2":1425,"length1":16,"length2":48},{"diffs":[[0,"        "],[1,"    "],[0,"if len(v"]],"start1":1756,"start2":1756,"length1":16,"length2":20},{"diffs":[[0,"es) > 0:"],[-1,"\n"],[1,"  # Explicitly check length\n              "],[0,"        "]],"start1":1784,"start2":1784,"length1":17,"length2":58},{"diffs":[[0,"        "],[1,"      "],[0,"move_ind"]],"start1":1842,"start2":1842,"length1":16,"length2":22},{"diffs":[[0,"s) - 1)\n"],[1,"                    "],[0,"        "]],"start1":1894,"start2":1894,"length1":16,"length2":36},{"diffs":[[0,"_index]\n"],[1,"                    "],[0,"        "]],"start1":1970,"start2":1970,"length1":16,"length2":36},{"diffs":[[0,"ted_move[0]\n"],[1,"                    "],[0,"            "]],"start1":2028,"start2":2028,"length1":24,"length2":44},{"diffs":[[0,"w_Ax"],[-1," = Ax\n                new_Ay = Ay"],[1,", new_Ay = Ax, Ay  # Stay put if no valid moves"],[0,"\n   "]],"start1":2138,"start2":2138,"length1":41,"length2":55},{"diffs":[[0,"dy: "],[-1,"Use"],[1,"Pick"],[0," the "],[-1,"simplified function\n            next_pos = choose_next_position(Ax, Ay)\n            new_Ax = next_pos[0]\n            new_Ay = next_pos[1]"],[1,"neighbor with the highest Q-value\n            new_Ax, new_Ay, _ = find_best_neighbor(Ax, Ay)"],[0,"\n\n  "]],"start1":2222,"start2":2222,"length1":153,"length2":109},{"diffs":[[0,"x] == 1:"],[1,"  # Safety check"],[0,"\n       "]],"start1":2460,"start2":2460,"length1":16,"length2":32},{"diffs":[[0,"w_Ax"],[-1," = Ax\n           "],[1,","],[0," new"]],"start1":2523,"start2":2523,"length1":25,"length2":9},{"diffs":[[0,"new_Ay ="],[1," Ax,"],[0," Ay\n    "]],"start1":2529,"start2":2529,"length1":16,"length2":20},{"diffs":[[0,"n)\n\n"],[-1,"    # Uncomment to debug\n    # print(\"Ep:\" + str(episode + 1) + \" E:\" + str(round(epsilon, 4)))\n\n# Optional: Show completion\nprint(\"Done\")"],[1,"\n# Function to update only the agent's positionupdate_agent_position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    # Clear the old position (restore maze brightness)\n    led.unplot(old_x, old_y)\n    # Set the new position to full brightness\n    led.plot_brightness(new_x, new_y, 255)\n\n#Function to statically show the maze \ndef show_maze():\n    for x in range(5):\n        for y in range(5):\n            if maze[y][x] == 1:  #note python array elements as [row][column]\n                led.plot_brightness(x, y, 1)  #note micro:bit led element as column,row\n\n# Function to find the best adjacent Q-value\ndef find_best_neighbor(x, y):\n    best_q = -9999  # Negative infinity as initial value\n    best_x = x\n    best_y = y\n    for i in range(4):  # Check all 4 directions\n        new_x = x + move_x[i]\n        new_y = y + move_y[i]\n        if (0 <= new_x <= 4 and 0 <= new_y <= 4 and maze[new_y][new_x] == 0):  # Valid move\n            q = Qtable[new_y][new_x]\n            if q > best_q:\n                best_q = q\n                best_x = new_x\n                best_y = new_y\n    return best_x, best_y, best_q\n"]],"start1":3129,"start2":3129,"length1":142,"length2":1125}]}]},{"timestamp":1740723666508,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"let "],[-1,"found_valid: boolean;\nlet attempts: number;\nlet i: number;\nlet temp_x: number;\nlet temp_y: number;\nlet best_q: number;\nlet next_x: number;\nlet next_y: number;\nlet nx: number;\nlet ny: number;\nlet q"],[1,"valid_moves: any[];\nlet temp_x: number;\nlet temp_y: number;\nlet move_index: number;\nlet selected_move: any;\nlet next_pos"],[0,": number"],[1,"[]"],[0,";\nle"]],"start1":92,"start2":92,"length1":212,"length2":138},{"diffs":[[0,"visible\n"],[1,"//  Function to choose the next position (simplified, no unused return values)\nfunction choose_next_position(x: number, y: number): number[] {\n    let nx: number;\n    let ny: number;\n    let q: number;\n    let best_q = -1000\n    //  Large negative number instead of -float('inf')\n    let next_x = x\n    let next_y = y\n    for (let i = 0; i < 4; i++) {\n        nx = x + move_x[i]\n        ny = y + move_y[i]\n        if (0 <= nx && nx <= 4 && (0 <= ny && ny <= 4) && maze[ny][nx] == 0) {\n            q = Qtable[ny][nx]\n            if (q > best_q) {\n                best_q = q\n                next_x = nx\n                next_y = ny\n            }\n            \n        }\n        \n    }\n    return [next_x, next_y]\n}\n\n//  Return as a list to avoid tuple unpacking issues\n"],[0,"//  Show"]],"start1":1790,"start2":1790,"length1":16,"length2":781},{"diffs":[[0,"    "],[-1,"//  Random "],[0,"valid"],[-1," "],[1,"_"],[0,"move"],[-1," without building a list\n            found_valid = false\n            attempts = 0\n            while (attempts < 4 && !found_valid) {\n                i = randint(0, 3)\n                //  Pick a random direction\n                temp_x = Ax + move_x[i]\n                temp_y = Ay + move_y[i]\n                if (0 <= temp_x && temp_x <= 4 && (0 <= temp_y && temp_y <= 4) && maze[temp_y][temp_x] == 0) {\n                    new_Ax = temp_x\n                    new_Ay = temp_y\n                    found_valid = true\n                }\n                \n                attempts = attempts + 1\n            }\n            if (!found_valid) {\n                new_Ax = Ax\n                new_Ay = Ay\n            }\n            \n        } else {\n            //  Stay put if no"],[1,"s = []\n            for (let i = 0; i < 4; i++) {\n                temp_x = Ax + move_x[i]\n                temp_y = Ay + move_y[i]\n                if (0 <= temp_x && temp_x <= 4 && (0 <= temp_y && temp_y <= 4) && maze[temp_y][temp_x] == 0) {\n                    valid_moves.push([temp_x, temp_y])\n                }\n                \n            }\n            if (valid_moves.length > 0) {\n                move_index = randint(0,"],[0," valid"],[-1," "],[1,"_"],[0,"move"],[-1," found\n            //  Inline greedy selection\n            best_q = -1000\n            next_x = Ax\n            next_y = Ay\n            for (i = 0; i < 4; i++) {\n                nx = Ax + move_x[i]\n                ny = Ay + move_y[i]\n                if (0 <= nx && nx <= 4 && (0 <= ny && ny <= 4) && maze[ny][nx] == 0) {\n                    q = Qtable[ny][nx]\n                    if (q > best_q) {\n                        best_q = q\n "],[1,"s.length - 1)\n                selected_move = valid_moves[move_index]\n                new_Ax = selected_move[0]\n                new_Ay = selected_move[1]\n            } else {\n"],[0,"    "]],"start1":3345,"start2":3345,"length1":1236,"length2":629},{"diffs":[[0,"        "],[-1,"       next_"],[1,"new_A"],[0,"x = "],[-1,"n"],[1,"A"],[0,"x\n      "]],"start1":3978,"start2":3978,"length1":33,"length2":26},{"diffs":[[0,"        "],[-1,"  "],[-1,"      next_"],[1,"new_A"],[0,"y = "],[-1,"n"],[1,"A"],[0,"y\n      "]],"start1":4006,"start2":4006,"length1":34,"length2":26},{"diffs":[[0,"        "],[-1,"    "],[-1,"    }\n        "],[1,"}\n"],[0,"        "]],"start1":4030,"start2":4030,"length1":34,"length2":18},{"diffs":[[0,"   \n        "],[-1,"        }"],[1,"} else {"],[0,"\n           "]],"start1":4049,"start2":4049,"length1":33,"length2":32},{"diffs":[[0,"        "],[-1,"    \n            }"],[1,"//  Greedy: Use the simplified function\n            next_pos = choose_next_position(Ax, Ay)"],[0,"\n       "]],"start1":4074,"start2":4074,"length1":34,"length2":107},{"diffs":[[0," = next_"],[-1,"x"],[1,"pos[0]"],[0,"\n       "]],"start1":4192,"start2":4192,"length1":17,"length2":22},{"diffs":[[0," = next_"],[-1,"y"],[1,"pos[1]"],[0,"\n       "]],"start1":4225,"start2":4225,"length1":17,"length2":22}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"isible\n\n"],[1,"# Function to choose the next position (simplified, no unused return values)\ndef choose_next_position(x, y):\n    best_q = -1000  # Large negative number instead of -float('inf')\n    next_x = x\n    next_y = y\n    for i in range(4):\n        nx = x + move_x[i]\n        ny = y + move_y[i]\n        if (0 <= nx <= 4 and 0 <= ny <= 4 and maze[ny][nx] == 0):\n            q = Qtable[ny][nx]\n            if q > best_q:\n                best_q = q\n                next_x = nx\n                next_y = ny\n    return [next_x, next_y]  # Return as a list to avoid tuple unpacking issues\n\n"],[0,"# Show t"]],"start1":1366,"start2":1366,"length1":16,"length2":589},{"diffs":[[0,"    "],[-1,"# Random "],[0,"valid"],[-1," "],[1,"_"],[0,"move"],[-1," without building a list\n            found_valid = False\n            attempts = 0\n            while attempts < 4 and not found_valid:\n                i = randint(0, 3)  # Pick a random direction"],[1,"s = []\n            for i in range(4):"],[0,"\n   "]],"start1":2650,"start2":2650,"length1":221,"length2":55},{"diffs":[[0,"    "],[-1,"new_Ax = temp_x\n                    new_Ay ="],[1,"valid_moves.append((temp_x,"],[0," temp_y"],[-1,"\n    "],[1,"))\n"],[0,"    "]],"start1":2888,"start2":2888,"length1":64,"length2":45},{"diffs":[[0,"    "],[-1,"    found_valid = True\n                attempts = attempts + 1\n            if not found_valid:\n                new_Ax = Ax\n                new_Ay = Ay  # Stay put if no"],[1,"if len(valid_moves) > 0:\n                move_index = randint(0, len(valid_moves) - 1)\n                selected_move ="],[0," valid"],[-1," "],[1,"_"],[0,"move"],[-1," found"],[1,"s[move_index]"],[0,"\n   "]],"start1":2937,"start2":2937,"length1":193,"length2":150},{"diffs":[[0,"    "],[-1,"else:\n"],[0,"        "],[-1,"    # Inline greedy selection\n            best_q = -1000\n            next_x = Ax\n            next_y = Ay\n            for i in range(4)"],[1,"new_Ax = selected_move[0]\n                new_Ay = selected_move[1]\n            else"],[0,":\n  "]],"start1":3088,"start2":3088,"length1":156,"length2":100},{"diffs":[[0,"       n"],[1,"ew_A"],[0,"x = Ax"],[-1," + move_x[i]"],[0,"\n       "]],"start1":3195,"start2":3195,"length1":34,"length2":26},{"diffs":[[0,"       n"],[1,"ew_A"],[0,"y = Ay"],[-1," + move_y[i]"],[0,"\n       "]],"start1":3223,"start2":3223,"length1":34,"length2":26},{"diffs":[[0,"    "],[1,"else:\n"],[0,"        "],[-1,"if (0 <= nx <= 4 and 0 <= ny <= 4 and maze[ny][nx] == 0):\n                    q = Qtable[ny][nx]\n                    if q > best_q:\n                        best_q = q\n                        next_x = nx\n                        next_y = ny"],[1,"    # Greedy: Use the simplified function\n            next_pos = choose_next_position(Ax, Ay)"],[0,"\n   "]],"start1":3246,"start2":3246,"length1":254,"length2":115},{"diffs":[[0," = next_"],[-1,"x"],[1,"pos[0]"],[0,"\n       "]],"start1":3376,"start2":3376,"length1":17,"length2":22},{"diffs":[[0," = next_"],[-1,"y"],[1,"pos[1]"],[0,"\n\n      "]],"start1":3409,"start2":3409,"length1":17,"length2":22}]}]},{"timestamp":1740723990063,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"ow_maze() {\n"],[1,"    let brightness: number;\n"],[0,"    for (let"]],"start1":1241,"start2":1241,"length1":24,"length2":52},{"diffs":[[0,"        "],[-1,"if ("],[1,"brightness = "],[0,"maze[y]["]],"start1":1357,"start2":1357,"length1":20,"length2":29},{"diffs":[[0,"e[y][x] == 1"],[-1,") {"],[1," ? 50 : 20"],[0,"\n           "]],"start1":1381,"start2":1381,"length1":27,"length2":34},{"diffs":[[0,"    "],[-1,"    led.plotBrightness(x, y, 5)\n            }\n            "],[1,"//  Walls = 50, paths = 20 (0-255 scale)\n            led.plotBrightness(x, y, brightness)"],[0,"\n   "]],"start1":1412,"start2":1412,"length1":66,"length2":97}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"        "],[1,"brightness = 50 "],[0,"if maze["]],"start1":926,"start2":926,"length1":16,"length2":32},{"diffs":[[0,"][x]"],[1," "],[0,"=="],[-1,"1:\n    "],[1," 1 else 20  # Walls = 50, paths = 20 (0-255 scale)\n"],[0,"    "]],"start1":959,"start2":959,"length1":17,"length2":62},{"diffs":[[0,"s(x, y, "],[-1,"5"],[1,"brightness"],[0,")\n\n# Fun"]],"start1":1047,"start2":1047,"length1":17,"length2":26},{"diffs":[[0,"y):\n"],[1,"    brightness = 20 if maze[old_y][old_x] == 0 else 50\n"],[0,"    led."],[-1,"un"],[0,"plot"],[1,"_brightness"],[0,"(old"]],"start1":1166,"start2":1166,"length1":22,"length2":86},{"diffs":[[0,"x, old_y"],[1,", brightness)"],[0,"\n    led"]],"start1":1253,"start2":1253,"length1":16,"length2":29}]}]},{"timestamp":1740724587568,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"oal_x = "],[-1,"4"],[1,"3"],[0,"\nlet goa"]],"start1":1133,"start2":1133,"length1":17,"length2":17},{"diffs":[[0,"oal_y = "],[-1,"3"],[1,"4"],[0,"\n//  Goa"]],"start1":1148,"start2":1148,"length1":17,"length2":17},{"diffs":[[0,"  le"],[-1,"d.unplot"],[1,"t brightness = maze[old_y][old_x] == 0 ? 20 : 50\n    led.plotBrightness"],[0,"(old"]],"start1":1594,"start2":1594,"length1":16,"length2":79},{"diffs":[[0,"x, old_y"],[1,", brightness"],[0,")\n    le"]],"start1":1674,"start2":1674,"length1":16,"length2":28}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"oal_x = "],[-1,"4"],[1,"3"],[0,"\ngoal_y "]],"start1":764,"start2":764,"length1":17,"length2":17},{"diffs":[[0,"oal_y = "],[-1,"3"],[1,"4"],[0,"  # Goal"]],"start1":775,"start2":775,"length1":17,"length2":17},{"diffs":[[0,"x, old_y"],[-1,")"],[0,"\n    led"]],"start1":1119,"start2":1119,"length1":17,"length2":16}]}]},{"timestamp":1740724902032,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,", 1, 1], [1, 0, "],[-1,"1"],[1,"0"],[0,", 0, 1], [1, 0, "]],"start1":714,"start2":714,"length1":33,"length2":33},{"diffs":[[0,"1, 0, 1, 0, "],[-1,"1"],[1,"0"],[0,"], [1, 1, 1,"]],"start1":758,"start2":758,"length1":25,"length2":25},{"diffs":[[0," 3)\n"],[-1,"maze[Ay_0][0] = 0\n// open up the maze at the starting update_agent_position\nmaze[randint(1, 3)][2] = 0\n// open up a random passage in teh wa\n"],[0,"//  "]],"start1":853,"start2":853,"length1":149,"length2":8},{"diffs":[[0,"le ("],[1,"["],[0,"Ax"],[1,", Ay]"],[0," != "],[1,"["],[0,"goal_x"],[-1," && Ay !="],[1,","],[0," goal_y"],[1,"]"],[0,") {\n"]],"start1":2176,"start2":2176,"length1":36,"length2":36},{"diffs":[[0," // "],[-1,"run u"],[1," U"],[0,"ntil"]],"start1":2219,"start2":2219,"length1":13,"length2":10}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," 1],\n    [1, 0, "],[-1,"1"],[1,"0"],[0,", 0, 1],\n    [1,"]],"start1":370,"start2":370,"length1":33,"length2":33},{"diffs":[[0,"   [1, 0, 1, 0, "],[-1,"1"],[1,"0"],[0,"],\n    [1, 1, 1,"]],"start1":418,"start2":418,"length1":33,"length2":33},{"diffs":[[0,", 3)"],[-1,"\nmaze[Ay_0][0] = 0  #open up the maze at the starting update_agent_position\nmaze[randint(1, 3)][2] = 0  #open up a random passage in teh wa"],[0,"\n\n# "]],"start1":512,"start2":512,"length1":147,"length2":8},{"diffs":[[0," (Ax"],[1,", Ay)"],[0," != "],[1,"("],[0,"goal_x"],[-1," and Ay !="],[1,","],[0," goa"]],"start1":1648,"start2":1648,"length1":28,"length2":25},{"diffs":[[0,"_y):"],[1," "],[0," #"],[-1,"run u"],[1," U"],[0,"ntil"]],"start1":1674,"start2":1674,"length1":15,"length2":13}]}]},{"timestamp":1740725501323,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"tion"],[-1," and maze randomization"],[0,"\nlet"]],"start1":814,"start2":814,"length1":31,"length2":8},{"diffs":[[0," Ax_0 = "],[-1,"0"],[1,"1"],[0,"\nlet Ay_"]],"start1":822,"start2":822,"length1":17,"length2":17},{"diffs":[[0,"e[Ay_0]["],[-1,"Ax_"],[0,"0] = 0\n/"]],"start1":860,"start2":860,"length1":19,"length2":16},{"diffs":[[0,"in t"],[-1,"h"],[0,"e"],[1,"h"],[0," wa"],[-1,"ll in the center of the maze\nlet goal_x = 4\n//  Goal position\nlet goal_y = randint(1, 3)\nmaze[goal_y][goal_x]"],[1,"\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay"],[0," = 0"],[1,".98"],[0,"\n// "],[-1,"open up the maze at the goal position\n//  Show the maze once at the start\nshow_maze()\npause(500)\n//  Brief pause to view the maze before agent appears\n//  Q-learning parameters\nlet alpha = 0.1\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.05\n//  Minimum exploration rate\nlet epsilon_decay = 0.98\n//  Multiply epsilon by this after each episode"],[1," Multiply epsilon by this after each episode\nlet goal_x = 4\nlet goal_y = 3\n//  Goal position\n//  Function to display the static maze (called once)\nfunction show_maze() {\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            if (maze[y][x] == 1) {\n                led.plotBrightness(x, y, 5)\n            }\n            \n        }\n    }\n}\n\n//  Function to update only the agent's position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    led.unplot(old_x, old_y)\n    led.plotBrightness(new_x, new_y, 255)\n    pause(100)\n}\n\n//  Brief delay to make movement visible\n//  Show the maze once at the start\nshow_maze()\npause(500)\n//  Brief pause to view the maze before agent appears"],[0,"\n// "]],"start1":988,"start2":988,"length1":551,"length2":991},{"diffs":[[0,"des\n"],[-1,"    serial.writeNumber(episode)\n"],[0,"    "]],"start1":2087,"start2":2087,"length1":40,"length2":8},{"diffs":[[0,"on_min)\n"],[-1,"    "],[1,"}\n"],[0,"//  Unco"]],"start1":4960,"start2":4960,"length1":20,"length2":18},{"diffs":[[0,"bug\n"],[-1,"    console.log"],[1,"//  print"],[0,"(\"Ep"]],"start1":4989,"start2":4989,"length1":23,"length2":17},{"diffs":[[0,"\"Ep:\" + "],[-1,"(\"\" + "],[1,"str"],[0,"(episode"]],"start1":5003,"start2":5003,"length1":22,"length2":19},{"diffs":[[0,"ode + 1)"],[-1,")"],[0," + \" E:\""]],"start1":5019,"start2":5019,"length1":17,"length2":16},{"diffs":[[0,"\" + "],[-1,"(\"\" + Math."],[1,"str("],[0,"roun"]],"start1":5034,"start2":5034,"length1":19,"length2":12},{"diffs":[[0,"ilon"],[-1," * 1000) / 1000))\n}\n//  Function to display the static maze (called once)\nfunction show_maze() {\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            if (maze[y][x] == 1) {\n                led.plotBrightness(x, y, 5)\n            }\n            \n        }\n    }\n}\n\n//  Function to update only the agent's position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    led.unplot(old_x, old_y)\n    led.plotBrightness(new_x, new_y, 255)\n    pause(100)\n}\n"],[1,", 4)))\n//  Optional: Show completion\nconsole.log(\"Done\")"],[0,"\n"]],"start1":5051,"start2":5051,"length1":529,"length2":61}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"tion"],[-1," and maze randomization"],[0,"\nAx_0 = "],[-1,"0"],[1,"1"],[0,"\nAy_"]],"start1":482,"start2":482,"length1":40,"length2":17},{"diffs":[[0,"e[Ay_0]["],[-1,"Ax_"],[0,"0] = 0  "]],"start1":520,"start2":520,"length1":19,"length2":16},{"diffs":[[0,"in t"],[-1,"h"],[0,"e"],[1,"h"],[0," wa"],[-1,"ll in the center of the maze\ngoal_x = 4  # Goal position\ngoal_y = randint(1,3) \nmaze[goal_y][goal_x] = 0  #open up the maze at the goal position\n\n# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears\n\n# Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\n"],[1,"\n\n# Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\ngoal_x = 4\ngoal_y = 3  # Goal position\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x]==1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(100)  # Brief delay to make movement visible\n\n# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears"],[0,"\n\n# "]],"start1":646,"start2":646,"length1":515,"length2":834},{"diffs":[[0,"des\n"],[-1,"    serial.write_number(episode)\n"],[0,"    "]],"start1":1559,"start2":1559,"length1":41,"length2":8},{"diffs":[[0,"ebug\n   "],[1," #"],[0," print(\""]],"start1":4099,"start2":4099,"length1":16,"length2":18},{"diffs":[[0,"\" + str("],[-1,"Math."],[0,"round(ep"]],"start1":4147,"start2":4147,"length1":21,"length2":16},{"diffs":[[0,"ilon"],[-1,"*1000)/1000))\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x]==1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(100)  # Brief delay to make movement visible\n"],[1,", 4)))\n\n# Optional: Show completion\nprint(\"Done\")"]],"start1":4164,"start2":4164,"length1":443,"length2":53}]}]},{"timestamp":1740726175512,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," goal_x "],[1,"and"],[0," Ay != g"]],"start1":1534,"start2":1534,"length1":16,"length2":19}]}]},{"timestamp":1740726180803,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," 0.9"],[-1,"7"],[1,"8"],[0,"\n// "]],"start1":1508,"start2":1508,"length1":9,"length2":9},{"diffs":[[0,"pisodes\n"],[1,"    serial.writeNumber(episode)\n"],[0,"    Ax ="]],"start1":1669,"start2":1669,"length1":16,"length2":48},{"diffs":[[0,"pause(10"],[1,"0"],[0,")\n    //"]],"start1":1871,"start2":1871,"length1":16,"length2":17},{"diffs":[[0,"l_x "],[-1,"||"],[1,"&&"],[0," Ay "]],"start1":1948,"start2":1948,"length1":10,"length2":10},{"diffs":[[0,".log(\"Ep"],[-1,"isode"],[0,":\" + (\"\""]],"start1":4624,"start2":4624,"length1":21,"length2":16},{"diffs":[[0,")) + \" E"],[-1,"psilon"],[0,":\" + (\"\""]],"start1":4655,"start2":4655,"length1":22,"length2":16}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," 0.9"],[-1,"7"],[1,"8"],[0,"  # "]],"start1":1129,"start2":1129,"length1":9,"length2":9},{"diffs":[[0,"pisodes\n"],[1,"    serial.write_number(episode)\n"],[0,"    Ax ="]],"start1":1262,"start2":1262,"length1":16,"length2":49},{"diffs":[[0,"pause(10"],[1,"0"],[0,")  # Bri"]],"start1":1461,"start2":1461,"length1":16,"length2":17},{"diffs":[[0," goal_x "],[-1,"or"],[0," Ay != g"]],"start1":1534,"start2":1534,"length1":18,"length2":16},{"diffs":[[0,"rint(\"Ep"],[-1,"isode"],[0,":\" + str"]],"start1":3846,"start2":3846,"length1":21,"length2":16},{"diffs":[[0,"1) + \" E"],[-1,"psilon"],[0,":\" + str"]],"start1":3873,"start2":3873,"length1":22,"length2":16}]}]},{"timestamp":1740726665652,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"er;\nlet "],[-1,"i"],[1,"found_valid: boolean;\nlet attempts: number;\nlet i: number;\nlet temp_x: number;\nlet temp_y"],[0,": number"]],"start1":88,"start2":88,"length1":17,"length2":105},{"diffs":[[0,"0][Ax_0] = 0\n// "],[-1," O"],[1,"o"],[0,"pen up the maze "]],"start1":888,"start2":888,"length1":34,"length2":33},{"diffs":[[0,"at the starting "],[1,"update_agent_"],[0,"position\nmaze[ra"]],"start1":921,"start2":921,"length1":32,"length2":45},{"diffs":[[0,"][2] = 0\n// "],[-1," O"],[1,"o"],[0,"pen up a ran"]],"start1":977,"start2":977,"length1":26,"length2":25},{"diffs":[[0,"the "],[-1,"center wall"],[1,"wall in the center of the maze"],[0,"\nlet"]],"start1":1017,"start2":1017,"length1":19,"length2":38},{"diffs":[[0," = 0\n// "],[-1," O"],[1,"o"],[0,"pen up t"]],"start1":1132,"start2":1132,"length1":18,"length2":17},{"diffs":[[0,"osition\n"],[1,"//  Show the maze once at the start\nshow_maze()\npause(500)\n//  Brief pause to view the maze before agent appears\n"],[0,"//  Q-le"]],"start1":1170,"start2":1170,"length1":16,"length2":129},{"diffs":[[0,"ode\n"],[-1,"//  Function to display the static maze (called once)\nfunction show_maze() {\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            if (maze[y][x] == 1) {\n                led.plotBrightness(x, y, 5)\n            }\n            \n        }\n    }\n}\n\n//  Function to update only the agent's position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    led.unplot(old_x, old_y)\n    led.plotBrightness(new_x, new_y, 255)\n    pause(100)\n}\n\n//  Brief delay to make movement visible\n//  Show the maze once at the start\nshow_maze()\npause(500)\n//  Brief pause to view the maze before agent appears\n"],[0,"//  "]],"start1":1558,"start2":1558,"length1":667,"length2":8},{"diffs":[[0,"     // "],[-1," R"],[1,"r"],[0,"un until"]],"start1":1941,"start2":1941,"length1":18,"length2":17},{"diffs":[[0,"dom "],[-1,"move (no valid move check)\n"],[1,"valid move without building a list\n            found_valid = false\n            attempts = 0\n            while (attempts < 4 && !found_valid) {\n "],[0,"    "]],"start1":2244,"start2":2244,"length1":35,"length2":152},{"diffs":[[0,"        "],[1,"   "],[0,"i = rand"]],"start1":2396,"start2":2396,"length1":16,"length2":19},{"diffs":[[0,"t(0, 3)\n"],[1,"    "],[0,"        "]],"start1":2417,"start2":2417,"length1":16,"length2":20},{"diffs":[[0,"            "],[1," "],[-1,"new_A"],[1,"   temp_"],[0,"x = Ax + mov"]],"start1":2469,"start2":2469,"length1":29,"length2":33},{"diffs":[[0,"            "],[-1,"new_A"],[1,"    temp_"],[0,"y = Ay + mov"]],"start1":2509,"start2":2509,"length1":29,"length2":33},{"diffs":[[0,"    "],[-1,"//  Ensure move stays within bounds\n"],[1,"    if (0 <= temp_x && temp_x <= 4 && (0 <= temp_y && temp_y <= 4) && maze[temp_y][temp_x] == 0) {\n    "],[0,"    "]],"start1":2557,"start2":2557,"length1":44,"length2":111},{"diffs":[[0,"    "],[-1,"if ("],[1,"    "],[0,"new_Ax "],[-1,"< 0 || new_Ax > 4 || new_Ay < 0 || new_Ay > 4) {\n                new_Ax = Ax\n                //  Revert to current position if out of bounds"],[1,"= temp_x\n                    new_Ay = temp_y\n                    found_valid = true\n                }\n                \n                attempts = attempts + 1\n            }\n            if (!found_valid) {\n                new_Ax = Ax"],[0,"\n   "]],"start1":2672,"start2":2672,"length1":159,"length2":251},{"diffs":[[0,"       } else {\n"],[1,"            //  Stay put if no valid move found\n"],[0,"            //  "]],"start1":2976,"start2":2976,"length1":32,"length2":80},{"diffs":[[0,"tion"],[-1," (only valid moves)"],[0,"\n   "]],"start1":3075,"start2":3075,"length1":27,"length2":8},{"diffs":[[0," Ax\n"],[-1,"            //  Stay put if hitting a wall\n"],[0,"    "]],"start1":3900,"start2":3900,"length1":51,"length2":8},{"diffs":[[0,"//  "],[-1,"Debug episode progress"],[1,"Uncomment to debug"],[0,"\n   "]],"start1":4557,"start2":4557,"length1":30,"length2":26},{"diffs":[[0,"//  "],[-1,"Optional: Show completion\nconsole.log(\"Done\")"],[1,"Function to display the static maze (called once)\nfunction show_maze() {\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            if (maze[y][x] == 1) {\n                led.plotBrightness(x, y, 5)\n            }\n            \n        }\n    }\n}\n\n//  Function to update only the agent's position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    led.unplot(old_x, old_y)\n    led.plotBrightness(new_x, new_y, 255)\n    pause(100)\n}\n"],[0,"\n"]],"start1":4690,"start2":4690,"length1":50,"length2":505}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"_0][Ax_0] = 0  #"],[-1," O"],[1,"o"],[0,"pen up the maze "]],"start1":547,"start2":547,"length1":34,"length2":33},{"diffs":[[0,"at the starting "],[1,"update_agent_"],[0,"position\nmaze[ra"]],"start1":580,"start2":580,"length1":32,"length2":45},{"diffs":[[0,")][2] = 0  #"],[-1," O"],[1,"o"],[0,"pen up a ran"]],"start1":635,"start2":635,"length1":26,"length2":25},{"diffs":[[0,"the "],[-1,"center wall"],[1,"wall in the center of the maze"],[0,"\ngoa"]],"start1":675,"start2":675,"length1":19,"length2":38},{"diffs":[[0,"= randint(1,"],[-1," "],[0,"3)"],[1," "],[0,"\nmaze[goal_y"]],"start1":745,"start2":745,"length1":27,"length2":27},{"diffs":[[0,"] = 0  #"],[-1," O"],[1,"o"],[0,"pen up t"]],"start1":780,"start2":780,"length1":18,"length2":17},{"diffs":[[0,"sition\n\n"],[1,"# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears\n\n"],[0,"# Q-lear"]],"start1":819,"start2":819,"length1":16,"length2":127},{"diffs":[[0,"de\n\n"],[-1,"# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(100)  # Brief delay to make movement visible\n\n# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears"],[0,"\n\n# "]],"start1":1179,"start2":1179,"length1":544,"length2":8},{"diffs":[[0,"oal_y): "],[-1," # R"],[1,"#r"],[0,"un until"]],"start1":1518,"start2":1518,"length1":20,"length2":18},{"diffs":[[0,"dom "],[-1,"move (no valid move check)\n"],[1,"valid move without building a list\n            found_valid = False\n            attempts = 0\n            while attempts < 4 and not found_valid:\n    "],[0,"    "]],"start1":1789,"start2":1789,"length1":35,"length2":156},{"diffs":[[0,"            "],[-1,"new_A"],[1,"    temp_"],[0,"x = Ax + mov"]],"start1":1998,"start2":1998,"length1":29,"length2":33},{"diffs":[[0,"            "],[1,"    "],[-1,"new_A"],[1,"temp_"],[0,"y = Ay + mov"]],"start1":2038,"start2":2038,"length1":29,"length2":33},{"diffs":[[0,"    "],[-1,"# Ensure move stays within bounds"],[1,"    if (0 <= temp_x <= 4 and 0 <= temp_y <= 4 and maze[temp_y][temp_x] == 0):"],[0,"\n   "]],"start1":2086,"start2":2086,"length1":41,"length2":85},{"diffs":[[0,"    "],[-1,"if"],[1,"       "],[0," new_Ax "],[-1,"< 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4:\n                new_Ax = Ax  # Revert to current position if out of bounds\n                new_Ay = Ay"],[1,"= temp_x\n                    new_Ay = temp_y\n                    found_valid = True\n                attempts = attempts + 1\n            if not found_valid:\n                new_Ax = Ax\n                new_Ay = Ay  # Stay put if no valid move found"],[0,"\n   "]],"start1":2176,"start2":2176,"length1":167,"length2":269},{"diffs":[[0,"tion"],[-1," (only valid moves)"],[0,"\n   "]],"start1":2489,"start2":2489,"length1":27,"length2":8},{"diffs":[[0,"= Ax"],[-1,"  # Stay put if hitting a wall"],[0,"\n   "]],"start1":3167,"start2":3167,"length1":38,"length2":8},{"diffs":[[0,"  # "],[-1,"Debug episode progress"],[1,"Uncomment to debug"],[0,"\n   "]],"start1":3786,"start2":3786,"length1":30,"length2":26},{"diffs":[[0,"ilon"],[-1," * "],[1,"*"],[0,"1000)"],[-1," / "],[1,"/"],[0,"1000"]],"start1":3883,"start2":3883,"length1":19,"length2":15},{"diffs":[[0,"\n\n# "],[-1,"Optional: Show completion\nprint(\"Done\")"],[1,"Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x]==1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(100)  # Brief delay to make movement visible"],[0,"\n"]],"start1":3900,"start2":3900,"length1":44,"length2":426}]}]},{"timestamp":1740727029467,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,") {\n"],[-1,"    if (maze[old_y][old_x] == 1) {\n        led.plotBrightness(old_x, old_y, 5)\n    } else {\n    "],[0,"    "]],"start1":1744,"start2":1744,"length1":104,"length2":8},{"diffs":[[0,"_y)\n"],[-1,"    }\n    \n"],[0,"    "]],"start1":1773,"start2":1773,"length1":19,"length2":8},{"diffs":[[0,"ny <= 4)"],[1," && maze[ny][nx] == 0"],[0,") {\n    "]],"start1":3429,"start2":3429,"length1":16,"length2":37},{"diffs":[[0," -5\n        "],[-1,"} else {"],[1,"    new_Ax = Ax"],[0,"\n           "]],"start1":3986,"start2":3986,"length1":32,"length2":39},{"diffs":[[0," // "],[-1,"new_Ax = Ax  #"],[0," Sta"]],"start1":4025,"start2":4025,"length1":22,"length2":8},{"diffs":[[0,"        "],[-1," //"],[0," new_Ay "]],"start1":4060,"start2":4060,"length1":19,"length2":16},{"diffs":[[0,"    new_Ay = Ay\n"],[1,"        } else {\n"],[0,"            rewa"]],"start1":4065,"start2":4065,"length1":32,"length2":49}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"y):\n"],[-1,"    if maze[old_y][old_x] == 1: \n        led.plot_brightness(old_x, old_y, 5)\n    else:\n    "],[0,"    "]],"start1":1338,"start2":1338,"length1":100,"length2":8},{"diffs":[[0," ny <= 4"],[1," and maze[ny][nx] == 0"],[0,"):\n     "]],"start1":2855,"start2":2855,"length1":16,"length2":38},{"diffs":[[0,"            "],[-1,"#"],[0,"new_Ax = Ax "]],"start1":3290,"start2":3290,"length1":25,"length2":24},{"diffs":[[0,"        "],[-1,"#"],[0,"new_Ay ="]],"start1":3348,"start2":3348,"length1":17,"length2":16}]}]},{"timestamp":1740727533600,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"er;\n"],[-1,"let episode_reward: number;\n"],[0,"let "]],"start1":28,"start2":28,"length1":36,"length2":8},{"diffs":[[0,"new_Ax: number;\n"],[1,""],[0,"let new_Ay: numb"]],"start1":36,"start2":36,"length1":32,"length2":32},{"diffs":[[0,"ode\n"],[-1,"    episode_reward = 0\n"],[0,"    "]],"start1":2331,"start2":2331,"length1":31,"length2":8},{"diffs":[[0,"   \n"],[-1,"        episode_reward = episode_reward + reward\n"],[0,"    "]],"start1":4216,"start2":4216,"length1":57,"length2":8},{"diffs":[[0,"Episode:"],[-1," "],[0,"\" + (\"\" "]],"start1":4833,"start2":4833,"length1":17,"length2":16},{"diffs":[[0," 1)) + \""],[-1," "],[0," Epsilon"]],"start1":4861,"start2":4861,"length1":17,"length2":16},{"diffs":[[0,"Epsilon:"],[-1," "],[0,"\" + (\"\" "]],"start1":4870,"start2":4870,"length1":17,"length2":16},{"diffs":[[0,"000)"],[-1," + \"  Total Reward: \" + episode_reward"],[0,")\n}\n"]],"start1":4918,"start2":4918,"length1":46,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ode\n"],[-1,"    episode_reward = 0\n"],[0,"    "]],"start1":1866,"start2":1866,"length1":31,"length2":8},{"diffs":[[0,"-1\n\n"],[-1,"        episode_reward = episode_reward + reward\n\n"],[0,"    "]],"start1":3475,"start2":3475,"length1":58,"length2":8},{"diffs":[[0,"Episode:"],[-1," "],[0,"\" + str("]],"start1":4068,"start2":4068,"length1":17,"length2":16},{"diffs":[[0,"+ 1) + \""],[-1," "],[0," Epsilon"]],"start1":4092,"start2":4092,"length1":17,"length2":16},{"diffs":[[0,"Epsilon:"],[-1," "],[0,"\" + str("]],"start1":4101,"start2":4101,"length1":17,"length2":16},{"diffs":[[0,"000)"],[-1," + \"  Total Reward: \" + episode_reward"],[0,")\n\n#"]],"start1":4147,"start2":4147,"length1":46,"length2":8}]}]},{"timestamp":1740728008523,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"if ("],[1,"["],[0,"new_Ax"],[-1," == goal_x &&"],[1,","],[0," new_Ay"],[1,"]"],[0," =="],[1," [goal_x,"],[0," goal_y"],[1,"]"],[0,") {\n"]],"start1":3987,"start2":3987,"length1":44,"length2":44},{"diffs":[[0,"rd = -5\n"],[-1,""],[0,"        "]],"start1":4119,"start2":4119,"length1":16,"length2":16},{"diffs":[[0,"       } else {\n"],[1,"            // new_Ax = Ax  # Stay put if hitting a wall\n            // new_Ay = Ay\n"],[0,"            rewa"]],"start1":4128,"start2":4128,"length1":32,"length2":116}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"w_Ax"],[1,", new_Ay)"],[0," == "],[1,"("],[0,"goal_x"],[-1," and new_Ay =="],[1,","],[0," goa"]],"start1":3260,"start2":3260,"length1":32,"length2":29},{"diffs":[[0,"][new_Ax] == 1:\n"],[-1,""],[0,"            rewa"]],"start1":3343,"start2":3343,"length1":32,"length2":32},{"diffs":[[0,"rd = -5\n"],[1,"            #new_Ax = Ax  # Stay put if hitting a wall\n            #new_Ay = Ay\n"],[0,"        "]],"start1":3375,"start2":3375,"length1":16,"length2":96}]}]},{"timestamp":1740728256360,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\"Done\")\n"],[-1,"\n"]],"start1":4225,"start2":4225,"length1":9,"length2":8}]}]},{"timestamp":1740729130104,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\")\n\n"],[-1,"music.play(music.create_sound_expression(WaveShape.NOISE,\n        2526,\n        2351,\n        255,\n        255,\n        50,\n        SoundExpressionEffect.NONE,\n        InterpolationCurve.LINEAR),\n    music.PlaybackMode.UNTIL_DONE)\nmusic.play(music.create_sound_expression(WaveShape.SINE,\n        1385,\n        2395,\n        255,\n        245,\n        500,\n        SoundExpressionEffect.NONE,\n        InterpolationCurve.CURVE),\n    music.PlaybackMode.UNTIL_DONE)"]],"start1":4230,"start2":4230,"length1":464,"length2":4}]}]},{"timestamp":1740729130214,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"pha = 0."],[-1,"2"],[1,"1"],[0,"\n//  Lea"]],"start1":1121,"start2":1121,"length1":17,"length2":17},{"diffs":[[0," 1)) + \""],[-1,"  "],[0,"  Epsilo"]],"start1":4878,"start2":4878,"length1":18,"length2":16},{"diffs":[[0,"000) + \""],[-1,"  "],[0,"  Total "]],"start1":4937,"start2":4937,"length1":18,"length2":16},{"diffs":[[0,"e\")\n"],[-1,"console.log(Qtable)\nfunction play_noise() {\n    music.play(music.createSoundExpression(WaveShape.Noise, 2526, 2351, 255, 255, 50, SoundExpressionEffect.None, InterpolationCurve.Linear), music.PlaybackMode.UntilDone)\n}\n\nfunction play_ding() {\n    music.play(music.createSoundExpression(WaveShape.Sine, 1385, 2395, 255, 245, 500, SoundExpressionEffect.None, InterpolationCurve.Curve), music.PlaybackMode.UntilDone)\n}\n\n"]],"start1":5029,"start2":5029,"length1":420,"length2":4}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"pha = 0."],[-1,"2"],[1,"1"],[0,"  # Lear"]],"start1":824,"start2":824,"length1":17,"length2":17},{"diffs":[[0,"+ 1) + \""],[-1,"  "],[0,"  Epsilo"]],"start1":4089,"start2":4089,"length1":18,"length2":16},{"diffs":[[0,"000) + \""],[-1,"  "],[0,"  Total "]],"start1":4146,"start2":4146,"length1":18,"length2":16},{"diffs":[[0,"e\")\n"],[-1,"print(Qtable)\n\ndef play_noise():\n    "],[1,"\n"],[0,"musi"]],"start1":4229,"start2":4229,"length1":45,"length2":9},{"diffs":[[0,"    "],[-1,"    "],[0,"2526,\n"],[-1,"    "],[0,"    "]],"start1":4296,"start2":4296,"length1":22,"length2":14},{"diffs":[[0,"51,\n        "],[-1,"    "],[0,"255,\n"],[-1,"    "],[0,"        255,"]],"start1":4316,"start2":4316,"length1":37,"length2":29},{"diffs":[[0,"        "],[-1,"    "],[0,"50,\n    "]],"start1":4346,"start2":4346,"length1":20,"length2":16},{"diffs":[[0,"    50,\n        "],[-1,"   "],[-1," "],[0,"SoundExpressionE"]],"start1":4350,"start2":4350,"length1":36,"length2":32},{"diffs":[[0,"ionEffect.NONE,\n"],[-1,"    "],[0,"        Interpol"]],"start1":4378,"start2":4378,"length1":36,"length2":32},{"diffs":[[0,"INEAR),\n    "],[-1,"   "],[-1," "],[0,"music.Playba"]],"start1":4422,"start2":4422,"length1":28,"length2":24},{"diffs":[[0,"NE)\n"],[-1,"\ndef play_ding():\n    "],[0,"musi"]],"start1":4461,"start2":4461,"length1":30,"length2":8},{"diffs":[[0,"    "],[-1," "],[-1,"   "],[0,"1385,\n"],[-1,"    "],[0,"    "]],"start1":4526,"start2":4526,"length1":22,"length2":14},{"diffs":[[0,"95,\n        "],[-1," "],[-1,"   "],[0,"255,\n"],[-1,"    "],[0,"        245,"]],"start1":4546,"start2":4546,"length1":37,"length2":29},{"diffs":[[0,"    "],[-1," "],[-1,"   "],[0,"500,\n"],[-1,"    "],[0,"    "]],"start1":4580,"start2":4580,"length1":21,"length2":13},{"diffs":[[0,"NE,\n        "],[-1," "],[-1,"   "],[0,"Interpolatio"]],"start1":4621,"start2":4621,"length1":28,"length2":24},{"diffs":[[0,"CURVE),\n"],[-1,"    "],[0,"    musi"]],"start1":4652,"start2":4652,"length1":20,"length2":16}]}]},{"timestamp":1740729715732,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"  pause("],[-1,"50)\n    "],[1,"100)\n}\n\n"],[0,"//  Brie"]],"start1":1956,"start2":1956,"length1":24,"length2":24},{"diffs":[[0,"ble\n"],[-1,"    if (new_x == goal_x && new_y == goal_y) {\n        led.toggle(goal_x, goal_y)\n        pause(50)\n    }\n    \n}\n\n"],[0,"//  "]],"start1":2009,"start2":2009,"length1":121,"length2":8}]},{"type":"edited","filename":"pxt.json","patch":[{"diffs":[[0,"nner"],[-1,"(RL)"],[1," - Copy"],[0,"\",\n "]],"start1":21,"start2":21,"length1":12,"length2":15}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"  pause("],[-1,"5"],[1,"10"],[0,"0)  # Br"]],"start1":1508,"start2":1508,"length1":17,"length2":18},{"diffs":[[0,"ble\n"],[-1,"    if (new_x == goal_x and new_y == goal_y):\n        led.toggle(goal_x, goal_y)\n        pause(50)\n        \n\n"],[0,"\n# S"]],"start1":1557,"start2":1557,"length1":117,"length2":8}]}]},{"timestamp":1740730311305,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"pha = 0."],[-1,"3"],[1,"2"],[0,"\n//  Lea"]],"start1":1121,"start2":1121,"length1":17,"length2":17},{"diffs":[[0,") {\n"],[-1,"        play_ding()\n        for (let i = 0; i < 7; i++) {\n    "],[0,"    "]],"start1":2055,"start2":2055,"length1":70,"length2":8},{"diffs":[[0,"        "],[-1,"    "],[0,"pause(50"]],"start1":2094,"start2":2094,"length1":20,"length2":16},{"diffs":[[0,"50)\n"],[-1,"        }\n    }\n    \n    if (maze[new_y][new_x] == 1) {\n        play_noise()\n"],[0,"    "]],"start1":2108,"start2":2108,"length1":85,"length2":8},{"diffs":[[0,"51, "],[-1,"50, 50, 1"],[1,"255, 255, 5"],[0,"0, S"]],"start1":5262,"start2":5262,"length1":17,"length2":19},{"diffs":[[0,", 2395, "],[-1,"50, 50"],[1,"255, 245"],[0,", 500, S"]],"start1":5455,"start2":5455,"length1":22,"length2":24}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"pha = 0."],[-1,"3"],[1,"2"],[0,"  # Lear"]],"start1":824,"start2":824,"length1":17,"length2":17},{"diffs":[[0,"y):\n"],[-1,"        play_ding()\n        for i in range(7):\n    "],[0,"    "]],"start1":1602,"start2":1602,"length1":59,"length2":8},{"diffs":[[0,"        "],[-1,"    "],[0,"pause(50"]],"start1":1641,"start2":1641,"length1":20,"length2":16},{"diffs":[[0,"    "],[-1,"if (maze[new_y][new_x] == 1):\n        play_noise()"],[1,"    "],[0,"\n\n\n#"]],"start1":1659,"start2":1659,"length1":58,"length2":12},{"diffs":[[0,"51,\n            "],[1,"25"],[0,"5"],[-1,"0"],[0,",\n            50"]],"start1":4472,"start2":4472,"length1":34,"length2":35},{"diffs":[[0,"55,\n            "],[1,"25"],[0,"5"],[-1,"0"],[0,",\n            10"]],"start1":4489,"start2":4489,"length1":34,"length2":35},{"diffs":[[0,"        "],[-1,"1"],[1,"5"],[0,"0,\n     "]],"start1":4514,"start2":4514,"length1":17,"length2":17},{"diffs":[[0,"95,\n            "],[1,"25"],[0,"5"],[-1,"0"],[0,",\n            50"]],"start1":4756,"start2":4756,"length1":34,"length2":35},{"diffs":[[0,"55,\n            "],[1,"24"],[0,"5"],[-1,"0"],[0,",\n            50"]],"start1":4773,"start2":4773,"length1":34,"length2":35}]}]},{"timestamp":1740730887078,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"ode < 10"],[1,"0"],[0,"; episod"]],"start1":2443,"start2":2443,"length1":16,"length2":17},{"diffs":[[0,"e\")\n"],[-1,"for (let row of Qtable) {\n    "],[0,"cons"]],"start1":5289,"start2":5289,"length1":38,"length2":8},{"diffs":[[0,"log("],[-1,"row)\n}"],[1,"Qtable)"],[0,"\nfun"]],"start1":5301,"start2":5301,"length1":14,"length2":15}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"range(10"],[1,"0"],[0,"):  # 10"]],"start1":1930,"start2":1930,"length1":16,"length2":17},{"diffs":[[0,"e\")\n"],[-1,"for row in Qtable:\n        print(row"],[1,"print(Qtable"],[0,")\n\nd"]],"start1":4442,"start2":4442,"length1":44,"length2":20}]}]},{"timestamp":1740731531205,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"][Ax] = "],[-1,"M"],[0,"new_q_va"]],"start1":3924,"start2":3924,"length1":17,"length2":16}]}]},{"timestamp":1740731531327,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"] = "],[-1,"Math.round(new_q_value * 100) / 100\n        // update Qtable element rounded to 100th"],[1,"new_q_value"],[0,"\n   "]],"start1":4739,"start2":4739,"length1":93,"length2":19}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"[Ax] = M"],[-1,"ath.round("],[0,"new_q_va"]],"start1":3925,"start2":3925,"length1":26,"length2":16},{"diffs":[[0,"alue"],[-1,"*100)/100 #update Qtable element rounded to 100th"],[0,"\n\n  "]],"start1":3940,"start2":3940,"length1":57,"length2":8}]}]},{"timestamp":1740731573720,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"dy selection"],[1," (only valid moves)"],[0,"\n           "]],"start1":2927,"start2":2927,"length1":24,"length2":43}]}]},{"timestamp":1740759239856,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"dy selection"],[1," (only valid moves)"],[0,"\n           "]],"start1":3571,"start2":3571,"length1":24,"length2":43}]}]},{"timestamp":1740759239934,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"let Ax: number;\nlet Ay: number;\nlet episode_reward: number;\nlet new_Ax: number;\nlet new_Ay: number;\nlet reward: number;\nlet i: number;\nlet best_q: number;\nlet next_x: number;\nlet next_y: number;\nlet nx: number;\nlet ny: number;\nlet q: number;\nlet old_q_value: number;\nlet next_q_value: number;\nlet new_q_value: number;\n"],[0,"//  Define Q-tab"]],"start1":0,"start2":0,"length1":16,"length2":334},{"diffs":[[0,"odes"],[-1," on button A\ninput.onButtonPressed(Button.A, function on_button_pressed_a() {\n    let Ax: number;\n    let Ay: number;\n    let episode_reward: number;\n    let new_Ax: number;\n    let new_Ay: number;\n    let reward: number;\n    let i: number;\n    let best_q: number;\n    let next_x: number;\n    let next_y: number;\n    let nx: number;\n    let ny: number;\n    let q: number;\n    let old_q_value: number;\n    let next_q_value: number;\n    let new_q_value: number;\n    let epsilon: number;\n    "],[1,"\n"],[0,"for "]],"start1":2412,"start2":2412,"length1":497,"length2":9},{"diffs":[[0,"pisode++) {\n"],[-1,"    "],[0,"    //  100 "]],"start1":2454,"start2":2454,"length1":28,"length2":24},{"diffs":[[0,"pisodes\n"],[-1,"    "],[0,"    Ax ="]],"start1":2488,"start2":2488,"length1":20,"length2":16},{"diffs":[[0," = Ax_0\n"],[-1,"    "],[0,"    //  "]],"start1":2502,"start2":2502,"length1":20,"length2":16},{"diffs":[[0,"episode\n    "],[-1,"   "],[-1," "],[0,"Ay = Ay_0\n  "]],"start1":2549,"start2":2549,"length1":28,"length2":24},{"diffs":[[0," = Ay_0\n"],[-1,"    "],[0,"    //  "]],"start1":2563,"start2":2563,"length1":20,"length2":16},{"diffs":[[0,"episode\n    "],[-1," "],[-1,"   "],[0,"episode_rewa"]],"start1":2610,"start2":2610,"length1":28,"length2":24},{"diffs":[[0,"_reward = 0\n"],[-1,"    "],[0,"    led.plot"]],"start1":2629,"start2":2629,"length1":28,"length2":24},{"diffs":[[0,"y, 255)\n"],[-1,"    "],[0,"    paus"]],"start1":2669,"start2":2669,"length1":20,"length2":16},{"diffs":[[0,"use(10)\n    "],[-1,"  "],[-1,"  "],[0,"//  Brief de"]],"start1":2683,"start2":2683,"length1":28,"length2":24},{"diffs":[[0,"ng position\n"],[-1,"    "],[0,"    while (A"]],"start1":2725,"start2":2725,"length1":28,"length2":24},{"diffs":[[0,") {\n        "],[-1," "],[-1,"   "],[0,"//  Run unti"]],"start1":2776,"start2":2776,"length1":28,"length2":24},{"diffs":[[0,"reached\n        "],[-1,"   "],[-1," "],[0,"new_Ax = 0\n     "]],"start1":2810,"start2":2810,"length1":36,"length2":32},{"diffs":[[0," new_Ax = 0\n"],[-1,"    "],[0,"        //  "]],"start1":2825,"start2":2825,"length1":28,"length2":24},{"diffs":[[0,"ore use\n        "],[-1,"  "],[-1,"  "],[0,"new_Ay = 0\n     "]],"start1":2867,"start2":2867,"length1":36,"length2":32},{"diffs":[[0," new_Ay = 0\n"],[-1,"    "],[0,"        //  "]],"start1":2882,"start2":2882,"length1":28,"length2":24},{"diffs":[[0,"ore use\n        "],[-1," "],[-1,"   "],[0,"reward = 0\n     "]],"start1":2924,"start2":2924,"length1":36,"length2":32},{"diffs":[[0," reward = 0\n"],[-1,"    "],[0,"        //  "]],"start1":2939,"start2":2939,"length1":28,"length2":24},{"diffs":[[0,"use\n        "],[-1,"    "],[0,"//  Explorat"]],"start1":2985,"start2":2985,"length1":28,"length2":24},{"diffs":[[0,"ion\n        "],[-1,"  "],[-1,"  "],[0,"if (Math.ran"]],"start1":3025,"start2":3025,"length1":28,"length2":24},{"diffs":[[0," epsilon) {\n"],[-1,"    "],[0,"            "]],"start1":3056,"start2":3056,"length1":28,"length2":24},{"diffs":[[0,"        "],[-1," "],[-1,"   "],[0,"i = rand"]],"start1":3122,"start2":3122,"length1":20,"length2":16},{"diffs":[[0,"t(0, 3)\n"],[-1,"    "],[0,"        "]],"start1":3140,"start2":3140,"length1":20,"length2":16},{"diffs":[[0,"ion\n            "],[-1,"    "],[0,"new_Ax = Ax + mo"]],"start1":3184,"start2":3184,"length1":36,"length2":32},{"diffs":[[0,"[i]\n            "],[-1,"   "],[-1," "],[0,"new_Ay = Ay + mo"]],"start1":3220,"start2":3220,"length1":36,"length2":32},{"diffs":[[0," Ay + move_y[i]\n"],[-1,"    "],[0,"            //  "]],"start1":3244,"start2":3244,"length1":36,"length2":32},{"diffs":[[0,"nds\n            "],[-1," "],[-1,"   "],[0,"if (new_Ax < 0 |"]],"start1":3304,"start2":3304,"length1":36,"length2":32},{"diffs":[[0,"w_Ay > 4) {\n"],[-1,"    "],[0,"            "]],"start1":3368,"start2":3368,"length1":28,"length2":24},{"diffs":[[0,"            "],[-1,"    "],[0,"//  Revert t"]],"start1":3412,"start2":3412,"length1":28,"length2":24},{"diffs":[[0,"                "],[-1,"   "],[-1," "],[0,"new_Ay = Ay\n    "]],"start1":3472,"start2":3472,"length1":36,"length2":32},{"diffs":[[0,"new_Ay = Ay\n"],[-1,"    "],[0,"            "]],"start1":3488,"start2":3488,"length1":28,"length2":24},{"diffs":[[0,"            "],[-1,"    \n    "],[1,"\n"],[0,"        } el"]],"start1":3514,"start2":3514,"length1":33,"length2":25},{"diffs":[[0,"            "],[-1,"    "],[0,"//  Inline g"]],"start1":3544,"start2":3544,"length1":28,"length2":24},{"diffs":[[0,"ion\n            "],[-1,"   "],[-1," "],[0,"best_q = -1000\n "]],"start1":3580,"start2":3580,"length1":36,"length2":32},{"diffs":[[0,"000\n            "],[-1,"    "],[0,"next_x = Ax\n    "]],"start1":3607,"start2":3607,"length1":36,"length2":32},{"diffs":[[0," Ax\n            "],[-1," "],[-1,"   "],[0,"next_y = Ay\n    "]],"start1":3631,"start2":3631,"length1":36,"length2":32},{"diffs":[[0,"next_y = Ay\n"],[-1,"    "],[0,"            "]],"start1":3647,"start2":3647,"length1":28,"length2":24},{"diffs":[[0,"        "],[-1,"    "],[0,"nx = Ax "]],"start1":3705,"start2":3705,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1,"  "],[-1,"  "],[0,"ny = Ay "]],"start1":3741,"start2":3741,"length1":20,"length2":16},{"diffs":[[0," Ay + move_y[i]\n"],[-1,"    "],[0,"                "]],"start1":3753,"start2":3753,"length1":36,"length2":32},{"diffs":[[0,"        "],[-1,"    "],[0,"q = Qtab"]],"start1":3847,"start2":3847,"length1":20,"length2":16},{"diffs":[[0,"ny][nx]\n"],[-1,"    "],[0,"        "]],"start1":3866,"start2":3866,"length1":20,"length2":16},{"diffs":[[0,"                "],[-1,"    "],[0,"best_q = q\n     "]],"start1":3920,"start2":3920,"length1":36,"length2":32},{"diffs":[[0,"t_q = q\n"],[-1,"    "],[0,"        "]],"start1":3939,"start2":3939,"length1":20,"length2":16},{"diffs":[[0,"                "],[-1,"    "],[0,"next_y = ny\n    "]],"start1":3991,"start2":3991,"length1":36,"length2":32},{"diffs":[[0,"            "],[-1," "],[-1,"   }\n    "],[1,"}\n"],[0,"            "]],"start1":4027,"start2":4027,"length1":34,"length2":26},{"diffs":[[0,"            "],[-1,"    }\n   "],[1,"}\n"],[0,"            "]],"start1":4066,"start2":4066,"length1":33,"length2":26},{"diffs":[[0,"                "],[-1," \n    "],[1,"\n"],[0,"            }\n  "]],"start1":4080,"start2":4080,"length1":38,"length2":33},{"diffs":[[0," \n            }\n"],[-1,"    "],[0,"            new_"]],"start1":4095,"start2":4095,"length1":36,"length2":32},{"diffs":[[0," next_x\n"],[-1,"    "],[0,"        "]],"start1":4131,"start2":4131,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1,"    }\n  "],[1,"}\n"],[0,"        "],[-1,"  \n    "],[1,"\n"],[0,"        "]],"start1":4167,"start2":4167,"length1":39,"length2":27},{"diffs":[[0," reward\n        "],[-1,"  "],[-1,"  "],[0,"if (new_Ax == go"]],"start1":4207,"start2":4207,"length1":36,"length2":32},{"diffs":[[0,") {\n            "],[-1,"    "],[0,"reward = 10\n    "]],"start1":4263,"start2":4263,"length1":36,"length2":32},{"diffs":[[0," 10\n        "],[-1," "],[-1,"   "],[0,"} else if (m"]],"start1":4287,"start2":4287,"length1":28,"length2":24},{"diffs":[[0,") {\n            "],[-1,"   "],[-1," "],[0,"reward = -5\n    "]],"start1":4335,"start2":4335,"length1":36,"length2":32},{"diffs":[[0," -5\n        "],[-1,"    "],[0,"} else {\n   "]],"start1":4359,"start2":4359,"length1":28,"length2":24},{"diffs":[[0,"e {\n            "],[-1,"  "],[-1,"  "],[0,"reward = -1\n    "]],"start1":4376,"start2":4376,"length1":36,"length2":32},{"diffs":[[0,"rd = -1\n"],[-1,"    "],[0,"        "]],"start1":4396,"start2":4396,"length1":20,"length2":16},{"diffs":[[0,"  }\n        "],[-1,"    \n    "],[1,"\n"],[0,"        epis"]],"start1":4410,"start2":4410,"length1":33,"length2":25},{"diffs":[[0,"ard\n        "],[-1,"    "],[0,"//  Q-value "]],"start1":4468,"start2":4468,"length1":28,"length2":24},{"diffs":[[0,"        "],[-1," "],[-1,"   "],[0,"old_q_va"]],"start1":4499,"start2":4499,"length1":20,"length2":16},{"diffs":[[0,"ble[Ay][Ax]\n"],[-1,"    "],[0,"        next"]],"start1":4524,"start2":4524,"length1":28,"length2":24},{"diffs":[[0,"Ay][new_Ax]\n"],[-1,"    "],[0,"        new_"]],"start1":4570,"start2":4570,"length1":28,"length2":24},{"diffs":[[0,"_value)\n        "],[-1,"   "],[-1," "],[0,"//  Update Q-tab"]],"start1":4664,"start2":4664,"length1":36,"length2":32},{"diffs":[[0,"Q-table\n"],[-1,"    "],[0,"        "]],"start1":4691,"start2":4691,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1,"  "],[-1,"  "],[0,"// updat"]],"start1":4760,"start2":4760,"length1":20,"length2":16},{"diffs":[[0,"o 100th\n"],[-1,"    "],[0,"        "]],"start1":4802,"start2":4802,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1,"    "],[0,"update_a"]],"start1":4855,"start2":4855,"length1":20,"length2":16},{"diffs":[[0,"Ay)\n        "],[-1,"   "],[-1," "],[0,"//  Move to "]],"start1":4905,"start2":4905,"length1":28,"length2":24},{"diffs":[[0,"        "],[-1,"    "],[0,"Ax = new"]],"start1":4942,"start2":4942,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1,"  "],[-1,"  "],[0,"Ay = new"]],"start1":4962,"start2":4962,"length1":20,"length2":16},{"diffs":[[0," new_Ay\n    "],[-1,"    }\n    "],[1,"}\n"],[0,"    //  Redu"]],"start1":4974,"start2":4974,"length1":34,"length2":26},{"diffs":[[0,"ay)\n    "],[-1,"   "],[-1," "],[0,"epsilon "]],"start1":5046,"start2":5046,"length1":20,"length2":16},{"diffs":[[0,"on_min)\n"],[-1,"    "],[0,"    //  "]],"start1":5103,"start2":5103,"length1":20,"length2":16},{"diffs":[[0,"rogress\n"],[-1,"    "],[0,"    cons"]],"start1":5134,"start2":5134,"length1":20,"length2":16},{"diffs":[[0,"reward)\n"],[-1,"    }\n    "],[1,"}\n"],[0,"//  Opti"]],"start1":5287,"start2":5287,"length1":26,"length2":18},{"diffs":[[0,"pletion\n"],[-1,"    "],[0,"console."]],"start1":5319,"start2":5319,"length1":20,"length2":16},{"diffs":[[0,"og(\""],[-1,"Training Complete\")\n    "],[1,"Done\")\n"],[0,"for "]],"start1":5336,"start2":5336,"length1":32,"length2":15},{"diffs":[[0,"f Qtable) {\n"],[-1,"    "],[0,"    console."]],"start1":5361,"start2":5361,"length1":28,"length2":24},{"diffs":[[0,"ow)\n"],[-1,"    }\n})"],[1,"}"],[0,"\nfun"]],"start1":5390,"start2":5390,"length1":16,"length2":9}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"odes"],[-1," on button A\ndef on_button_pressed_a():\n    "],[1,"\n"],[0,"for "]],"start1":1910,"start2":1910,"length1":52,"length2":9},{"diffs":[[0,"pisodes\n"],[-1,"    "],[0,"    Ax ="]],"start1":1958,"start2":1958,"length1":20,"length2":16},{"diffs":[[0,"the episode\n"],[-1,"    "],[0,"    Ay = Ay_"]],"start1":2010,"start2":2010,"length1":28,"length2":24},{"diffs":[[0,"episode\n"],[-1,"    "],[0,"    epis"]],"start1":2070,"start2":2070,"length1":20,"length2":16},{"diffs":[[0," 0\n    \n    "],[-1," "],[-1,"   "],[0,"led.plot_bri"]],"start1":2098,"start2":2098,"length1":28,"length2":24},{"diffs":[[0,"y, 255)\n"],[-1,"    "],[0,"    paus"]],"start1":2135,"start2":2135,"length1":20,"length2":16},{"diffs":[[0,"   \n    "],[-1,"  "],[-1,"  "],[0,"while (A"]],"start1":2199,"start2":2199,"length1":20,"length2":16},{"diffs":[[0,"reached\n        "],[-1,"    "],[0,"new_Ax = 0  # De"]],"start1":2266,"start2":2266,"length1":36,"length2":32},{"diffs":[[0,"ore use\n        "],[-1,"   "],[-1," "],[0,"new_Ay = 0  # De"]],"start1":2314,"start2":2314,"length1":36,"length2":32},{"diffs":[[0," before use\n"],[-1,"    "],[0,"        rewa"]],"start1":2358,"start2":2358,"length1":28,"length2":24},{"diffs":[[0,"        "],[-1,"    "],[0,"# Explor"]],"start1":2419,"start2":2419,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1," "],[-1,"   "],[0,"if Math."]],"start1":2457,"start2":2457,"length1":20,"length2":16},{"diffs":[[0,"psilon:\n"],[-1,"    "],[0,"        "]],"start1":2485,"start2":2485,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1,"  "],[-1,"  "],[0,"i = rand"]],"start1":2545,"start2":2545,"length1":20,"length2":16},{"diffs":[[0,"ion\n            "],[-1,"    "],[0,"new_Ax = Ax + mo"]],"start1":2594,"start2":2594,"length1":36,"length2":32},{"diffs":[[0,"[i]\n            "],[-1,"   "],[-1," "],[0,"new_Ay = Ay + mo"]],"start1":2630,"start2":2630,"length1":36,"length2":32},{"diffs":[[0,"+ move_y[i]\n"],[-1,"    "],[0,"            "]],"start1":2658,"start2":2658,"length1":28,"length2":24},{"diffs":[[0,"s within bounds\n"],[-1,"    "],[0,"            if n"]],"start1":2700,"start2":2700,"length1":36,"length2":32},{"diffs":[[0,"                "],[-1,"    "],[0,"new_Ax = Ax  # R"]],"start1":2785,"start2":2785,"length1":36,"length2":32},{"diffs":[[0,"                "],[-1,"    "],[0,"new_Ay = Ay\n    "]],"start1":2860,"start2":2860,"length1":36,"length2":32},{"diffs":[[0,"Ay = Ay\n"],[-1,"    "],[0,"        "]],"start1":2880,"start2":2880,"length1":20,"length2":16},{"diffs":[[0,"    "],[-1,"   "],[1,"# Inline"],[0," greedy"],[-1,"_"],[1," "],[0,"sele"]],"start1":2910,"start2":2910,"length1":19,"length2":24},{"diffs":[[0,"tion"],[-1,"(Ax,Ay) #takes the step toward the highest Q-value\n\n    "],[1,"\n            best_q = -1000\n            next_x = Ax\n            next_y = Ay\n            for i in range(4):\n                nx = Ax + move_x[i]\n                ny = Ay + move_y[i]\n                if (0 <= nx <= 4 and 0 <= ny <= 4):\n                    q = Qtable[ny][nx]\n                    if q > best_q:\n                        best_q = q\n                        next_x = nx\n                        next_y = ny\n            new_Ax = next_x\n            new_Ay = next_y\n\n"],[0,"    "]],"start1":2935,"start2":2935,"length1":64,"length2":477},{"diffs":[[0," reward\n"],[-1,"    "],[0,"        "]],"start1":3427,"start2":3427,"length1":20,"length2":16},{"diffs":[[0,"y):\n            "],[-1,"    "],[0,"reward = 10\n    "]],"start1":3483,"start2":3483,"length1":36,"length2":32},{"diffs":[[0,"        "],[-1," "],[-1,"   "],[0,"elif maz"]],"start1":3511,"start2":3511,"length1":20,"length2":16},{"diffs":[[0," 1:\n            "],[-1,"    "],[0,"reward = -5\n    "]],"start1":3547,"start2":3547,"length1":36,"length2":32},{"diffs":[[0," -5\n        "],[-1,"  "],[-1,"  "],[0,"else:\n"],[-1,"    "],[0,"            "]],"start1":3571,"start2":3571,"length1":38,"length2":30},{"diffs":[[0,"        "],[-1,"   "],[-1," "],[0,"episode_"]],"start1":3614,"start2":3614,"length1":20,"length2":16},{"diffs":[[0,"reward\n\n"],[-1,"    "],[0,"        "]],"start1":3656,"start2":3656,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1,"    "],[0,"old_q_va"]],"start1":3689,"start2":3689,"length1":20,"length2":16},{"diffs":[[0,"Ax]\n        "],[-1," "],[-1,"   "],[0,"next_q_value"]],"start1":3722,"start2":3722,"length1":28,"length2":24},{"diffs":[[0,"Ay][new_Ax]\n"],[-1,"    "],[0,"        new_"]],"start1":3760,"start2":3760,"length1":28,"length2":24},{"diffs":[[0,"value)\n\n        "],[-1,"  "],[-1,"  "],[0,"# Update Q-table"]],"start1":3855,"start2":3855,"length1":36,"length2":32},{"diffs":[[0,"Q-table\n"],[-1,"    "],[0,"        "]],"start1":3880,"start2":3880,"length1":20,"length2":16},{"diffs":[[0,"th\n\n        "],[-1,"   "],[-1," "],[0,"# Update age"]],"start1":3982,"start2":3982,"length1":28,"length2":24},{"diffs":[[0,"display\n"],[-1,"    "],[0,"        "]],"start1":4021,"start2":4021,"length1":20,"length2":16},{"diffs":[[0,"ew_Ay)\n\n"],[-1,"    "],[0,"        "]],"start1":4076,"start2":4076,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1,"    "],[0,"Ax = new"]],"start1":4115,"start2":4115,"length1":20,"length2":16},{"diffs":[[0,"        "],[-1," "],[-1,"   "],[0,"Ay = new"]],"start1":4135,"start2":4135,"length1":20,"length2":16},{"diffs":[[0,"Ay\n\n    "],[-1,"  "],[-1,"  "],[0,"# Reduce"]],"start1":4152,"start2":4152,"length1":20,"length2":16},{"diffs":[[0,"ay)\n    "],[-1,"   "],[-1," "],[0,"epsilon "]],"start1":4212,"start2":4212,"length1":20,"length2":16},{"diffs":[[0,"n_min)\n\n    "],[-1,"   "],[-1," "],[0,"# Debug epis"]],"start1":4265,"start2":4265,"length1":28,"length2":24},{"diffs":[[0,"rogress\n"],[-1,"    "],[0,"    prin"]],"start1":4294,"start2":4294,"length1":20,"length2":16},{"diffs":[[0,"eward)\n\n"],[-1,"    "],[0,"# Option"]],"start1":4436,"start2":4436,"length1":20,"length2":16},{"diffs":[[0,"ion\n"],[-1,"    "],[0,"print(\""],[-1,"Training Complete\")\n    "],[1,"Done\")\n"],[0,"for "]],"start1":4468,"start2":4468,"length1":43,"length2":22},{"diffs":[[0,"row)"],[-1,"\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n\n# Inline greedy selection\ndef greedy_selection():\n    best_q = -1000\n    next_x = Ax\n    next_y = Ay\n    for i in range(4):\n        nx = Ax + move_x[i]\n        ny = Ay + move_y[i]\n        if (0 <= nx <= 4 and 0 <= ny <= 4):\n            q = Qtable[ny][nx]\n            if q > best_q:\n                best_q = q\n                next_x = nx\n                next_y = ny\n    new_Ax = next_x\n    new_Ay = next_y"],[0,"\n\nde"]],"start1":4519,"start2":4519,"length1":470,"length2":8}]}]},{"timestamp":1740759835390,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," let i: number;\n"],[1,"    let best_q: number;\n    let next_x: number;\n    let next_y: number;\n    let nx: number;\n    let ny: number;\n    let q: number;\n"],[0,"    let old_q_va"]],"start1":2323,"start2":2323,"length1":32,"length2":163},{"diffs":[[0,"        "],[1,"//  Inline "],[0,"greedy"],[-1,"_"],[1," "],[0,"selectio"]],"start1":3850,"start2":3850,"length1":23,"length2":34},{"diffs":[[0,"tion"],[-1,"(Ax, Ay)\n            }\n            \n            // takes the step toward the highest Q-value"],[1,"\n                best_q = -1000\n                next_x = Ax\n                next_y = Ay\n                for (i = 0; i < 4; i++) {\n                    nx = Ax + move_x[i]\n                    ny = Ay + move_y[i]\n                    if (0 <= nx && nx <= 4 && (0 <= ny && ny <= 4)) {\n                        q = Qtable[ny][nx]\n                        if (q > best_q) {\n                            best_q = q\n                            next_x = nx\n                            next_y = ny\n                        }\n                        \n                    }\n                    \n                }\n                new_Ax = next_x\n                new_Ay = next_y\n            }\n            "],[0,"\n   "]],"start1":3881,"start2":3881,"length1":100,"length2":694},{"diffs":[[0,"\n})\n"],[-1,"//  Inline greedy selection\nfunction greedy_selection(Ax: number, Ay: number) {\n    let nx: number;\n    let ny: number;\n    let q: number;\n    let best_q = -1000\n    let next_x = Ax\n    let next_y = Ay\n    for (let i = 0; i < 4; i++) {\n        nx = Ax + move_x[i]\n        ny = Ay + move_y[i]\n        if (0 <= nx && nx <= 4 && (0 <= ny && ny <= 4)) {\n            q = Qtable[ny][nx]\n            if (q > best_q) {\n                best_q = q\n                next_x = nx\n                next_y = ny\n            }\n            \n        }\n        \n    }\n    let new_Ax = next_x\n    let new_Ay = next_y\n}\n\n"],[0,"func"]],"start1":5926,"start2":5926,"length1":605,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"lection("],[-1,"Ax,Ay"],[0,"):\n    b"]],"start1":4438,"start2":4438,"length1":21,"length2":16}]}]},{"timestamp":1740759849510,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[1,"from microbit import *\n\n# Define Q-table as a 5x5 grid with a single Q-value per state\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\n\n# Define possible moves\nmove_x = [0, 0, -1, 1]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 1, 1, 1, 1]\n]\n\n# Agent starting position and maze randomization\nAx_0 = 0\nAy_0 = randint(1, 3)\nmaze[Ay_0][Ax_0] = 0  # Open up the maze at the starting position\nmaze[randint(1, 3)][2] = 0  # Open up a random passage in the center wall\ngoal_x = 4  # Goal position\ngoal_y = randint(1, 3)\nmaze[goal_y][goal_x] = 0  # Open up the maze at the goal position\n\n# Q-learning parameters\nalpha = 0.3  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.97  # Multiply epsilon by this after each episode\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1: \n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(50)  # Brief delay to make movement visible\n    if (new_x == goal_x and new_y == goal_y):\n        play_ding()\n        for i in range(7):\n            led.toggle(goal_x, goal_y)\n            pause(50)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n\n\n# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears\n\n# Run multiple training episodes on button A\ndef on_button_pressed_a():\n    for episode in range(10):  # 100 training episodes\n        Ax = Ax_0  # Declare Ax at the start of the episode\n        Ay = Ay_0  # Declare Ay at the start of the episode\n        episode_reward = 0\n    \n        led.plot_brightness(Ax, Ay, 255)\n        pause(10)  # Brief delay to show starting position\n    \n        while (Ax != goal_x or Ay != goal_y):  # Run until goal is reached\n            new_Ax = 0  # Declare new_Ax before use\n            new_Ay = 0  # Declare new_Ay before use\n            reward = 0  # Declare reward before use\n\n            # Exploration vs Exploitation\n            if Math.random() < epsilon:\n                # Random move (no valid move check)\n                i = randint(0, 3)  # Pick a random direction\n                new_Ax = Ax + move_x[i]\n                new_Ay = Ay + move_y[i]\n                # Ensure move stays within bounds\n                if new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4:\n                    new_Ax = Ax  # Revert to current position if out of bounds\n                    new_Ay = Ay\n            else:\n                greedy_selection(Ax,Ay) #takes the step toward the highest Q-value\n\n            # Determine reward\n            if (new_Ax == goal_x and new_Ay == goal_y):\n                reward = 10\n            elif maze[new_Ay][new_Ax] == 1:\n                reward = -5\n            else:\n                reward = -1\n\n            episode_reward = episode_reward + reward\n\n            # Q-value update\n            old_q_value = Qtable[Ay][Ax]\n            next_q_value = Qtable[new_Ay][new_Ax]\n            new_q_value = old_q_value + alpha * (reward + gamma * next_q_value - old_q_value)\n\n            # Update Q-table\n            Qtable[Ay][Ax] = Math.round(new_q_value*100)/100 #update Qtable element rounded to 100th\n\n            # Update agent position on display\n            update_agent_position(Ax, Ay, new_Ax, new_Ay)\n\n            # Move to new position\n            Ax = new_Ax\n            Ay = new_Ay\n\n        # Reduce epsilon after each episode (exponential decay)\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n        # Debug episode progress\n        print(\"Episode: \" + str(episode + 1) + \"    Epsilon: \" + str(Math.round(epsilon * 1000) / 1000) + \"    Total Reward: \" + episode_reward)\n\n    # Optional: Show completion\n    print(\"Training Complete\")\n    for row in Qtable:\n        print(row)\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n\n# Inline greedy selection\ndef greedy_selection(Ax,Ay):\n    best_q = -1000\n    next_x = Ax\n    next_y = Ay\n    for i in range(4):\n        nx = Ax + move_x[i]\n        ny = Ay + move_y[i]\n        if (0 <= nx <= 4 and 0 <= ny <= 4):\n            q = Qtable[ny][nx]\n            if q > best_q:\n                best_q = q\n                next_x = nx\n                next_y = ny\n    new_Ax = next_x\n    new_Ay = next_y\n\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526,\n            2351,\n            50,\n            50,\n            10,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            1385,\n            2395,\n            50,\n            50,\n            500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.CURVE),\n        music.PlaybackMode.UNTIL_DONE)"]],"start1":0,"start2":0,"length1":0,"length2":5374}]}]},{"timestamp":1740800967260,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[-1,"from microbit import *\nimport music\n\n# Define Q-table as a 5x5 grid with a single Q-value per state\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\n\n# Define possible moves\nmove_x = [0, 0, -1, 1]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 1, 1, 1, 1]\n]\n\n# Agent starting position and maze randomization\nAx_0 = 0\nAy_0 = randint(1, 3)\nmaze[Ay_0][Ax_0] = 0  # Open up the maze at the starting position\nmaze[randint(1, 3)][2] = 0  # Open up a random passage in the center wall\ngoal_x = 4  # Goal position\ngoal_y = randint(1, 3)\nmaze[goal_y][goal_x] = 0  # Open up the maze at the goal position\n\n# Q-learning parameters\nalpha = 0.3  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.97  # Multiply epsilon by this after each episode\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1: \n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(50)  # Brief delay to make movement visible\n    if (new_x == goal_x and new_y == goal_y):\n        play_ding()\n        for i in range(7):\n            led.toggle(goal_x, goal_y)\n            pause(50)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n\n# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears\n\n# Run multiple training episodes on button A\ndef on_button_pressed_a():\n    global epsilon  # Need to declare epsilon as global to modify it\n    \n    for episode in range(10):  # 10 training episodes\n        Ax = Ax_0\n        Ay = Ay_0\n        episode_reward = 0\n    \n        led.plot_brightness(Ax, Ay, 255)\n        pause(10)  # Brief delay to show starting position\n    \n        while (Ax != goal_x or Ay != goal_y):  # Run until goal is reached\n            # Exploration vs Exploitation\n            if Math.random() < epsilon:\n                # Random move\n                i = randint(0, 3)\n                new_Ax = Ax + move_x[i]\n                new_Ay = Ay + move_y[i]\n                # Ensure move stays within bounds\n                if new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4:\n                    new_Ax = Ax\n                    new_Ay = Ay\n            else:\n                new_Ax, new_Ay = greedy_selection(Ax, Ay)  # Get new coordinates from greedy selection\n\n            # Determine reward\n            if (new_Ax == goal_x and new_Ay == goal_y):\n                reward = 10\n            elif maze[new_Ay][new_Ax] == 1:\n                reward = -5\n            else:\n                reward = -1\n\n            episode_reward += reward\n\n            # Q-value update\n            old_q_value = Qtable[Ay][Ax]\n            next_q_value = Qtable[new_Ay][new_Ax]\n            new_q_value = old_q_value + alpha * (reward + gamma * next_q_value - old_q_value)\n            Qtable[Ay][Ax] = Math.round(new_q_value * 100) / 100  # Round to 2 decimal places\n\n            # Update agent position on display\n            update_agent_position(Ax, Ay, new_Ax, new_Ay)\n\n            # Move to new position\n            Ax = new_Ax\n            Ay = new_Ay\n\n        # Reduce epsilon after each episode\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n        # Debug episode progress\n        print(\"Episode:\", episode + 1, \"Epsilon:\", Math.round(epsilon * 1000) / 1000, \"Reward:\", episode_reward)\n\n    print(\"Training Complete\")\n    for row in Qtable:\n        print(row)\n\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n# Greedy selection function\ndef greedy_selection(Ax, Ay):\n    best_q = -1000\n    next_x = Ax\n    next_y = Ay\n    for i in range(4):\n        nx = Ax + move_x[i]\n        ny = Ay + move_y[i]\n        if 0 <= nx <= 4 and 0 <= ny <= 4:\n            q = Qtable[ny][nx]\n            if q > best_q:\n                best_q = q\n                next_x = nx\n                next_y = ny\n    return next_x, next_y\n\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526, 2351, 50, 50, 10,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            1385, 2395, 50, 50, 500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.CURVE),\n        music.PlaybackMode.UNTIL_DONE)"]],"start1":0,"start2":0,"length1":4896,"length2":0}]}]},{"timestamp":1740800967337,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"er;\n"],[-1,"    let i: number;\n"],[0,"    "]],"start1":2244,"start2":2244,"length1":27,"length2":8},{"diffs":[[0,"reward: number;\n"],[1,"    let i: number;\n"],[0,"    let old_q_va"]],"start1":2304,"start2":2304,"length1":32,"length2":51},{"diffs":[[0,"    "],[-1,"\n    //  Need to declare epsilon as global to modify it"],[1,"let epsilon: number;"],[0,"\n   "]],"start1":2427,"start2":2427,"length1":63,"length2":28},{"diffs":[[0,"  //  10"],[1,"0"],[0," trainin"]],"start1":2511,"start2":2511,"length1":16,"length2":17},{"diffs":[[0,"    "],[-1,"Ay = Ay_0"],[1,"//  Declare Ax at the start of the episode\n        Ay = Ay_0\n        //  Declare Ay at the start of the episode"],[0,"\n   "]],"start1":2561,"start2":2561,"length1":17,"length2":119},{"diffs":[[0,"reached\n"],[1,"            new_Ax = 0\n            //  Declare new_Ax before use\n            new_Ay = 0\n            //  Declare new_Ay before use\n            reward = 0\n            //  Declare reward before use\n"],[0,"        "]],"start1":2893,"start2":2893,"length1":16,"length2":211},{"diffs":[[0,"dom move"],[1," (no valid move check)"],[0,"\n       "]],"start1":3206,"start2":3206,"length1":16,"length2":38},{"diffs":[[0,"t(0, 3)\n"],[1,"                //  Pick a random direction\n"],[0,"        "]],"start1":3263,"start2":3263,"length1":16,"length2":60},{"diffs":[[0,"new_Ax = Ax\n"],[1,"                    //  Revert to current position if out of bounds\n"],[0,"            "]],"start1":3543,"start2":3543,"length1":24,"length2":92},{"diffs":[[0,"    "],[-1," let [new_Ax, new_Ay] ="],[0," gre"]],"start1":3722,"start2":3722,"length1":31,"length2":8},{"diffs":[[0," // "],[-1," Get new coordinates from greedy selection"],[1,"takes the step toward the highest Q-value"],[0,"\n   "]],"start1":3790,"start2":3790,"length1":50,"length2":49},{"diffs":[[0,"ard "],[-1,"+"],[0,"="],[1," episode_reward +"],[0," rew"]],"start1":4132,"start2":4132,"length1":10,"length2":26},{"diffs":[[0,"_value)\n"],[1,"            //  Update Q-table\n"],[0,"        "]],"start1":4370,"start2":4370,"length1":16,"length2":47},{"diffs":[[0," // "],[-1," Round to 2 decimal places"],[1,"update Qtable element rounded to 100th"],[0,"\n   "]],"start1":4485,"start2":4485,"length1":34,"length2":46},{"diffs":[[0,"ter each episode"],[1," (exponential decay)"],[0,"\n        epsilon"]],"start1":4759,"start2":4759,"length1":32,"length2":52},{"diffs":[[0," 1)) + \""],[1,"  "],[0,"  Epsilo"]],"start1":4946,"start2":4946,"length1":16,"length2":18},{"diffs":[[0,"lon: \" +"],[1," (\"\" +"],[0," Math.ro"]],"start1":4962,"start2":4962,"length1":16,"length2":22},{"diffs":[[0,") / 1000"],[1,")"],[0," + \"  "],[1,"  Total "],[0,"Reward: "]],"start1":5002,"start2":5002,"length1":22,"length2":31},{"diffs":[[0,"ward)\n    }\n"],[1,"    //  Optional: Show completion\n"],[0,"    console."]],"start1":5047,"start2":5047,"length1":24,"length2":58},{"diffs":[[0,"\n})\n//  "],[-1,"G"],[1,"Inline g"],[0,"reedy se"]],"start1":5190,"start2":5190,"length1":17,"length2":24},{"diffs":[[0,"election"],[-1," function"],[0,"\nfunctio"]],"start1":5213,"start2":5213,"length1":25,"length2":16},{"diffs":[[0,"ber)"],[-1,": number[]"],[0," {\n "]],"start1":5267,"start2":5267,"length1":18,"length2":8},{"diffs":[[0,"    "],[-1,"return [next_x,"],[1,"let new_Ax = next_x\n    let new_Ay ="],[0," next_y"],[-1,"]"],[0,"\n}\n\n"]],"start1":5740,"start2":5740,"length1":31,"length2":51}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"_y][old_x] == 1:"],[1," "],[0,"\n        led.plo"]],"start1":1370,"start2":1370,"length1":32,"length2":33},{"diffs":[[0,"ode:"],[-1," \" + str("],[1,"\", "],[0,"epis"]],"start1":3805,"start2":3805,"length1":17,"length2":11},{"diffs":[[0," + 1"],[-1,") + \"  "],[1,", \""],[0,"Epsilon:"],[-1," \" +"],[1,"\","],[0," Mat"]],"start1":3819,"start2":3819,"length1":27,"length2":21},{"diffs":[[0,"1000"],[-1," + \"  "],[1,", \""],[0,"Reward:"],[-1," \" +"],[1,"\","],[0," epi"]],"start1":3866,"start2":3866,"length1":25,"length2":20}]}]},{"timestamp":1740801091665,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"sed_a)\n\n"],[-1,"\n\n\n"],[0,"# Greedy"]],"start1":4038,"start2":4038,"length1":19,"length2":16}]}]},{"timestamp":1740802072129,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"a)\n\n"],[-1,"def on_button_pressed_b():\n    basic.show_string(\"Hello!\")\ninput.on_button_pressed(Button.B, on_button_pressed_b)"],[0,"\n\n\n#"]],"start1":4042,"start2":4042,"length1":121,"length2":8}]}]},{"timestamp":1740802073639,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"\n})\n"],[-1,"//  on button B run the  onbest path through the maze based on trained Q-table \n//  Get new coordinates from greedy selection\ninput.onButtonPressed(Button.B, function on_button_pressed_b() {\n    let Ax = Ax_0\n    let Ay = Ay_0\n    let episode_reward = 0\n    led.plotBrightness(Ax, Ay, 255)\n    pause(10)\n    //  Brief delay to show starting position\n    while (Ax != goal_x || Ay != goal_y) {\n        //  Run until goal is reached\n        let [new_Ax, new_Ay] = greedy_selection(Ax, Ay)\n    }\n})\n"],[0,"//  "]],"start1":4687,"start2":4687,"length1":504,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"a)\n\n"],[-1,"# on button B run the  onbest path through the maze based on trained Q-table \ndef on_button_pressed_b():\n    Ax = Ax_0\n    Ay = Ay_0\n    episode_reward = 0\n     \n    led.plot_brightness(Ax, Ay, 255)\n    pause(10)  # Brief delay to "],[1,"def on_button_pressed_b():\n    basic."],[0,"show"],[-1," starting position\n    while (Ax != goal_x or Ay != goal_y):  # Run until goal is reached\n        new_Ax, new_Ay = greedy_selection(Ax, Ay)  # Get new coordinates from greedy selection\n        "],[1,"_string(\"Hello!\")"],[0,"\ninp"]],"start1":4042,"start2":4042,"length1":436,"length2":66}]}]},{"timestamp":1740802318471,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"update_agent_position(old_x, old_y, new_x, new_y)"],[0,"\ninp"]],"start1":4470,"start2":4470,"length1":57,"length2":8}]}]},{"timestamp":1740803054419,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ion("],[-1,"Ax, A"],[1,"old_x, old_"],[0,"y, new_"],[-1,"A"],[0,"x, new_"],[-1,"A"],[0,"y)\n"],[-1,"\n"],[0,"inpu"]],"start1":4492,"start2":4492,"length1":33,"length2":36}]}]},{"timestamp":1740803087680,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," 0; i < "],[-1,"5"],[1,"7"],[0,"; i++) {"]],"start1":1781,"start2":1781,"length1":17,"length2":17},{"diffs":[[0,"    "],[-1,"reward = get_reward("],[1,"if (new_Ax == goal_x && new_Ay == goal_y) {\n                reward = 10\n            } else if (maze["],[0,"new_A"],[-1,"x, "],[1,"y]["],[0,"new_A"],[-1,"y)"],[1,"x] == 1) {\n                reward = -5\n            } else {\n                reward = -1\n            }\n            "],[0,"\n   "]],"start1":3504,"start2":3504,"length1":43,"length2":235},{"diffs":[[0,"new_Ax, new_Ay)\n"],[1,"            //  Move to new position\n"],[0,"            Ax ="]],"start1":4187,"start2":4187,"length1":32,"length2":69},{"diffs":[[0,"-table \n"],[1,"//  Get new coordinates from greedy selection\n"],[0,"input.on"]],"start1":4763,"start2":4763,"length1":16,"length2":62},{"diffs":[[0,") {\n"],[-1,"    let reward: number;\n"],[0,"    "]],"start1":4878,"start2":4878,"length1":32,"length2":8},{"diffs":[[0,"    "],[-1,"    //  Get new coordinates from greedy selection\n        reward = get_reward(new_Ax, new_Ay)\n        //  Determine reward\n        episode_reward += reward\n        update_agent_position(Ax, Ay, new_Ax, new_Ay)\n        //  move the agent to next position\n        Ax = new_Ax\n        Ay = new_Ay\n    }\n})\n//  Get reward of the next Move\nfunction get_reward(new_Ax: number, new_Ay: number): number {\n    let reward: number;\n    if (new_Ax == goal_x && new_Ay == goal_y) {\n        reward = 10\n    } else if (maze[new_Ay][new_Ax] == 1) {\n        reward = -5\n    } else {\n        reward = -1\n    }\n    \n    return reward\n"],[0,"}\n"],[1,"})"],[0,"\n// "]],"start1":5178,"start2":5178,"length1":625,"length2":12}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," for i in range("],[-1,"5"],[1,"7"],[0,"):\n            l"]],"start1":1645,"start2":1645,"length1":33,"length2":33},{"diffs":[[0,"    "],[-1,"reward = get_reward("],[1,"if (new_Ax == goal_x and new_Ay == goal_y):\n                reward = 10\n            elif maze["],[0,"new_A"],[-1,"x, "],[1,"y]["],[0,"new_A"],[-1,"y)"],[1,"x] == 1:\n                reward = -5\n            else:\n                reward = -1\n"],[0,"\n   "]],"start1":2915,"start2":2915,"length1":43,"length2":198},{"diffs":[[0," new_Ay)"],[1,"\n\n            # Move to new position"],[0,"\n       "]],"start1":3553,"start2":3553,"length1":16,"length2":52},{"diffs":[[0,"_reward = 0\n"],[1,"     \n"],[0,"    led.plot"]],"start1":4190,"start2":4190,"length1":24,"length2":30},{"diffs":[[0,"    "],[-1,"reward = get_reward(new_Ax, new_Ay)     # Determine reward\n        episode_reward += reward\n        update_agent_position(Ax, Ay, new_Ax, new_Ay)  # move the agent to next position\n        Ax = new_Ax\n        Ay = new_Ay\n    \ninput.on_button_pressed(Button.B, on_button_pressed_b)\n\n\n# Get reward of the next Move\ndef get_reward(new_Ax, new_Ay):\n    if (new_Ax == goal_x and new_Ay == goal_y):\n        reward = 10\n    elif maze[new_Ay][new_Ax] == 1:\n        reward = -5\n    else:\n        reward = -1\n    return reward    "],[1,"update_agent_position(Ax, Ay, new_Ax, new_Ay)\n\ninput.on_button_pressed(Button.B, on_button_pressed_b)\n"],[0,"\n\n# "]],"start1":4470,"start2":4470,"length1":528,"length2":110}]}]},{"timestamp":1740803687643,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"ode < 10"],[-1,"0"],[0,"; episod"]],"start1":2517,"start2":2517,"length1":17,"length2":16},{"diffs":[[0," Epsilon"],[-1," ="],[1,":"],[0," \" + Mat"]],"start1":4274,"start2":4274,"length1":18,"length2":17},{"diffs":[[0,"1000 + \"  Reward"],[-1," ="],[1,":"],[0," \" + episode_rew"]],"start1":4317,"start2":4317,"length1":34,"length2":33},{"diffs":[[0,"ion\n"],[-1,"    let steps = 0\n"],[0,"    "]],"start1":4786,"start2":4786,"length1":26,"length2":8},{"diffs":[[0,"    "],[-1,"pause(100)\n        //  move slower through the smart run for better viewing\n        Ax = new_Ax\n        Ay = new_Ay\n        steps = steps + 1\n    }\n    // count the number of steps\n    led.toggle(goal_x, goal_y)\n    //  keep the agent sprite lit when finishing the run\n    console.log(\"Intelligent run complete in \" + steps + \" steps. \" + \"  Reward = \" + episode_reward)"],[1,"Ax = new_Ax\n        Ay = new_Ay\n    }"],[0,"\n})\n"]],"start1":5189,"start2":5189,"length1":378,"length2":45}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"range(10"],[-1,"0"],[0,"):  # 10"]],"start1":2058,"start2":2058,"length1":17,"length2":16},{"diffs":[[0," Epsilon"],[-1," ="],[1,":"],[0," \" + Mat"]],"start1":3643,"start2":3643,"length1":18,"length2":17},{"diffs":[[0,"1000 + \"  Reward"],[-1," ="],[1,":"],[0," \" + episode_rew"]],"start1":3686,"start2":3686,"length1":34,"length2":33},{"diffs":[[0,"ion\n"],[-1,"    steps = 0\n"],[0,"    "]],"start1":4099,"start2":4099,"length1":22,"length2":8},{"diffs":[[0,"    "],[-1,"pause(100) # move slower through the smart run for better viewing\n        Ax = new_Ax\n        Ay = new_Ay\n        steps = steps + 1   #count the number of steps\n    led.toggle(goal_x, goal_y)    # keep the agent sprite lit when finishing the run\n    print(\"Intelligent run complete in \" + steps + \" steps. \" + \"  Reward = \" + episode_reward)"],[1,"Ax = new_Ax\n        Ay = new_Ay\n    "],[0,"\ninp"]],"start1":4462,"start2":4462,"length1":349,"length2":44}]}]},{"timestamp":1740804068975,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"):  # 10"],[-1,"0"],[0," trainin"]],"start1":2067,"start2":2067,"length1":17,"length2":16},{"diffs":[[0,"ion\n"],[-1,"        print(\"Episode\", episode + 1, \"Start:\", Ax, Ay, \"Goal:\", goal_x, goal_y)\n"],[0,"    "]],"start1":2258,"start2":2258,"length1":89,"length2":8},{"diffs":[[0,"goal is reached\n"],[1,"            # Exploration vs Exploitation\n"],[0,"            if M"]],"start1":2326,"start2":2326,"length1":32,"length2":74},{"diffs":[[0,"unds"],[-1," AND is adjacent"],[0,"\n   "]],"start1":2613,"start2":2613,"length1":24,"length2":8},{"diffs":[[0,"             if "],[-1,"("],[0,"new_Ax < 0 or ne"]],"start1":2621,"start2":2621,"length1":33,"length2":32},{"diffs":[[0," > 4"],[-1," or\n                    abs(new_Ax - Ax) + abs(new_Ay - Ay) != 1)"],[0,":\n  "]],"start1":2685,"start2":2685,"length1":73,"length2":8},{"diffs":[[0," Ay)"],[-1,"\n                # Ensure greedy move is adjacent\n                if (new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4 or\n                    abs(new_Ax - Ax) + abs(new_Ay - Ay) != 1):\n                    new_Ax = Ax\n                    new_Ay = Ay"],[1,"  # Get new coordinates from greedy selection"],[0,"\n\n  "]],"start1":2826,"start2":2826,"length1":260,"length2":53},{"diffs":[[0,"Ay)\n"],[-1,"            print(\"Move from:\", Ax, Ay, \"to:\", new_Ax, new_Ay, \"Reward:\", reward)\n            \n"],[0,"    "]],"start1":3404,"start2":3404,"length1":103,"length2":8},{"diffs":[[0,"n = \" + "],[-1,"str("],[0,"Math.rou"]],"start1":3651,"start2":3651,"length1":20,"length2":16},{"diffs":[[0,") / 1000"],[-1,")"],[0," + \"  Re"]],"start1":3684,"start2":3684,"length1":17,"length2":16},{"diffs":[[0,"\"  Reward = \" + "],[-1,"str("],[0,"episode_reward))"]],"start1":3695,"start2":3695,"length1":36,"length2":32},{"diffs":[[0,"_reward)"],[-1,")"],[0,"\n\n    pr"]],"start1":3718,"start2":3718,"length1":17,"length2":16},{"diffs":[[0,"d_a)\n\n# "],[-1,"O"],[1,"o"],[0,"n button"]],"start1":3852,"start2":3852,"length1":17,"length2":17},{"diffs":[[0,"the "],[-1,"optimal"],[1," onbest"],[0," pat"]],"start1":3876,"start2":3876,"length1":15,"length2":15},{"diffs":[[0," Q-table"],[1," "],[0,"\ndef on_"]],"start1":3926,"start2":3926,"length1":16,"length2":17},{"diffs":[[0," Ay)"],[-1,"\n        # Ensure move is adjacent\n        if (new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4 or\n            abs(new_Ax - Ax) + abs(new_Ay - Ay) != 1):\n            new_Ax = Ax\n            new_Ay = Ay"],[1,"  # Get new coordinates from greedy selection"],[0,"\n   "]],"start1":4236,"start2":4236,"length1":213,"length2":53},{"diffs":[[0,"(new_Ax, new_Ay)"],[1,"     # Determine reward"],[0,"\n        episode"]],"start1":4313,"start2":4313,"length1":32,"length2":55},{"diffs":[[0," new_Ax, new_Ay)"],[1,"  # move the agent to next position"],[0,"\n        pause(1"]],"start1":4423,"start2":4423,"length1":32,"length2":67},{"diffs":[[0,"use(100)"],[-1," "],[0," # "],[-1,"M"],[1,"m"],[0,"ove slow"]],"start1":4485,"start2":4485,"length1":21,"length2":20},{"diffs":[[0," slower "],[1,"through the smart run "],[0,"for bett"]],"start1":4500,"start2":4500,"length1":16,"length2":38},{"diffs":[[0,"teps + 1"],[1,"   #count the number of steps"],[0,"\n    led"]],"start1":4606,"start2":4606,"length1":16,"length2":45},{"diffs":[[0,"l_y)"],[1,"  "],[0,"  # "],[-1,"K"],[1,"k"],[0,"eep "]],"start1":4670,"start2":4670,"length1":13,"length2":15},{"diffs":[[0,"inishing"],[1," the run"],[0,"\n    pri"]],"start1":4712,"start2":4712,"length1":16,"length2":24},{"diffs":[[0,"\" + "],[-1,"str("],[0,"steps"],[-1,")"],[0," + \""]],"start1":4768,"start2":4768,"length1":18,"length2":13},{"diffs":[[0,"eward = \" + "],[-1,"str("],[0,"episode_rewa"]],"start1":4797,"start2":4797,"length1":28,"length2":24},{"diffs":[[0,"_reward)"],[-1,")\n"],[0,"\ninput.o"]],"start1":4816,"start2":4816,"length1":18,"length2":16},{"diffs":[[0,"sed_b)\n\n"],[1,"\n"],[0,"# Get re"]],"start1":4873,"start2":4873,"length1":16,"length2":17},{"diffs":[[0,"n reward"],[1,"    "],[0,"\n\n# Gree"]],"start1":5107,"start2":5107,"length1":16,"length2":20}]}]},{"timestamp":1740810609053,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"e from:\""],[1,","],[0," Ax, Ay,"]],"start1":3759,"start2":3759,"length1":16,"length2":17}]}]},{"timestamp":1740810639140,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"  //  10"],[-1,"0"],[0," trainin"]],"start1":2547,"start2":2547,"length1":17,"length2":16},{"diffs":[[0,"goal is reached\n"],[1,"            //  Exploration vs Exploitation\n"],[0,"            if ("]],"start1":2818,"start2":2818,"length1":32,"length2":76},{"diffs":[[0,"unds"],[-1," AND is adjacent"],[0,"\n   "]],"start1":3114,"start2":3114,"length1":24,"length2":8},{"diffs":[[0," > 4"],[-1," || Math.abs(new_Ax - Ax) + Math.abs(new_Ay - Ay) != 1"],[0,") {\n"]],"start1":3187,"start2":3187,"length1":62,"length2":8},{"diffs":[[0,"    "],[1,"}\n"],[0,"    "],[-1,"//  Ensure greedy move is adjacent\n                if (new_Ax < 0 || new_Ax > 4 || new_Ay < 0 || new_Ay > 4 || Math.abs(new_Ax - Ax) + Math.abs(new_Ay - Ay) != 1) {\n                    new_Ax = Ax\n                    new_Ay = Ay\n                }\n                \n            }\n            "],[1,"        \n            //  Get new coordinates from greedy selection"],[0,"\n   "]],"start1":3387,"start2":3387,"length1":302,"length2":80},{"diffs":[[0,"Ay)\n"],[-1,"            console.log(\"Move from:\" + Ax + \" \" + Ay + \"to: \" + new_Ax + \" \" + new_Ay + \"Reward: \" + reward)\n"],[0,"    "]],"start1":4008,"start2":4008,"length1":117,"length2":8},{"diffs":[[0,"on = \" +"],[-1," (\"\" +"],[0," Math.ro"]],"start1":4281,"start2":4281,"length1":22,"length2":16},{"diffs":[[0,") / 1000"],[-1,")"],[0," + \"  Re"]],"start1":4315,"start2":4315,"length1":17,"length2":16},{"diffs":[[0," \"  Reward = \" +"],[-1," (\"\" +"],[0," episode_reward)"]],"start1":4325,"start2":4325,"length1":38,"length2":32},{"diffs":[[0,"_reward)"],[-1,")"],[0,"\n    }\n "]],"start1":4349,"start2":4349,"length1":17,"length2":16},{"diffs":[[0,"\n})\n//  "],[-1,"O"],[1,"o"],[0,"n button"]],"start1":4461,"start2":4461,"length1":17,"length2":17},{"diffs":[[0,"the "],[-1,"optimal"],[1," onbest"],[0," pat"]],"start1":4485,"start2":4485,"length1":15,"length2":15},{"diffs":[[0," Q-table"],[1," "],[0,"\ninput.o"]],"start1":4535,"start2":4535,"length1":16,"length2":17},{"diffs":[[0,") {\n"],[-1,"    let new_Ax: number;\n    let new_Ay: number;\n"],[0,"    "]],"start1":4606,"start2":4606,"length1":56,"length2":8},{"diffs":[[0,"//  "],[-1,"Ensure move is adjacent\n        if (new_Ax < 0 || new_Ax > 4 || new_Ay < 0 || new_Ay > 4 || Math.abs(new_Ax - Ax) + Math.abs(new_Ay - Ay) != 1) {\n            new_Ax = Ax\n            new_Ay = Ay\n        }\n        "],[1,"Get new coordinates from greedy selection"],[0,"\n   "]],"start1":4956,"start2":4956,"length1":220,"length2":49},{"diffs":[[0,"new_Ax, new_Ay)\n"],[1,"        //  Determine reward\n"],[0,"        episode_"]],"start1":5030,"start2":5030,"length1":32,"length2":61},{"diffs":[[0,"new_Ax, new_Ay)\n"],[1,"        //  move the agent to next position\n"],[0,"        pause(10"]],"start1":5146,"start2":5146,"length1":32,"length2":76},{"diffs":[[0,"    //  "],[-1,"M"],[1,"m"],[0,"ove slow"]],"start1":5229,"start2":5229,"length1":17,"length2":17},{"diffs":[[0," slower "],[1,"through the smart run "],[0,"for bett"]],"start1":5241,"start2":5241,"length1":16,"length2":38},{"diffs":[[0,"s + 1\n    }\n"],[1,"    // count the number of steps\n"],[0,"    led.togg"]],"start1":5350,"start2":5350,"length1":24,"length2":57},{"diffs":[[0,"//  "],[-1,"K"],[1,"k"],[0,"eep "]],"start1":5430,"start2":5430,"length1":9,"length2":9},{"diffs":[[0,"inishing"],[1," the run"],[0,"\n    con"]],"start1":5466,"start2":5466,"length1":16,"length2":24},{"diffs":[[0," \" +"],[-1," (\"\" +"],[0," steps"],[-1,")"],[0," + \""]],"start1":5527,"start2":5527,"length1":21,"length2":14},{"diffs":[[0,"rd = \" +"],[-1," (\"\" +"],[0," episode"]],"start1":5560,"start2":5560,"length1":22,"length2":16},{"diffs":[[0,"_reward)"],[-1,")"],[0,"\n})\n//  "]],"start1":5576,"start2":5576,"length1":17,"length2":16}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"tion\n   "],[1,"     print(\"Episode\", episode + 1, \"Start:\", Ax, Ay, \"Goal:\", goal_x, goal_y)\n    "],[0,"\n       "]],"start1":2258,"start2":2258,"length1":16,"length2":98},{"diffs":[[0,"m:\" "],[-1,"+ Ax + \" \" + Ay +"],[1,"Ax, Ay,"],[0," \"to:"],[-1," \" +"],[1,"\","],[0," new_Ax"],[-1," + \" \" +"],[1,","],[0," new_Ay"],[-1," +"],[1,","],[0," \"Re"]],"start1":3764,"start2":3764,"length1":58,"length2":38},{"diffs":[[0,"ard:"],[-1," \"+"],[1,"\","],[0," rew"]],"start1":3803,"start2":3803,"length1":11,"length2":10}]}]},{"timestamp":1740810738827,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," 0.0"],[-1,"1"],[1,"5"],[0,"  # "],[-1,"Lower minimum to favor"],[1,"Minimum"],[0," explo"],[-1,"it"],[1,"r"],[0,"ation "],[-1,"l"],[1,"r"],[0,"ate"],[-1,"r"],[0,"\neps"]],"start1":953,"start2":953,"length1":54,"length2":37},{"diffs":[[0," 0.9"],[-1,"95"],[1,"7"],[0,"  # "],[-1,"Slower decay for more exploration"],[1,"Multiply epsilon by this after each episode"],[0,"\n\n# "]],"start1":1002,"start2":1002,"length1":47,"length2":56},{"diffs":[[0,"rs\n\n"],[-1,"# Function to get a valid random move\ndef get_valid_random_move(Ax, Ay):\n    while True:\n        i = randint(0, 3)\n        new_Ax = Ax + move_x[i]\n        new_Ay = Ay + move_y[i]\n        distance = abs(new_Ax - Ax) + abs(new_Ay - Ay)\n        if (0 <= new_Ax <= 4 and 0 <= new_Ay <= 4 and distance == 1 and maze[new_Ay][new_Ax] == 0):\n            return new_Ax, new_Ay\n        print(\"Random move rejected:\", Ax, Ay, \"to\", new_Ax, new_Ay)\n\n# Function to get a valid greedy move\ndef get_valid_greedy_move(Ax, Ay):\n    best_q = -1000\n    next_x = -1  # Invalid default to detect no valid move\n    next_y = -1\n    for i in range(4):\n        nx = Ax + move_x[i]\n        ny = Ay + move_y[i]\n        if (0 <= nx <= 4 and 0 <= ny <= 4 and maze[ny][nx] == 0):\n            q = Qtable[ny][nx]\n            if q > best_q:\n                best_q = q\n                next_x = nx\n                next_y = ny\n    if next_x == -1:  # No valid move found, pick a random valid one\n        print(\"Greedy found no valid move, falling back to random\")\n        return get_valid_random_move(Ax, Ay)\n    return next_x, next_y\n\n"],[0,"# Ru"]],"start1":1889,"start2":1889,"length1":1108,"length2":8},{"diffs":[[0," epsilon"],[1,"  # Need to declare epsilon as global to modify it"],[0,"\n    \n  "]],"start1":1975,"start2":1975,"length1":16,"length2":66},{"diffs":[[0,"(10)"],[-1,"\n        print(\"Episode\", episode + 1, \"Start:\", Ax, Ay, \"Goal:\", goal_x, goal_y)\n "],[1,"  # Brief delay to show starting position\n"],[0,"   \n"]],"start1":2217,"start2":2217,"length1":91,"length2":50},{"diffs":[[0,"r Ay != goal_y):"],[1,"  # Run until goal is reached"],[0,"\n            if "]],"start1":2296,"start2":2296,"length1":32,"length2":61},{"diffs":[[0,"    "],[-1,"new_Ax, new_Ay = get_valid_random_move(Ax, Ay)"],[1,"# Random move\n                i = randint(0, 3)\n                new_Ax = Ax + move_x[i]\n                new_Ay = Ay + move_y[i]\n                # Ensure move stays within bounds AND is adjacent\n                if (new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4 or\n                    abs(new_Ax - Ax) + abs(new_Ay - Ay) != 1):\n                    new_Ax = Ax\n                    new_Ay = Ay"],[0,"\n   "]],"start1":2394,"start2":2394,"length1":54,"length2":404},{"diffs":[[0," = g"],[-1,"et_valid_"],[1,"reedy_selection(Ax, Ay)\n                # Ensure "],[0,"greedy"],[-1,"_"],[1," "],[0,"move"],[-1,"(Ax, Ay)\n"],[1," is adjacent\n                if (new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4 or\n                    abs(new_Ax - Ax) + abs(new_Ay - Ay) != 1):\n                    new_Ax = Ax\n                    new_Ay = Ay\n\n            # Determine reward"],[0,"\n   "]],"start1":2843,"start2":2843,"length1":37,"length2":315},{"diffs":[[0,"reward\n\n"],[1,"            # Q-value update\n"],[0,"        "]],"start1":3233,"start2":3233,"length1":16,"length2":45},{"diffs":[[0,"ext_"],[-1,"max_q = max_q_value("],[1,"q_value = Qtable["],[0,"new_A"],[-1,"x, "],[1,"y]["],[0,"new_A"],[-1,"y)"],[1,"x]"],[0,"\n   "]],"start1":3324,"start2":3324,"length1":43,"length2":40},{"diffs":[[0," * next_"],[-1,"max_q"],[1,"q_value"],[0," - old_q"]],"start1":3424,"start2":3424,"length1":21,"length2":23},{"diffs":[[0,"0) / 100"],[-1,"\n"],[1,"  # Round to 2 decimal places\n\n            # Update agent position on display"],[0,"\n       "]],"start1":3511,"start2":3511,"length1":17,"length2":93},{"diffs":[[0,"om:\""],[-1,", Ax, Ay,"],[1," + Ax + \" \" + Ay +"],[0," \"to:"],[-1,"\","],[1," \" +"],[0," new_Ax"],[-1,","],[1," + \" \" +"],[0," new_Ay"],[-1,","],[1," +"],[0," \"Re"]],"start1":3681,"start2":3681,"length1":40,"length2":59},{"diffs":[[0,"\"Reward:"],[-1,"\","],[1," \"+"],[0," reward)"]],"start1":3737,"start2":3737,"length1":18,"length2":19},{"diffs":[[0,"y = new_Ay\n\n"],[1,"        # Reduce epsilon after each episode\n"],[0,"        epsi"]],"start1":3807,"start2":3807,"length1":24,"length2":68},{"diffs":[[0,"lon_min)"],[1,"\n\n        # Debug episode progress"],[0,"\n       "]],"start1":3914,"start2":3914,"length1":16,"length2":50},{"diffs":[[0,"Episode:"],[-1,"\", "],[1," \" + str("],[0,"episode "]],"start1":3972,"start2":3972,"length1":19,"length2":25},{"diffs":[[0," + 1"],[-1,", \"Epsilon:\", "],[1,") + \"  Epsilon = \" + str("],[0,"Math"]],"start1":3996,"start2":3996,"length1":22,"length2":33},{"diffs":[[0,"1000"],[-1,", \"Reward:\", "],[1,") + \"  Reward = \" + str("],[0,"epis"]],"start1":4054,"start2":4054,"length1":21,"length2":32},{"diffs":[[0,"_reward)"],[1,")"],[0,"\n\n    pr"]],"start1":4089,"start2":4089,"length1":16,"length2":17},{"diffs":[[0,"mal path"],[1," through the maze based on trained Q-table"],[0,"\ndef on_"]],"start1":4256,"start2":4256,"length1":16,"length2":58},{"diffs":[[0,"ause(10)"],[1,"  # Brief delay to show starting position"],[0,"\n    ste"]],"start1":4427,"start2":4427,"length1":16,"length2":57},{"diffs":[[0,"_y):"],[-1,"\n        new_Ax, new_Ay = get_valid_greedy_move(Ax, Ay)"],[1,"  # Run until goal is reached\n        new_Ax, new_Ay = greedy_selection(Ax, Ay)\n        # Ensure move is adjacent\n        if (new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4 or\n            abs(new_Ax - Ax) + abs(new_Ay - Ay) != 1):\n            new_Ax = Ax\n            new_Ay = Ay"],[0,"\n   "]],"start1":4528,"start2":4528,"length1":63,"length2":292},{"diffs":[[0,"use(100)"],[1,"  # Move slower for better viewing"],[0,"\n       "]],"start1":4958,"start2":4958,"length1":16,"length2":50},{"diffs":[[0,"eps "],[-1,"+"],[0,"= "],[-1,"1\n    "],[1,"steps + 1"],[0,"\n   "]],"start1":5051,"start2":5051,"length1":17,"length2":19},{"diffs":[[0,"l_x, goal_y)"],[1,"  # Keep the agent sprite lit when finishing"],[0,"\n    print(\""]],"start1":5085,"start2":5085,"length1":24,"length2":68},{"diffs":[[0,"e in"],[-1,"\", steps,"],[1," \" + str(steps) +"],[0," \""],[1," "],[0,"steps."],[1," \" + \" "],[0," Rew"]],"start1":5176,"start2":5176,"length1":25,"length2":41},{"diffs":[[0,"rd ="],[-1,"\", "],[1," \" + str("],[0,"epis"]],"start1":5218,"start2":5218,"length1":11,"length2":17},{"diffs":[[0,"_reward)"],[1,")"],[0,"\n\ninput."]],"start1":5238,"start2":5238,"length1":16,"length2":17},{"diffs":[[0,"he next "],[-1,"m"],[1,"M"],[0,"ove\ndef "]],"start1":5322,"start2":5322,"length1":17,"length2":17},{"diffs":[[0,"\n# G"],[-1,"et maximum Q-value from next state's possible a"],[1,"reedy selection fun"],[0,"ction"],[-1,"s"],[0,"\ndef "],[-1,"max_q_value"],[1,"greedy_selection"],[0,"(Ax,"]],"start1":5539,"start2":5539,"length1":77,"length2":53},{"diffs":[[0,"x, Ay):\n    "],[-1,"max"],[1,"best"],[0,"_q = -1000\n "]],"start1":5590,"start2":5590,"length1":27,"length2":28},{"diffs":[[0,"= -1000\n"],[1,"    next_x = Ax\n    next_y = Ay\n"],[0,"    for "]],"start1":5609,"start2":5609,"length1":16,"length2":48},{"diffs":[[0," if q > "],[-1,"max"],[1,"best"],[0,"_q:\n    "]],"start1":5812,"start2":5812,"length1":19,"length2":20},{"diffs":[[0,"        "],[-1,"max"],[1,"best"],[0,"_q = q\n "]],"start1":5836,"start2":5836,"length1":19,"length2":20},{"diffs":[[0,"    "],[-1,"return max_q"],[1,"            next_x = nx\n                next_y = ny\n    return next_x, next_y"],[0,"\n\nde"]],"start1":5855,"start2":5855,"length1":20,"length2":85}]}]},{"timestamp":1740813295207,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ted:"],[-1," \" + str(Ax) + \" \" + str(Ay) +"],[1,"\", Ax, Ay,"],[0," \""],[-1," "],[0,"to"],[-1," \" + str(new_Ax) + \" \" + str("],[1,"\", new_Ax, "],[0,"new_Ay)"],[-1,")"],[0,"\n\n# "]],"start1":2301,"start2":2301,"length1":80,"length2":40},{"diffs":[[0,"\"Episode"],[-1," \" + str("],[1,"\", "],[0,"episode "]],"start1":3294,"start2":3294,"length1":25,"length2":19},{"diffs":[[0," + 1"],[-1,") +"],[1,","],[0," \""],[-1," "],[0,"Start:"],[-1," \" + str(Ax) + \" \" + str(Ay) +"],[1,"\", Ax, Ay,"],[0," \""],[-1," "],[0,"Goal:"],[-1," \" + str(goal_x) + \" \" + str("],[1,"\", goal_x, "],[0,"goal_y)"],[-1,")"],[0,"\n   "]],"start1":3312,"start2":3312,"length1":95,"length2":52},{"diffs":[[0,"rom:"],[-1," \" + str(Ax) + \" \" + str(Ay) +"],[1,"\", Ax, Ay,"],[0," \""],[-1," "],[0,"to:"],[-1," \" + str(new_Ax) + \" \" + str("],[1,"\", new_Ax, "],[0,"new_Ay"],[-1,") +"],[1,","],[0," \""],[-1," "],[0,"Reward:"],[-1," \" + str("],[1,"\", "],[0,"reward)"],[-1,")"],[0,"\n   "]],"start1":4018,"start2":4018,"length1":109,"length2":60},{"diffs":[[0,"Episode:"],[-1," \" + str("],[1,"\", "],[0,"episode "]],"start1":4212,"start2":4212,"length1":25,"length2":19},{"diffs":[[0," + 1"],[-1,") +"],[1,","],[0," \""],[-1," "],[0,"Epsilon:"],[-1," \" + str("],[1,"\", "],[0,"Math"]],"start1":4230,"start2":4230,"length1":31,"length2":22},{"diffs":[[0,"1000"],[-1,") +"],[1,","],[0," \""],[-1," "],[0,"Reward:"],[-1," \" + str("],[1,"\", "],[0,"epis"]],"start1":4277,"start2":4277,"length1":30,"length2":21},{"diffs":[[0,"_reward)"],[-1,")"],[0,"\n\n    pr"]],"start1":4301,"start2":4301,"length1":17,"length2":16},{"diffs":[[0,"int("],[-1,"str("],[0,"row)"],[-1,")"],[0,"\n\nin"]],"start1":4375,"start2":4375,"length1":17,"length2":12},{"diffs":[[0,"e in"],[-1," \" + str(steps) +"],[1,"\", steps,"],[0," \""],[-1," "],[0,"step"]],"start1":4995,"start2":4995,"length1":28,"length2":19},{"diffs":[[0,"rd ="],[-1," \" + str("],[1,"\", "],[0,"epis"]],"start1":5021,"start2":5021,"length1":17,"length2":11},{"diffs":[[0,"_reward)"],[-1,")"],[0,"\n\ninput."]],"start1":5035,"start2":5035,"length1":17,"length2":16}]}]},{"timestamp":1740813476076,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," 0.0"],[-1,"1"],[1,"5"],[0,"\n//  "],[-1,"Lower minimum to favor"],[1,"Minimum"],[0," explo"],[-1,"it"],[1,"r"],[0,"ation "],[-1,"l"],[1,"r"],[0,"ate"],[-1,"r"],[0,"\nlet"]],"start1":934,"start2":934,"length1":55,"length2":38},{"diffs":[[0," 0.9"],[-1,"95"],[1,"7"],[0,"\n//  "],[-1,"Slower decay for more exploration"],[1,"Multiply epsilon by this after each episode"],[0,"\n// "]],"start1":988,"start2":988,"length1":48,"length2":57},{"diffs":[[0,"ars\n"],[-1,"//  Function to get a valid random move (allows walls)\nfunction get_valid_random_move(Ax: number, Ay: number): number[] {\n    let i: number;\n    let proposed_x: number;\n    let proposed_y: number;\n    let distance: any;\n    let attempts = 0\n    while (attempts < 10) {\n        //  Limit attempts to prevent infinite loop\n        i = randint(0, 3)\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        distance = Math.abs(proposed_x - Ax) + Math.abs(proposed_y - Ay)\n        if (0 <= proposed_x && proposed_x <= 4 && (0 <= proposed_y && proposed_y <= 4) && distance == 1) {\n            return [proposed_x, proposed_y]\n        }\n        \n        console.log(\"Random move rejected: \" + (\"\" + Ax) + \" \" + (\"\" + Ay) + \" to \" + (\"\" + proposed_x) + \" \" + (\"\" + proposed_y))\n        attempts += 1\n    }\n    //  Fallback: return a valid move if all attempts fail (shouldn't happen in this maze)\n    for (i = 0; i < 4; i++) {\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if (0 <= proposed_x && proposed_x <= 4 && (0 <= proposed_y && proposed_y <= 4)) {\n            return [proposed_x, proposed_y]\n        }\n        \n    }\n    return [Ax, Ay]\n}\n\n//  Should never reach here, but avoids infinite loop\n//  Function to get a valid greedy move (allows walls)\nfunction get_valid_greedy_move(Ax: number, Ay: number): number[] {\n    let nx: number;\n    let ny: number;\n    let q: number;\n    let best_q = -1000\n    let next_x = Ax\n    //  Default to current position (will be overwritten)\n    let next_y = Ay\n    let valid_found = false\n    for (let i = 0; i < 4; i++) {\n        nx = Ax + move_x[i]\n        ny = Ay + move_y[i]\n        if (0 <= nx && nx <= 4 && (0 <= ny && ny <= 4)) {\n            q = Qtable[ny][nx]\n            if (q > best_q) {\n                best_q = q\n                next_x = nx\n                next_y = ny\n                valid_found = true\n            }\n            \n        }\n        \n    }\n    if (!valid_found) {\n        //  If no valid move found (unlikely), fallback to random\n        console.log(\"Greedy found no valid move, falling back to random\")\n        return get_valid_random_move(Ax, Ay)\n    }\n    \n    return [next_x, next_y]\n}\n\n"],[0,"//  "]],"start1":2060,"start2":2060,"length1":2218,"length2":8},{"diffs":[[0,"reward: number;\n"],[1,"    let i: number;\n"],[0,"    let new_Ax: "]],"start1":2232,"start2":2232,"length1":32,"length2":51},{"diffs":[[0,"et next_"],[-1,"max_q"],[1,"q_value"],[0,": number"]],"start1":2373,"start2":2373,"length1":21,"length2":23},{"diffs":[[0,"umber;\n    \n"],[1,"    //  Need to declare epsilon as global to modify it\n"],[0,"    for (let"]],"start1":2420,"start2":2420,"length1":24,"length2":79},{"diffs":[[0,"    "],[-1,"new_Ax = 0\n        //  Declare at start of scope\n        new_Ay = 0\n        led.plotBrightness(Ax, Ay, 255)\n        pause(10)\n        console.log(\"Episode \" + (\"\" + (episode + 1)) + \" Start: \" + (\"\" + Ax) + \" \" + (\"\" + Ay) + \" Goal: \" + (\"\" + goal_x) + \" \" + (\"\" + goal_y))"],[1,"led.plotBrightness(Ax, Ay, 255)\n        pause(10)\n        //  Brief delay to show starting position"],[0,"\n   "]],"start1":2642,"start2":2642,"length1":281,"length2":107},{"diffs":[[0,"Ay != goal_y) {\n"],[1,"            //  Run until goal is reached\n"],[0,"            if ("]],"start1":2777,"start2":2777,"length1":32,"length2":74},{"diffs":[[0,"    "],[-1,"let [new_Ax,"],[1,"//  Random move\n                i = randint(0, 3)\n                new_Ax = Ax + move_x[i]\n               "],[0," new_Ay"],[-1,"]"],[0," = "],[-1,"get_valid_random_move(Ax, Ay)"],[1,"Ay + move_y[i]\n                //  Ensure move stays within bounds AND is adjacent\n                if (new_Ax < 0 || new_Ax > 4 || new_Ay < 0 || new_Ay > 4 || Math.abs(new_Ax - Ax) + Math.abs(new_Ay - Ay) != 1) {\n                    new_Ax = Ax\n                    new_Ay = Ay\n                }\n                "],[0,"\n   "]],"start1":2890,"start2":2890,"length1":60,"length2":434},{"diffs":[[0," = g"],[-1,"et_valid_"],[1,"reedy_selection(Ax, Ay)\n                //  Ensure "],[0,"greedy"],[-1,"_"],[1," "],[0,"move"],[-1,"(Ax, Ay)\n            }\n            "],[1," is adjacent\n                if (new_Ax < 0 || new_Ax > 4 || new_Ay < 0 || new_Ay > 4 || Math.abs(new_Ax - Ax) + Math.abs(new_Ay - Ay) != 1) {\n                    new_Ax = Ax\n                    new_Ay = Ay\n                }\n                \n            }\n            \n            //  Determine reward"],[0,"\n   "]],"start1":3378,"start2":3378,"length1":63,"length2":371},{"diffs":[[0,"eward += reward\n"],[1,"            //  Q-value update\n"],[0,"            old_"]],"start1":3815,"start2":3815,"length1":32,"length2":63},{"diffs":[[0,"ext_"],[-1,"max_q = max_q_value("],[1,"q_value = Qtable["],[0,"new_A"],[-1,"x, "],[1,"y]["],[0,"new_A"],[-1,"y)"],[1,"x]"],[0,"\n   "]],"start1":3916,"start2":3916,"length1":43,"length2":40},{"diffs":[[0," * next_"],[-1,"max_q"],[1,"q_value"],[0," - old_q"]],"start1":4016,"start2":4016,"length1":21,"length2":23},{"diffs":[[0,") / 100\n"],[1,"            //  Round to 2 decimal places\n            //  Update agent position on display\n"],[0,"        "]],"start1":4104,"start2":4104,"length1":16,"length2":107},{"diffs":[[0,"rom:"],[-1," \" + (\""],[0,"\" + Ax"],[-1,")"],[0," + \" \" +"],[-1," (\"\" +"],[0," Ay"],[-1,")"],[0," + \""],[-1," "],[0,"to: \" +"],[-1," (\"\" +"],[0," new_Ax"],[-1,")"],[0," + \" \" +"],[-1," (\"\" +"],[0," new_Ay"],[-1,")"],[0," + \""],[-1," "],[0,"Rewa"]],"start1":4292,"start2":4292,"length1":93,"length2":62},{"diffs":[[0," \" +"],[-1," (\"\" +"],[0," reward)"],[-1,")"],[0,"\n   "]],"start1":4357,"start2":4357,"length1":23,"length2":16},{"diffs":[[0,"ew_Ay\n        }\n"],[1,"        //  Reduce epsilon after each episode\n"],[0,"        epsilon "]],"start1":4412,"start2":4412,"length1":32,"length2":78},{"diffs":[[0,"on_min)\n"],[1,"        //  Debug episode progress\n"],[0,"        "]],"start1":4531,"start2":4531,"length1":16,"length2":51},{"diffs":[[0," + \""],[1," "],[0," Epsilon"],[-1,":"],[1," ="],[0," \" +"]],"start1":4628,"start2":4628,"length1":17,"length2":19},{"diffs":[[0," + \""],[1," "],[0," Reward"],[-1,":"],[1," ="],[0," \" +"]],"start1":4688,"start2":4688,"length1":16,"length2":18},{"diffs":[[0,"ole.log("],[-1,"\"\" + "],[0,"row)\n   "]],"start1":4815,"start2":4815,"length1":21,"length2":16},{"diffs":[[0,"mal path"],[1," through the maze based on trained Q-table"],[0,"\ninput.o"]],"start1":4865,"start2":4865,"length1":16,"length2":58},{"diffs":[[0,"essed_b() {\n"],[1,"    let new_Ax: number;\n    let new_Ay: number;\n"],[0,"    let rewa"]],"start1":4969,"start2":4969,"length1":24,"length2":72},{"diffs":[[0,"= 0\n"],[-1,"    let new_Ax = 0\n    //  Declare at start of scope\n    let new_Ay = 0\n"],[0,"    "]],"start1":5112,"start2":5112,"length1":80,"length2":8},{"diffs":[[0,"  pause(10)\n"],[1,"    //  Brief delay to show starting position\n"],[0,"    let step"]],"start1":5154,"start2":5154,"length1":24,"length2":70},{"diffs":[[0,"    "],[-1,"let [new_Ax, new_Ay] = get_valid_greedy_move(Ax, Ay)"],[1,"//  Run until goal is reached\n        let [new_Ax, new_Ay] = greedy_selection(Ax, Ay)\n        //  Ensure move is adjacent\n        if (new_Ax < 0 || new_Ax > 4 || new_Ay < 0 || new_Ay > 4 || Math.abs(new_Ax - Ax) + Math.abs(new_Ay - Ay) != 1) {\n            new_Ax = Ax\n            new_Ay = Ay\n        }\n        "],[0,"\n   "]],"start1":5277,"start2":5277,"length1":60,"length2":318},{"diffs":[[0,"se(100)\n"],[1,"        //  Move slower for better viewing\n"],[0,"        "]],"start1":5734,"start2":5734,"length1":16,"length2":59},{"diffs":[[0,"  steps "],[-1,"+"],[0,"="],[1," steps +"],[0," 1\n    }"]],"start1":5831,"start2":5831,"length1":18,"length2":25},{"diffs":[[0,"_x, goal_y)\n"],[1,"    //  Keep the agent sprite lit when finishing\n"],[0,"    console."]],"start1":5876,"start2":5876,"length1":24,"length2":73},{"diffs":[[0,"\" steps."],[1," \" + \" "],[0," Reward "]],"start1":6001,"start2":6001,"length1":16,"length2":23},{"diffs":[[0,"he next "],[-1,"m"],[1,"M"],[0,"ove\nfunc"]],"start1":6075,"start2":6075,"length1":17,"length2":17},{"diffs":[[0,"_reward(new_"],[1,"A"],[0,"x: number, n"]],"start1":6100,"start2":6100,"length1":24,"length2":25},{"diffs":[[0,"Ax: number, new_"],[1,"A"],[0,"y: number): numb"]],"start1":6112,"start2":6112,"length1":32,"length2":33},{"diffs":[[0,"er;\n    if (new_"],[1,"A"],[0,"x == goal_x && n"]],"start1":6170,"start2":6170,"length1":32,"length2":33},{"diffs":[[0,"= goal_x && new_"],[1,"A"],[0,"y == goal_y) {\n "]],"start1":6190,"start2":6190,"length1":32,"length2":33},{"diffs":[[0,"se if (maze[new_"],[1,"A"],[0,"y][new_"],[1,"A"],[0,"x] == 1) {\n     "]],"start1":6250,"start2":6250,"length1":39,"length2":41},{"diffs":[[0,"/  G"],[-1,"et maximum Q-value from next state's possible a"],[1,"reedy selection fun"],[0,"ction"],[-1,"s"],[0,"\nfun"]],"start1":6372,"start2":6372,"length1":61,"length2":32},{"diffs":[[0,"ion "],[-1,"max_q_value"],[1,"greedy_selection"],[0,"(Ax:"]],"start1":6406,"start2":6406,"length1":19,"length2":24},{"diffs":[[0," number): number"],[1,"[]"],[0," {\n    let nx: n"]],"start1":6442,"start2":6442,"length1":32,"length2":34},{"diffs":[[0,"    let "],[-1,"max"],[1,"best"],[0,"_q = -10"]],"start1":6522,"start2":6522,"length1":19,"length2":20},{"diffs":[[0,"= -1000\n"],[1,"    let next_x = Ax\n    let next_y = Ay\n"],[0,"    for "]],"start1":6537,"start2":6537,"length1":16,"length2":56},{"diffs":[[0,"if (q > "],[-1,"max"],[1,"best"],[0,"_q) {\n  "]],"start1":6776,"start2":6776,"length1":19,"length2":20},{"diffs":[[0,"    "],[-1,"max"],[1,"best"],[0,"_q = q\n"],[1,"                next_x = nx\n                next_y = ny\n"],[0,"    "]],"start1":6806,"start2":6806,"length1":18,"length2":75},{"diffs":[[0,"urn "],[-1,"max_q"],[1,"[next_x, next_y]"],[0,"\n}\n\n"]],"start1":6936,"start2":6936,"length1":13,"length2":24}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," random move"],[-1," (allows walls)"],[0,"\ndef get_val"]],"start1":1926,"start2":1926,"length1":39,"length2":24},{"diffs":[[0,"    "],[-1,"attempts = 0\n    while attempts < 10:  # Limit attempts to prevent infinite loop"],[1,"while True:"],[0,"\n   "]],"start1":1974,"start2":1974,"length1":88,"length2":19},{"diffs":[[0," 3)\n        "],[-1,"proposed_"],[1,"new_A"],[0,"x = Ax + mov"]],"start1":2012,"start2":2012,"length1":33,"length2":29},{"diffs":[[0,"[i]\n        "],[-1,"proposed_"],[1,"new_A"],[0,"y = Ay + mov"]],"start1":2044,"start2":2044,"length1":33,"length2":29},{"diffs":[[0,"e = abs("],[-1,"proposed_"],[1,"new_A"],[0,"x - Ax) "]],"start1":2095,"start2":2095,"length1":25,"length2":21},{"diffs":[[0,") + abs("],[-1,"proposed_"],[1,"new_A"],[0,"y - Ay)\n"]],"start1":2114,"start2":2114,"length1":25,"length2":21},{"diffs":[[0,"   if (0 <= "],[-1,"proposed_"],[1,"new_A"],[0,"x <= 4 and 0"]],"start1":2140,"start2":2140,"length1":33,"length2":29},{"diffs":[[0," 4 and 0 <= "],[-1,"proposed_"],[1,"new_A"],[0,"y <= 4 and d"]],"start1":2161,"start2":2161,"length1":33,"length2":29},{"diffs":[[0,"istance == 1"],[1," and maze[new_Ay][new_Ax] == 0"],[0,"):\n         "]],"start1":2190,"start2":2190,"length1":24,"length2":54},{"diffs":[[0," return "],[-1,"proposed_x, proposed_"],[1,"new_Ax, new_A"],[0,"y\n      "]],"start1":2246,"start2":2246,"length1":37,"length2":29},{"diffs":[[0,"\" + str("],[-1,"proposed_"],[1,"new_A"],[0,"x) + \" \""]],"start1":2341,"start2":2341,"length1":25,"length2":21},{"diffs":[[0,"str("],[-1,"proposed_y))\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail (shouldn't happen in this maze)\n    for i in range(4):\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if 0 <= proposed_x <= 4 and 0 <= proposed_y <= 4:\n            return proposed_x, proposed_y\n    return Ax, Ay  # Should never reach here, but avoids infinite loop"],[1,"new_Ay))"],[0,"\n\n# "]],"start1":2365,"start2":2365,"length1":397,"length2":16},{"diffs":[[0,"move"],[-1," (allows walls)"],[0,"\ndef"]],"start1":2412,"start2":2412,"length1":23,"length2":8},{"diffs":[[0,"x = "],[-1,"Ax"],[1,"-1"],[0,"  # "],[-1,"D"],[1,"Invalid d"],[0,"efau"]],"start1":2480,"start2":2480,"length1":15,"length2":23},{"diffs":[[0," to "],[-1,"current position (will be overwritten)\n    next_y = Ay\n    valid_found = False"],[1,"detect no valid move\n    next_y = -1"],[0,"\n   "]],"start1":2505,"start2":2505,"length1":86,"length2":44},{"diffs":[[0,"y[i]\n        if "],[1,"("],[0,"0 <= nx <= 4 and"]],"start1":2620,"start2":2620,"length1":32,"length2":33},{"diffs":[[0,"and 0 <= ny <= 4"],[1," and maze[ny][nx] == 0)"],[0,":\n            q "]],"start1":2650,"start2":2650,"length1":32,"length2":55},{"diffs":[[0,"    "],[-1,"            valid_found = True\n    if not valid_found"],[1,"if next_x == -1"],[0,":  # "],[-1,"If n"],[1,"N"],[0,"o va"]],"start1":2832,"start2":2832,"length1":70,"length2":29},{"diffs":[[0,"ound"],[-1," (unlikely), fallback to"],[1,", pick a"],[0," random"],[1," valid one"],[0,"\n   "]],"start1":2871,"start2":2871,"length1":39,"length2":33},{"diffs":[[0,"= 0\n"],[-1,"        new_Ax = 0  # Declare at start of scope\n        new_Ay = 0\n"],[0,"    "]],"start1":3252,"start2":3252,"length1":75,"length2":8},{"diffs":[[0,"= 0\n"],[-1,"    new_Ax = 0  # Declare at start of scope\n    new_Ay = 0\n"],[0,"    "]],"start1":4718,"start2":4718,"length1":67,"length2":8},{"diffs":[[0,"_reward(new_"],[1,"A"],[0,"x, new_"],[1,"A"],[0,"y):\n    if ("]],"start1":5322,"start2":5322,"length1":31,"length2":33},{"diffs":[[0,"y):\n    if (new_"],[1,"A"],[0,"x == goal_x and "]],"start1":5343,"start2":5343,"length1":32,"length2":33},{"diffs":[[0," goal_x and new_"],[1,"A"],[0,"y == goal_y):\n  "]],"start1":5364,"start2":5364,"length1":32,"length2":33},{"diffs":[[0,"if maze[new_"],[1,"A"],[0,"y][new_"],[1,"A"],[0,"x] == 1:\n   "]],"start1":5421,"start2":5421,"length1":31,"length2":33}]}]},{"timestamp":1740814319793,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"   new_Ax = "],[-1,"4"],[1,"0"],[0,"  # Declare "]],"start1":3804,"start2":3804,"length1":25,"length2":25}]}]},{"timestamp":1740814661347,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"er): number[] {\n"],[1,"    let i: number;\n"],[0,"    let proposed"]],"start1":2178,"start2":2178,"length1":32,"length2":51},{"diffs":[[0,"osed_x: "],[-1,"any"],[1,"number"],[0,";\n    le"]],"start1":2225,"start2":2225,"length1":19,"length2":22},{"diffs":[[0,"_y: "],[-1,"any"],[1,"number"],[0,";\n  "]],"start1":2257,"start2":2257,"length1":11,"length2":14},{"diffs":[[0,"let "],[-1,"choice: number;\n    let valid_move"],[1,"distance: any;\n    let attempt"],[0,"s = "],[-1,"[]"],[1,"0"],[0,"\n    "],[-1,"for (let i = 0; i < 4; i++) {"],[1,"while (attempts < 10) {\n        //  Limit attempts to prevent infinite loop\n        i = randint(0, 3)"],[0,"\n   "]],"start1":2273,"start2":2273,"length1":82,"length2":149},{"diffs":[[0," Ay + move_y[i]\n"],[1,"        distance = Math.abs(proposed_x - Ax) + Math.abs(proposed_y - Ay)\n"],[0,"        if (0 <="]],"start1":2475,"start2":2475,"length1":32,"length2":105},{"diffs":[[0,"= 4)"],[-1,") {\n            valid_moves.push("],[1," && distance == 1) {\n            return "],[0,"[pro"]],"start1":2646,"start2":2646,"length1":41,"length2":48},{"diffs":[[0,"posed_y]"],[-1,")"],[0,"\n       "]],"start1":2706,"start2":2706,"length1":17,"length2":16},{"diffs":[[0,"y]\n        }"],[-1," else {\n"],[1,"\n   "],[0,"     "],[1,"\n "],[0,"       conso"]],"start1":2712,"start2":2712,"length1":37,"length2":35},{"diffs":[[0,"    "],[-1,"}\n        \n    }\n    if ("],[1,"attempts += 1\n    }\n    //  Fallback: return a "],[0,"valid"],[-1,"_"],[1," "],[0,"move"],[-1,"s.length > 0) {\n        choice = randint(0, valid_moves.length - 1)\n        return valid_moves[choice"],[1," if all attempts fail (shouldn't happen in this maze)\n    for (i = 0; i < 4; i++) {\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i"],[0,"]\n    "],[-1,"}\n"],[0,"    "],[-1,"\n    console.log(\"No valid moves found from: \" + (\"\" + Ax) + \" \" + (\"\" + Ay))\n    //  Should never happen\n    return [Ax, Ay]\n}\n\n//  Fallback (shouldn't occur)"],[1,"if (0 <= proposed_x && proposed_x <= 4 && (0 <= proposed_y && proposed_y <= 4)) {\n            return [proposed_x, proposed_y]\n        }\n        \n    }\n    return [Ax, Ay]\n}\n\n//  Should never reach here, but avoids infinite loop"],[0,"\n// "]],"start1":2869,"start2":2869,"length1":315,"length2":456},{"diffs":[[0,"let "],[-1,"choice: number;"],[1,"best_q = -1000\n    let next_x = Ax\n    //  Default to current position (will be overwritten)"],[0,"\n   "]],"start1":3507,"start2":3507,"length1":23,"length2":100},{"diffs":[[0,"en)\n    let "],[-1,"best_q = -1000"],[1,"next_y = Ay"],[0,"\n    let bes"]],"start1":3600,"start2":3600,"length1":38,"length2":35},{"diffs":[[0,"let "],[-1,"best_moves = []"],[1,"valid_found = false"],[0,"\n   "]],"start1":3628,"start2":3628,"length1":23,"length2":27},{"diffs":[[0,"    "],[-1,"best_moves = [[nx, ny]]\n            } else if (q == best_q) {\n                best_moves.push([nx,"],[1,"next_x = nx\n                next_y ="],[0," ny"],[-1,"])"],[0,"\n       "],[-1,"     }\n"],[0,"    "]],"start1":3900,"start2":3900,"length1":126,"length2":55},{"diffs":[[0,"    "],[-1,"   \n        } else {\n            console.log(\"Greedy move rejected: \" + (\"\" + Ax) + \" \" + (\"\" + Ay) + \" to \" + (\"\" + nx) + \" \" + (\"\" + ny))\n        }\n        \n    }\n    if (best_moves.length > 0) {\n        choice = randint(0, best_moves.length - 1)\n        return best_moves[choice]\n    }\n    \n"],[1,"valid_found = true\n            }\n            \n        }\n        \n    }\n    if (!valid_found) {\n        //  If no valid move found (unlikely), fallback to random\n    "],[0,"    "]],"start1":3956,"start2":3956,"length1":302,"length2":173},{"diffs":[[0,"move"],[1,","],[0," f"],[-1,"rom: \" + (\"\" + Ax) + \" \" + (\"\" + Ay))\n    //  Should never happen\n"],[1,"alling back to random\")\n    "],[0,"    "]],"start1":4164,"start2":4164,"length1":76,"length2":39},{"diffs":[[0,"Ax, Ay)\n"],[1,"    }\n    \n    return [next_x, next_y]\n"],[0,"}\n\n//  R"]],"start1":4232,"start2":4232,"length1":16,"length2":55},{"diffs":[[0,"number;\n    let "],[-1,"old_"],[1,"new_A"],[0,"x: number;\n    l"]],"start1":4458,"start2":4458,"length1":36,"length2":37},{"diffs":[[0,"number;\n    let "],[-1,"old_"],[1,"new_A"],[0,"y: number;\n    l"]],"start1":4482,"start2":4482,"length1":36,"length2":37},{"diffs":[[0,"_reward = 0\n"],[1,"        new_Ax = 0\n        //  Declare at start of scope\n        new_Ay = 0\n"],[0,"        led."]],"start1":4768,"start2":4768,"length1":24,"length2":100},{"diffs":[[0,") {\n"],[-1,"            old_x = Ax\n            //  Store current position for update\n            old_y = Ay\n"],[0,"    "]],"start1":5105,"start2":5105,"length1":104,"length2":8},{"diffs":[[0,"           let ["],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay] = get_valid_"]],"start1":5157,"start2":5157,"length1":36,"length2":44},{"diffs":[[0,"           let ["],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay] = get_valid_"]],"start1":5247,"start2":5247,"length1":36,"length2":44},{"diffs":[[0,"   \n"],[-1,"            //  Process the move\n"],[0,"    "]],"start1":5334,"start2":5334,"length1":41,"length2":8},{"diffs":[[0,"rd = get_reward("],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay)\n            "]],"start1":5354,"start2":5354,"length1":36,"length2":44},{"diffs":[[0," Qtable["],[-1,"old_y][old_"],[1,"Ay][A"],[0,"x]\n     "]],"start1":5448,"start2":5448,"length1":27,"length2":21},{"diffs":[[0,"q_value("],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay)\n    "]],"start1":5493,"start2":5493,"length1":20,"length2":28},{"diffs":[[0," Qtable["],[-1,"old_y][old_"],[1,"Ay][A"],[0,"x] = Mat"]],"start1":5620,"start2":5620,"length1":27,"length2":21},{"diffs":[[0,"osition("],[-1,"old_x, old_y, Ax, "],[1,"Ax, Ay, new_Ax, new_"],[0,"Ay)\n    "]],"start1":5700,"start2":5700,"length1":34,"length2":36},{"diffs":[[0,"+ (\"\" + "],[-1,"old_"],[1,"A"],[0,"x) + \" \""]],"start1":5770,"start2":5770,"length1":20,"length2":17},{"diffs":[[0,"+ (\"\" + "],[-1,"old_"],[1,"A"],[0,"y) + \" t"]],"start1":5788,"start2":5788,"length1":20,"length2":17},{"diffs":[[0,"\" to: \" + (\"\" + "],[1,"new_"],[0,"Ax) + \" \" + (\"\" "]],"start1":5802,"start2":5802,"length1":32,"length2":36},{"diffs":[[0,") + \" \" + (\"\" + "],[1,"new_"],[0,"Ay) + \" Reward: "]],"start1":5824,"start2":5824,"length1":32,"length2":36},{"diffs":[[0," + reward))\n"],[1,"            Ax = new_Ax\n            Ay = new_Ay\n"],[0,"        }\n  "]],"start1":5867,"start2":5867,"length1":24,"length2":72},{"diffs":[[0,") {\n"],[-1,"    let old_x: number;\n    let old_y: number;\n"],[0,"    "]],"start1":6364,"start2":6364,"length1":54,"length2":8},{"diffs":[[0,"_reward = 0\n"],[1,"    let new_Ax = 0\n    //  Declare at start of scope\n    let new_Ay = 0\n"],[0,"    led.plot"]],"start1":6443,"start2":6443,"length1":24,"length2":96},{"diffs":[[0,"    "],[-1,"old_x = Ax\n        //  Store current position for update\n        old_y = Ay\n        let [Ax, "],[1,"let [new_Ax, new_"],[0,"Ay] "]],"start1":6642,"start2":6642,"length1":101,"length2":25},{"diffs":[[0,"_reward("],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay)\n    "]],"start1":6719,"start2":6719,"length1":20,"length2":28},{"diffs":[[0,"ion("],[-1,"old_x, old_y, Ax, "],[1,"Ax, Ay, new_Ax, new_"],[0,"Ay)\n"]],"start1":6802,"start2":6802,"length1":26,"length2":28},{"diffs":[[0,"se(100)\n"],[1,"        Ax = new_Ax\n        Ay = new_Ay\n"],[0,"        "]],"start1":6841,"start2":6841,"length1":16,"length2":56}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"valid_moves = []"],[1,"attempts = 0\n    while attempts < 10:  # Limit attempts to prevent infinite loop"],[0,"\n    "],[-1,"for i in"],[1,"    i ="],[0," ran"],[-1,"ge(4):"],[1,"dint(0, 3)"],[0,"\n   "]],"start1":1989,"start2":1989,"length1":47,"length2":114},{"diffs":[[0," Ay + move_y[i]\n"],[1,"        distance = abs(proposed_x - Ax) + abs(proposed_y - Ay)\n"],[0,"        if 0 <= "]],"start1":2156,"start2":2156,"length1":32,"length2":95},{"diffs":[[0,"\n        if "],[1,"("],[0,"0 <= propose"]],"start1":2234,"start2":2234,"length1":24,"length2":25},{"diffs":[[0,"<= 4"],[-1,":\n            valid_moves.append(("],[1," and distance == 1):\n            return "],[0,"prop"]],"start1":2288,"start2":2288,"length1":42,"length2":48},{"diffs":[[0,"oposed_y"],[-1,"))\n        else:\n    "],[1,"\n"],[0,"        "]],"start1":2346,"start2":2346,"length1":37,"length2":17},{"diffs":[[0,"    "],[-1,"if len("],[1,"    attempts += 1\n    # Fallback: return a "],[0,"valid"],[-1,"_"],[1," "],[0,"move"],[-1,"s) > 0:\n        choice = randint(0, len(valid_moves) - 1)\n        return valid_moves[choice]\n    print(\"No valid moves found from: \" + str(Ax) + \" \" + str(Ay))  # Should never happen\n    return Ax, Ay  # Fallback (shouldn't occur)"],[1," if all attempts fail (shouldn't happen in this maze)\n    for i in range(4):\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if 0 <= proposed_x <= 4 and 0 <= proposed_y <= 4:\n            return proposed_x, proposed_y\n    return Ax, Ay  # Should never reach here, but avoids infinite loop"],[0,"\n\n# "]],"start1":2472,"start2":2472,"length1":255,"length2":380},{"diffs":[[0,"    "],[-1,"best_moves = []"],[1,"next_x = Ax  # Default to current position (will be overwritten)\n    next_y = Ay\n    valid_found = False"],[0,"\n   "]],"start1":2957,"start2":2957,"length1":23,"length2":112},{"diffs":[[0,"    "],[-1,"best_moves = [(nx, ny)]\n            elif q == best_q:\n                best_moves.append((nx, ny))\n        else:\n            print(\"Greedy move rejected: \" + str(Ax) + \" \" + str(Ay) + \" to \" + str(nx) + \" \" + str(ny))\n    if len(best_moves) > 0:\n        choice = randint(0, len(best_moves) - 1)\n        return best_moves[choice]\n"],[1,"next_x = nx\n                next_y = ny\n                valid_found = True\n    if not valid_found:  # If no valid move found (unlikely), fallback to random\n    "],[0,"    "]],"start1":3284,"start2":3284,"length1":336,"length2":168},{"diffs":[[0,"move"],[1,","],[0," f"],[-1,"rom: \" + str(Ax) + \" \" + str(Ay))  # Should never happen\n"],[1,"alling back to random\")\n    "],[0,"    "]],"start1":3481,"start2":3481,"length1":67,"length2":39},{"diffs":[[0,"(Ax, Ay)"],[1,"\n    return next_x, next_y"],[0,"\n\n# Run "]],"start1":3548,"start2":3548,"length1":16,"length2":42},{"diffs":[[0,"_reward = 0\n"],[1,"        new_Ax = 4  # Declare at start of scope\n        new_Ay = 0\n"],[0,"    \n       "]],"start1":3787,"start2":3787,"length1":24,"length2":91},{"diffs":[[0,"y):\n"],[-1,"            old_x = Ax  # Store current position for update\n            old_y = Ay\n"],[0,"    "]],"start1":4107,"start2":4107,"length1":91,"length2":8},{"diffs":[[0,"                "],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay = get_valid_r"]],"start1":4151,"start2":4151,"length1":36,"length2":44},{"diffs":[[0,"                "],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay = get_valid_g"]],"start1":4232,"start2":4232,"length1":36,"length2":44},{"diffs":[[0,"y)\n\n"],[-1,"            # Process the move\n"],[0,"    "]],"start1":4292,"start2":4292,"length1":39,"length2":8},{"diffs":[[0,"rd = get_reward("],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay)\n            "]],"start1":4312,"start2":4312,"length1":36,"length2":44},{"diffs":[[0," Qtable["],[-1,"old_y][old_"],[1,"Ay][A"],[0,"x]\n     "]],"start1":4407,"start2":4407,"length1":27,"length2":21},{"diffs":[[0,"q_value("],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay)\n    "]],"start1":4452,"start2":4452,"length1":20,"length2":28},{"diffs":[[0," Qtable["],[-1,"old_y][old_"],[1,"Ay][A"],[0,"x] = Mat"]],"start1":4579,"start2":4579,"length1":27,"length2":21},{"diffs":[[0,"osition("],[-1,"old_x, old_y, Ax, "],[1,"Ax, Ay, new_Ax, new_"],[0,"Ay)\n    "]],"start1":4660,"start2":4660,"length1":34,"length2":36},{"diffs":[[0,"\" + str("],[-1,"old_"],[1,"A"],[0,"x) + \" \""]],"start1":4722,"start2":4722,"length1":20,"length2":17},{"diffs":[[0,"\" + str("],[-1,"old_"],[1,"A"],[0,"y) + \" t"]],"start1":4738,"start2":4738,"length1":20,"length2":17},{"diffs":[[0,"+ \" to: \" + str("],[1,"new_"],[0,"Ax) + \" \" + str("]],"start1":4750,"start2":4750,"length1":32,"length2":36},{"diffs":[[0,"Ax) + \" \" + str("],[1,"new_"],[0,"Ay) + \" Reward: "]],"start1":4770,"start2":4770,"length1":32,"length2":36},{"diffs":[[0,"str(reward))"],[1,"\n            \n            Ax = new_Ax\n            Ay = new_Ay"],[0,"\n\n        ep"]],"start1":4810,"start2":4810,"length1":24,"length2":85},{"diffs":[[0,"_reward = 0\n"],[1,"    new_Ax = 0  # Declare at start of scope\n    new_Ay = 0\n"],[0,"    led.plot"]],"start1":5320,"start2":5320,"length1":24,"length2":83},{"diffs":[[0,"    "],[-1,"old_x = Ax  # Store current position for update\n        old_y = Ay\n        Ax, "],[1,"new_Ax, new_"],[0,"Ay ="]],"start1":5502,"start2":5502,"length1":87,"length2":20},{"diffs":[[0,"Ax, Ay)\n"],[-1,"        \n"],[0,"        "]],"start1":5545,"start2":5545,"length1":25,"length2":16},{"diffs":[[0,"_reward("],[-1,"Ax, "],[1,"new_Ax, new_"],[0,"Ay)\n    "]],"start1":5573,"start2":5573,"length1":20,"length2":28},{"diffs":[[0,"ion("],[-1,"old_x, old_y, Ax, "],[1,"Ax, Ay, new_Ax, new_"],[0,"Ay)\n"]],"start1":5656,"start2":5656,"length1":26,"length2":28},{"diffs":[[0,"se(100)\n"],[1,"        Ax = new_Ax\n        Ay = new_Ay\n"],[0,"        "]],"start1":5695,"start2":5695,"length1":16,"length2":56}]}]},{"timestamp":1740815257598,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"] {\n"],[-1,"    let i: number;\n"],[0,"    "]],"start1":2190,"start2":2190,"length1":27,"length2":8},{"diffs":[[0,"proposed_x: "],[-1,"number"],[1,"any"],[0,";\n    let pr"]],"start1":2202,"start2":2202,"length1":30,"length2":27},{"diffs":[[0,"proposed_y: "],[-1,"number"],[1,"any"],[0,";\n    let di"]],"start1":2227,"start2":2227,"length1":30,"length2":27},{"diffs":[[0,"let "],[-1,"distance: any;\n    let attempt"],[1,"choice: number;\n    let valid_move"],[0,"s = "],[-1,"0"],[1,"[]"],[0,"\n    "],[-1,"while (attempts < 10) {\n        //  Limit attempts to prevent infinite loop\n        i = randint(0, 3)"],[1,"for (let i = 0; i < 4; i++) {"],[0,"\n   "]],"start1":2248,"start2":2248,"length1":149,"length2":82},{"diffs":[[0,"[i]\n"],[-1,"        distance = Math.abs(proposed_x - Ax) + Math.abs(proposed_y - Ay)\n"],[0,"    "]],"start1":2395,"start2":2395,"length1":81,"length2":8},{"diffs":[[0,"= 4)"],[-1," && distance == 1) {\n            return "],[1,") {\n            valid_moves.push("],[0,"[pro"]],"start1":2481,"start2":2481,"length1":48,"length2":41},{"diffs":[[0,"d_x, proposed_y]"],[1,")"],[0,"\n        }\n     "]],"start1":2526,"start2":2526,"length1":32,"length2":33},{"diffs":[[0,"])\n        }"],[-1,"\n    "],[1," else {\n"],[0,"    "],[-1,"\n"],[0,"        cons"]],"start1":2541,"start2":2541,"length1":34,"length2":36},{"diffs":[[0,"    "],[-1,"attempts += 1\n    }\n    //  Fallback: return a "],[1,"}\n        \n    }\n    if ("],[0,"valid"],[-1," "],[1,"_"],[0,"move"],[-1," if all attempts fail\n    for (i = 0; i < 4; i++) {\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i"],[1,"s.length > 0) {\n        choice = randint(0, valid_moves.length - 1)\n        return valid_moves[choice"],[0,"]\n    "],[1,"}\n"],[0,"    "],[-1,"if (0 <= proposed_x && proposed_x <= 4 && (0 <= proposed_y && proposed_y <= 4)) {\n            return [proposed_x, proposed_y]\n        }\n        \n    }\n    return [Ax, Ay]\n}\n\n//  Should never reach here, but avoids infinite loop"],[1,"\n    console.log(\"No valid moves found from: \" + (\"\" + Ax) + \" \" + (\"\" + Ay))\n    //  Should never happen\n    return [Ax, Ay]\n}\n\n//  Fallback (shouldn't occur)"],[0,"\n// "]],"start1":2700,"start2":2700,"length1":424,"length2":315},{"diffs":[[0,"let "],[-1,"best_q = -1000\n    let next_x = Ax\n    //  Default to current position (will be overwritten)"],[1,"choice: number;"],[0,"\n   "]],"start1":3197,"start2":3197,"length1":100,"length2":23},{"diffs":[[0,"let "],[-1,"next_y = Ay"],[1,"best_q = -1000"],[0,"\n   "]],"start1":3221,"start2":3221,"length1":19,"length2":22},{"diffs":[[0,"let "],[-1,"valid_found = false"],[1,"best_moves = []"],[0,"\n   "]],"start1":3244,"start2":3244,"length1":27,"length2":23},{"diffs":[[0,"    "],[-1,"next_x = nx\n                next_y ="],[1,"best_moves = [[nx, ny]]\n            } else if (q == best_q) {\n                best_moves.push([nx,"],[0," ny"],[1,"])"],[0,"\n   "]],"start1":3512,"start2":3512,"length1":47,"length2":111},{"diffs":[[0,"    "],[-1,"    valid_found = true\n            }\n            \n        }\n        \n    }\n    if (!valid_found) {\n        //  If no valid move found (unlikely), fallback to random"],[1,"}\n            \n        } else {\n            console.log(\"Greedy move rejected: \" + (\"\" + Ax) + \" \" + (\"\" + Ay) + \" to \" + (\"\" + nx) + \" \" + (\"\" + ny))\n        }\n        \n    }\n    if (best_moves.length > 0) {\n        choice = randint(0, best_moves.length - 1)\n        return best_moves[choice]\n    }"],[0,"\n    "],[1,"\n"],[0,"    "]],"start1":3628,"start2":3628,"length1":177,"length2":313},{"diffs":[[0,"move"],[-1,","],[0," f"],[-1,"alling back to random\")\n    "],[1,"rom: \" + (\"\" + Ax) + \" \" + (\"\" + Ay))\n    //  Should never happen\n"],[0,"    "]],"start1":3976,"start2":3976,"length1":39,"length2":76},{"diffs":[[0,"Ay)\n"],[-1,"    }\n    \n    return [next_x, next_y]\n"],[0,"}\n\n/"]],"start1":4085,"start2":4085,"length1":47,"length2":8},{"diffs":[[0,"current position"],[1," for update"],[0,"\n            old"]],"start1":4886,"start2":4886,"length1":32,"length2":43},{"diffs":[[0,"       let ["],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y] = get_val"]],"start1":4989,"start2":4989,"length1":37,"length2":29},{"diffs":[[0,"       let ["],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y] = get_val"]],"start1":5071,"start2":5071,"length1":37,"length2":29},{"diffs":[[0," }\n            \n"],[1,"            //  Process the move\n"],[0,"            rewa"]],"start1":5134,"start2":5134,"length1":32,"length2":65},{"diffs":[[0," get_reward("],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n         "]],"start1":5203,"start2":5203,"length1":37,"length2":29},{"diffs":[[0,"q_value("],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n     "]],"start1":5336,"start2":5336,"length1":29,"length2":21},{"diffs":[[0,"d_x, old_y, "],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n         "]],"start1":5551,"start2":5551,"length1":37,"length2":29},{"diffs":[[0,"+ (\"\" + "],[-1,"next_"],[1,"A"],[0,"x) + \" \""]],"start1":5655,"start2":5655,"length1":21,"length2":17},{"diffs":[[0,"+ (\"\" + "],[-1,"next_"],[1,"A"],[0,"y) + \" R"]],"start1":5673,"start2":5673,"length1":21,"length2":17},{"diffs":[[0,"d))\n"],[-1,"            Ax = next_x\n            Ay = next_y\n"],[0,"    "]],"start1":5712,"start2":5712,"length1":56,"length2":8},{"diffs":[[0,"position"],[1," for update"],[0,"\n       "]],"start1":6446,"start2":6446,"length1":16,"length2":27},{"diffs":[[0,"   let ["],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y] = get"]],"start1":6490,"start2":6490,"length1":29,"length2":21},{"diffs":[[0,"_reward("],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n     "]],"start1":6558,"start2":6558,"length1":29,"length2":21},{"diffs":[[0,"_y, "],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n "]],"start1":6647,"start2":6647,"length1":21,"length2":13},{"diffs":[[0,"00)\n"],[-1,"        Ax = next_x\n        Ay = next_y\n"],[0,"    "]],"start1":6674,"start2":6674,"length1":48,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    "],[-1,"attempts = 0\n    while attempts < 10:  # Limit attempts to prevent infinite loop\n        i = randint(0, 3)"],[1,"valid_moves = []\n    for i in range(4):"],[0,"\n   "]],"start1":1989,"start2":1989,"length1":114,"length2":47},{"diffs":[[0,"[i]\n"],[-1,"        distance = abs(proposed_x - Ax) + abs(proposed_y - Ay)\n"],[0,"    "]],"start1":2101,"start2":2101,"length1":71,"length2":8},{"diffs":[[0,"     if "],[-1,"("],[0,"0 <= pro"]],"start1":2108,"start2":2108,"length1":17,"length2":16},{"diffs":[[0,"<= 4"],[-1," and distance == 1):\n            return "],[1,":\n            valid_moves.append(("],[0,"prop"]],"start1":2157,"start2":2157,"length1":48,"length2":42},{"diffs":[[0,", proposed_y"],[-1,"\n"],[1,"))\n        else:\n    "],[0,"        prin"]],"start1":2205,"start2":2205,"length1":25,"length2":45},{"diffs":[[0,"    "],[-1,"    attempts += 1\n    # Fallback: return a "],[1,"if len("],[0,"valid"],[-1," "],[1,"_"],[0,"move"],[-1," if all attempts fail\n    for i in range(4):\n        proposed_x = Ax + move_x[i]\n   "],[1,"s) > 0:\n        choice = randint(0, len(valid_moves) - 1)\n        return valid_moves[choice]\n"],[0,"    "],[-1," "],[0,"pr"],[-1,"oposed_y = Ay + move_y[i]\n        if 0 <= proposed_x <= 4 and 0 <= proposed_y <= 4:\n            return proposed_x, proposed_y\n    return Ax, Ay  # Should never reach here, but avoids infinite loop"],[1,"int(\"No valid moves found from: \" + str(Ax) + \" \" + str(Ay))  # Should never happen\n    return Ax, Ay  # Fallback (shouldn't occur)"],[0,"\n\n# "]],"start1":2355,"start2":2355,"length1":348,"length2":255},{"diffs":[[0,"    "],[-1,"next_x = Ax  # Default to current position (will be overwritten)\n    next_y = Ay\n    valid_found = False"],[1,"best_moves = []"],[0,"\n   "]],"start1":2715,"start2":2715,"length1":112,"length2":23},{"diffs":[[0,"    "],[-1,"next_x = nx\n                next_y = ny\n                valid_found = True\n    if not valid_found:  # If no valid move found (unlikely), fallback to random\n    "],[1,"best_moves = [(nx, ny)]\n            elif q == best_q:\n                best_moves.append((nx, ny))\n        else:\n            print(\"Greedy move rejected: \" + str(Ax) + \" \" + str(Ay) + \" to \" + str(nx) + \" \" + str(ny))\n    if len(best_moves) > 0:\n        choice = randint(0, len(best_moves) - 1)\n        return best_moves[choice]\n"],[0,"    "]],"start1":2953,"start2":2953,"length1":168,"length2":336},{"diffs":[[0,"move"],[-1,","],[0," f"],[-1,"alling back to random\")\n    "],[1,"rom: \" + str(Ax) + \" \" + str(Ay))  # Should never happen\n"],[0,"    "]],"start1":3318,"start2":3318,"length1":39,"length2":67},{"diffs":[[0," Ay)"],[-1,"\n    return next_x, next_y"],[0,"\n\n# "]],"start1":3417,"start2":3417,"length1":34,"length2":8},{"diffs":[[0,"current position"],[1," for update"],[0,"\n            old"]],"start1":3915,"start2":3915,"length1":32,"length2":43},{"diffs":[[0,"            "],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y = get_vali"]],"start1":4010,"start2":4010,"length1":37,"length2":29},{"diffs":[[0,"            "],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y = get_vali"]],"start1":4083,"start2":4083,"length1":37,"length2":29},{"diffs":[[0,"x, Ay)\n\n"],[1,"            # Process the move\n"],[0,"        "]],"start1":4127,"start2":4127,"length1":16,"length2":47},{"diffs":[[0," get_reward("],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n         "]],"start1":4186,"start2":4186,"length1":37,"length2":29},{"diffs":[[0,"q_value("],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n     "]],"start1":4320,"start2":4320,"length1":29,"length2":21},{"diffs":[[0,"d_x, old_y, "],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n         "]],"start1":4536,"start2":4536,"length1":37,"length2":29},{"diffs":[[0,"\" + str("],[-1,"next_"],[1,"A"],[0,"x) + \" \""]],"start1":4628,"start2":4628,"length1":21,"length2":17},{"diffs":[[0,"\" + str("],[-1,"next_"],[1,"A"],[0,"y) + \" R"]],"start1":4644,"start2":4644,"length1":21,"length2":17},{"diffs":[[0,"rd))"],[-1,"\n            \n            Ax = next_x\n            Ay = next_y"],[0,"\n\n  "]],"start1":4680,"start2":4680,"length1":69,"length2":8},{"diffs":[[0,"position"],[1," for update"],[0,"\n       "]],"start1":5276,"start2":5276,"length1":16,"length2":27},{"diffs":[[0,"        "],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y = get_"]],"start1":5315,"start2":5315,"length1":29,"length2":21},{"diffs":[[0,"dy_move(Ax, Ay)\n"],[1,"        \n"],[0,"        reward ="]],"start1":5346,"start2":5346,"length1":32,"length2":41},{"diffs":[[0,"_reward("],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n     "]],"start1":5391,"start2":5391,"length1":29,"length2":21},{"diffs":[[0,"_y, "],[-1,"next_x, next_"],[1,"Ax, A"],[0,"y)\n "]],"start1":5480,"start2":5480,"length1":21,"length2":13},{"diffs":[[0,"00)\n"],[-1,"        Ax = next_x\n        Ay = next_y\n"],[0,"    "]],"start1":5507,"start2":5507,"length1":48,"length2":8}]}]},{"timestamp":1740815531552,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," update "],[1,"only "],[0,"the agen"]],"start1":1274,"start2":1274,"length1":16,"length2":21}]}]},{"timestamp":1740860069496,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," update "],[1,"only "],[0,"the agen"]],"start1":1339,"start2":1339,"length1":16,"length2":21}]}]},{"timestamp":1740860069930,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"let old_"],[-1,"A"],[0,"x: numbe"]],"start1":4433,"start2":4433,"length1":17,"length2":16},{"diffs":[[0,"old_"],[-1,"Ay: number;\n    let next_Ax: number;\n    let next_A"],[0,"y: n"]],"start1":4460,"start2":4460,"length1":59,"length2":8},{"diffs":[[0,"    old_"],[-1,"A"],[0,"x = Ax\n "]],"start1":5002,"start2":5002,"length1":17,"length2":16},{"diffs":[[0,"current "],[-1,"agent "],[0,"position"]],"start1":5039,"start2":5039,"length1":22,"length2":16},{"diffs":[[0,"old_"],[-1,"A"],[0,"y = Ay\n"],[-1,"            next_Ax = 0\n            // initialize the next agent position\n            next_Ay = 0\n"],[0,"    "]],"start1":5068,"start2":5068,"length1":114,"length2":15},{"diffs":[[0,"  let [next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y] = get_val"]],"start1":5136,"start2":5136,"length1":34,"length2":32},{"diffs":[[0,"  let [next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y] = get_val"]],"start1":5226,"start2":5226,"length1":34,"length2":32},{"diffs":[[0,"reward(next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n         "]],"start1":5333,"start2":5333,"length1":34,"length2":32},{"diffs":[[0,"ble[old_"],[-1,"A"],[0,"y][old_"],[-1,"A"],[0,"x]\n     "]],"start1":5422,"start2":5422,"length1":25,"length2":23},{"diffs":[[0,"_value(next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n         "]],"start1":5470,"start2":5470,"length1":34,"length2":32},{"diffs":[[0,"old_"],[-1,"A"],[0,"y][old_"],[-1,"A"],[0,"x] ="]],"start1":5604,"start2":5604,"length1":17,"length2":15},{"diffs":[[0,"old_"],[-1,"A"],[0,"x, old_"],[-1,"A"],[0,"y, next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n "]],"start1":5690,"start2":5690,"length1":35,"length2":31},{"diffs":[[0,"\" + old_"],[-1,"A"],[0,"x) + \" \""]],"start1":5762,"start2":5762,"length1":17,"length2":16},{"diffs":[[0,"\" + old_"],[-1,"A"],[0,"y) + \" t"]],"start1":5783,"start2":5783,"length1":17,"length2":16},{"diffs":[[0," + next_"],[-1,"A"],[0,"x) + \" \""]],"start1":5809,"start2":5809,"length1":17,"length2":16},{"diffs":[[0," + next_"],[-1,"A"],[0,"y) + \" R"]],"start1":5831,"start2":5831,"length1":17,"length2":16},{"diffs":[[0,"      Ax = next_"],[-1,"A"],[0,"x\n            Ay"]],"start1":5879,"start2":5879,"length1":33,"length2":32},{"diffs":[[0,"      Ay = next_"],[-1,"A"],[0,"y\n        }\n    "]],"start1":5903,"start2":5903,"length1":33,"length2":32},{"diffs":[[0,"t [next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y] = get"]],"start1":6689,"start2":6689,"length1":26,"length2":24},{"diffs":[[0,"reward(next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n        e"]],"start1":6761,"start2":6761,"length1":34,"length2":32},{"diffs":[[0,"ard\n"],[-1,"        // count the total reward for the intelligent run\n"],[0,"    "]],"start1":6813,"start2":6813,"length1":66,"length2":8},{"diffs":[[0,"ext_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n "]],"start1":6862,"start2":6862,"length1":18,"length2":16},{"diffs":[[0," = next_"],[-1,"A"],[0,"x\n      "]],"start1":6906,"start2":6906,"length1":17,"length2":16},{"diffs":[[0," = next_"],[-1,"A"],[0,"y\n      "]],"start1":6926,"start2":6926,"length1":17,"length2":16},{"diffs":[[0,"  }\n"],[-1,"    // count the steps taken for intelligent run\n"],[0,"    "]],"start1":6957,"start2":6957,"length1":57,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    old_"],[-1,"A"],[0,"x = Ax  "]],"start1":4015,"start2":4015,"length1":17,"length2":16},{"diffs":[[0,"current "],[-1,"agent "],[0,"position"]],"start1":4039,"start2":4039,"length1":22,"length2":16},{"diffs":[[0,"old_"],[-1,"A"],[0,"y = Ay\n"],[-1,"            next_Ax = 0  #initialize the next agent position\n            next_Ay = 0\n"],[0,"    "]],"start1":4068,"start2":4068,"length1":101,"length2":15},{"diffs":[[0,"       next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y = get_vali"]],"start1":4128,"start2":4128,"length1":34,"length2":32},{"diffs":[[0,"       next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y = get_vali"]],"start1":4209,"start2":4209,"length1":34,"length2":32},{"diffs":[[0,"reward(next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n         "]],"start1":4289,"start2":4289,"length1":34,"length2":32},{"diffs":[[0,"ble[old_"],[-1,"A"],[0,"y][old_"],[-1,"A"],[0,"x]\n     "]],"start1":4379,"start2":4379,"length1":25,"length2":23},{"diffs":[[0,"_value(next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n         "]],"start1":4427,"start2":4427,"length1":34,"length2":32},{"diffs":[[0,"old_"],[-1,"A"],[0,"y][old_"],[-1,"A"],[0,"x] ="]],"start1":4561,"start2":4561,"length1":17,"length2":15},{"diffs":[[0,"old_"],[-1,"A"],[0,"x, old_"],[-1,"A"],[0,"y, next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n "]],"start1":4648,"start2":4648,"length1":35,"length2":31},{"diffs":[[0,"str(old_"],[-1,"A"],[0,"x) + \" \""]],"start1":4712,"start2":4712,"length1":17,"length2":16},{"diffs":[[0,"str(old_"],[-1,"A"],[0,"y) + \" t"]],"start1":4731,"start2":4731,"length1":17,"length2":16},{"diffs":[[0,"tr(next_"],[-1,"A"],[0,"x) + \" \""]],"start1":4755,"start2":4755,"length1":17,"length2":16},{"diffs":[[0,"tr(next_"],[-1,"A"],[0,"y) + \" R"]],"start1":4775,"start2":4775,"length1":17,"length2":16},{"diffs":[[0," = next_"],[-1,"A"],[0,"x\n      "]],"start1":4842,"start2":4842,"length1":17,"length2":16},{"diffs":[[0," = next_"],[-1,"A"],[0,"y\n\n     "]],"start1":4866,"start2":4866,"length1":17,"length2":16},{"diffs":[[0,"   next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y = get_"]],"start1":5500,"start2":5500,"length1":26,"length2":24},{"diffs":[[0,"reward(next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n        e"]],"start1":5571,"start2":5571,"length1":34,"length2":32},{"diffs":[[0,"ward"],[-1,"  #count the total reward for the intelligent run"],[0,"\n   "]],"start1":5622,"start2":5622,"length1":57,"length2":8},{"diffs":[[0,"ext_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y)\n "]],"start1":5672,"start2":5672,"length1":18,"length2":16},{"diffs":[[0," = next_"],[-1,"Ax  "],[1,"x"],[0,"\n       "]],"start1":5716,"start2":5716,"length1":20,"length2":17},{"diffs":[[0," = next_"],[-1,"A"],[0,"y\n      "]],"start1":5736,"start2":5736,"length1":17,"length2":16},{"diffs":[[0,"+= 1"],[-1,"  #count the steps taken for intelligent run"],[0,"\n   "]],"start1":5760,"start2":5760,"length1":52,"length2":8}]}]},{"timestamp":1740860534541,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"posed_y:"],[1," number;\n    let distance:"],[0," any;\n  "]],"start1":2247,"start2":2247,"length1":16,"length2":42},{"diffs":[[0,"\n       "],[-1," //"],[0," distanc"]],"start1":2485,"start2":2485,"length1":19,"length2":16},{"diffs":[[0,"tance = "],[1,"Math."],[0,"abs(prop"]],"start1":2497,"start2":2497,"length1":16,"length2":21},{"diffs":[[0,"- Ax) + "],[1,"Math."],[0,"abs(prop"]],"start1":2525,"start2":2525,"length1":16,"length2":21},{"diffs":[[0,"proposed_y <= 4)"],[1," && distance == 1"],[0,") {\n            "]],"start1":2629,"start2":2629,"length1":32,"length2":49},{"diffs":[[0,"log("],[-1,"i + \": "],[1,"\""],[0,"Rand"]],"start1":2745,"start2":2745,"length1":15,"length2":9},{"diffs":[[0,"jected: "],[-1,"["],[0,"\" + (\"\" "]],"start1":2764,"start2":2764,"length1":17,"length2":16},{"diffs":[[0,"\"\" + Ax) + \""],[-1,"]["],[1," "],[0,"\" + (\"\" + Ay"]],"start1":2777,"start2":2777,"length1":26,"length2":25},{"diffs":[[0," + \""],[-1,"]"],[0," to "],[-1,"["],[0,"\" + "]],"start1":2803,"start2":2803,"length1":14,"length2":12},{"diffs":[[0,"d_x) + \""],[-1,"]["],[1," "],[0,"\" + (\"\" "]],"start1":2828,"start2":2828,"length1":18,"length2":17},{"diffs":[[0,"d_y)"],[-1," + \"]\""],[0,")\n  "]],"start1":2854,"start2":2854,"length1":14,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"        "],[-1,"#"],[0,"distance"]],"start1":2167,"start2":2167,"length1":17,"length2":16},{"diffs":[[0,"d_y <= 4"],[1," and distance == 1"],[0,"):\n     "]],"start1":2279,"start2":2279,"length1":16,"length2":34},{"diffs":[[0,"int("],[-1,"i + \": "],[1,"\""],[0,"Rand"]],"start1":2360,"start2":2360,"length1":15,"length2":9},{"diffs":[[0,"jected: "],[-1,"["],[0,"\" + str("]],"start1":2379,"start2":2379,"length1":17,"length2":16},{"diffs":[[0," str(Ax) + \""],[-1,"]["],[1," "],[0,"\" + str(Ay) "]],"start1":2390,"start2":2390,"length1":26,"length2":25},{"diffs":[[0," + \""],[-1,"]"],[0," to "],[-1,"["],[0,"\" + "]],"start1":2414,"start2":2414,"length1":14,"length2":12},{"diffs":[[0,"d_x) + \""],[-1,"]["],[1," "],[0,"\" + str("]],"start1":2437,"start2":2437,"length1":18,"length2":17},{"diffs":[[0,"d_y)"],[-1," + \"]\""],[0,")\n  "]],"start1":2461,"start2":2461,"length1":14,"length2":8}]}]},{"timestamp":1740861080346,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"le ("],[-1,"true) {"],[1,"attempts < 10) {\n        //  Limit attempts to prevent infinite loop"],[0,"\n   "]],"start1":2289,"start2":2289,"length1":15,"length2":76},{"diffs":[[0," += 1\n    }\n"],[-1,"}\n\n"],[1,"    "],[0,"//  Fallback"]],"start1":2843,"start2":2843,"length1":27,"length2":28},{"diffs":[[0,"ts fail\n"],[1,"    for (i = 0; i < 4; i++) {\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if (0 <= proposed_x && proposed_x <= 4 && (0 <= proposed_y && proposed_y <= 4)) {\n            return [proposed_x, proposed_y]\n        }\n        \n    }\n    return [Ax, Ay]\n}\n\n//  Should never reach here, but avoids infinite loop\n"],[0,"//  Func"]],"start1":2906,"start2":2906,"length1":16,"length2":354}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ile "],[-1,"True:  "],[1,"attempts < 10:  # Limit attempts to prevent infinite loop"],[0,"\n   "]],"start1":2007,"start2":2007,"length1":15,"length2":65},{"diffs":[[0,"ts fail\n"],[1,"    for i in range(4):\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if 0 <= proposed_x <= 4 and 0 <= proposed_y <= 4:\n            return proposed_x, proposed_y\n    return Ax, Ay  # Should never reach here, but avoids infinite loop"],[0,"\n\n# Func"]],"start1":2538,"start2":2538,"length1":16,"length2":281}]}]},{"timestamp":1740861192972,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\nAy_0 = "],[-1,"Math."],[0,"randint("]],"start1":531,"start2":531,"length1":21,"length2":16}]}]},{"timestamp":1740862052450,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ath."],[-1,"floor"],[0,"rand"]],"start1":540,"start2":540,"length1":13,"length2":8}]}]},{"timestamp":1740862054093,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"0 = "],[-1,"Math.floor(Math.random() * 3) + 1"],[1,"randint(1, 3)"],[0,"\nmaz"]],"start1":484,"start2":484,"length1":41,"length2":21},{"diffs":[[0,"aze["],[-1,"Math.floor(Math.random() * 3) + 1"],[1,"randint(1, 3)"],[0,"][2]"]],"start1":570,"start2":570,"length1":41,"length2":21},{"diffs":[[0,"y = "],[-1,"Math.floor(Math.random() * 3) + 1"],[1,"randint(1, 3)"],[0,"\nmaz"]],"start1":686,"start2":686,"length1":41,"length2":21},{"diffs":[[0,"i = "],[-1,"Math.floor(Math.random() * 4"],[1,"randint(0, 3"],[0,")\n  "]],"start1":2309,"start2":2309,"length1":36,"length2":20},{"diffs":[[0," Ax + move_x[i]\n"],[-1,""],[0,"        proposed"]],"start1":2347,"start2":2347,"length1":32,"length2":32},{"diffs":[[0," Ay + move_y[i]\n"],[1,"        // distance = abs(proposed_x - Ax) + abs(proposed_y - Ay)\n"],[0,"        if (0 <="]],"start1":2383,"start2":2383,"length1":32,"length2":98},{"diffs":[[0,"et next_x = "],[-1,"0"],[1,"Ax"],[0,"\n    //  ini"]],"start1":3061,"start2":3061,"length1":25,"length2":26},{"diffs":[[0,"//  "],[-1,"initialize the next agent position"],[1,"Default to current position (will be overwritten)"],[0,"\n   "]],"start1":3080,"start2":3080,"length1":42,"length2":57},{"diffs":[[0,"et next_y = "],[-1,"0"],[1,"Ay"],[0,"\n    let val"]],"start1":3139,"start2":3139,"length1":25,"length2":26}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"th.floor"],[-1,"(Math.random()*3)+1"],[1,"randint(1, 3)"],[0,"\nmaze[Ay"]],"start1":541,"start2":541,"length1":35,"length2":29},{"diffs":[[0,"aze["],[-1,"Math.floor(Math.random()*3)+1"],[1,"randint(1, 3)"],[0,"][2]"]],"start1":630,"start2":630,"length1":37,"length2":21},{"diffs":[[0,"y = "],[-1,"Math.floor(Math.random()*3)+1"],[1,"randint(1, 3)"],[0,"\nmaz"]],"start1":736,"start2":736,"length1":37,"length2":21},{"diffs":[[0,"i = "],[-1,"Math.floor(Math.random()*4"],[1,"randint(0, 3"],[0,")\n  "]],"start1":2037,"start2":2037,"length1":34,"length2":20},{"diffs":[[0," Ay + move_y[i]\n"],[1,"        #distance = abs(proposed_x - Ax) + abs(proposed_y - Ay)\n"],[0,"        if (0 <="]],"start1":2111,"start2":2111,"length1":32,"length2":96},{"diffs":[[0,"x = "],[-1,"0"],[1,"Ax"],[0,"  # "],[-1,"initialize the next agent position"],[1,"Default to current position (will be overwritten)"],[0,"\n   "]],"start1":2624,"start2":2624,"length1":47,"length2":63},{"diffs":[[0,"   next_y = "],[-1,"0"],[1,"Ay"],[0,"\n    valid_f"]],"start1":2685,"start2":2685,"length1":25,"length2":26}]}]},{"timestamp":1740862651421,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"umber): "],[-1,"any"],[1,"number"],[0,"[] {\n   "]],"start1":2230,"start2":2230,"length1":19,"length2":22},{"diffs":[[0,"osed_x: "],[-1,"any"],[1,"number"],[0,";\n    le"]],"start1":2280,"start2":2280,"length1":19,"length2":22},{"diffs":[[0,"    let "],[-1,"proposed_A"],[1,"n"],[0,"x: numbe"]],"start1":2984,"start2":2984,"length1":26,"length2":17},{"diffs":[[0,"    let "],[-1,"proposed_A"],[1,"n"],[0,"y: numbe"]],"start1":3004,"start2":3004,"length1":26,"length2":17},{"diffs":[[0," = -"],[-1,"9999"],[1,"1000"],[0,"\n   "]],"start1":3057,"start2":3057,"length1":12,"length2":12},{"diffs":[[0,"   let next_"],[-1,"A"],[0,"x = 0\n    //"]],"start1":3067,"start2":3067,"length1":25,"length2":24},{"diffs":[[0,"   let next_"],[-1,"A"],[0,"y = 0\n    le"]],"start1":3129,"start2":3129,"length1":25,"length2":24},{"diffs":[[0,"        "],[-1,"proposed_A"],[1,"n"],[0,"x = Ax +"]],"start1":3209,"start2":3209,"length1":26,"length2":17},{"diffs":[[0,"        "],[-1,"proposed_A"],[1,"n"],[0,"y = Ay +"]],"start1":3237,"start2":3237,"length1":26,"length2":17},{"diffs":[[0," <= "],[-1,"proposed_Ax && proposed_Ax <= 4 && (0 <= proposed_Ay && proposed_A"],[1,"nx && nx <= 4 && (0 <= ny && n"],[0,"y <="]],"start1":3278,"start2":3278,"length1":74,"length2":38},{"diffs":[[0,"ble["],[-1,"proposed_Ax][proposed_Ay"],[1,"ny][nx"],[0,"]\n  "]],"start1":3342,"start2":3342,"length1":32,"length2":14},{"diffs":[[0,"ext_"],[-1,"A"],[0,"x = "],[-1,"proposed_A"],[1,"n"],[0,"x\n  "]],"start1":3428,"start2":3428,"length1":23,"length2":13},{"diffs":[[0,"ext_"],[-1,"A"],[0,"y = "],[-1,"proposed_A"],[1,"n"],[0,"y\n  "]],"start1":3456,"start2":3456,"length1":23,"length2":13},{"diffs":[[0,"\n        \n    }\n"],[1,"    if (!valid_found) {\n        //  If no valid move found (unlikely), fallback to random\n        console.log(\"Greedy found no valid move, falling back to random\")\n        return get_valid_random_move(Ax, Ay)\n    }\n    \n"],[0,"    return [next"]],"start1":3538,"start2":3538,"length1":32,"length2":252},{"diffs":[[0,"n [next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y]\n}\n\n//"]],"start1":3783,"start2":3783,"length1":26,"length2":24}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," = -"],[-1,"9999"],[1,"1000"],[0,"\n   "]],"start1":2594,"start2":2594,"length1":12,"length2":12},{"diffs":[[0," -1000\n    next_"],[-1,"A"],[0,"x = 0  # initial"]],"start1":2596,"start2":2596,"length1":33,"length2":32},{"diffs":[[0,"sition\n    next_"],[-1,"A"],[0,"y = 0\n    valid_"]],"start1":2649,"start2":2649,"length1":33,"length2":32},{"diffs":[[0,"        "],[-1,"proposed_A"],[1,"n"],[0,"x = Ax +"]],"start1":2718,"start2":2718,"length1":26,"length2":17},{"diffs":[[0,"        "],[-1,"proposed_A"],[1,"n"],[0,"y = Ay +"]],"start1":2746,"start2":2746,"length1":26,"length2":17},{"diffs":[[0,"if 0 <= "],[-1,"proposed_A"],[1,"n"],[0,"x <= 4 a"]],"start1":2782,"start2":2782,"length1":26,"length2":17},{"diffs":[[0,"nd 0 <= "],[-1,"proposed_A"],[1,"n"],[0,"y <= 4:\n"]],"start1":2799,"start2":2799,"length1":26,"length2":17},{"diffs":[[0,"ble["],[-1,"proposed_Ax][proposed_Ay"],[1,"ny][nx"],[0,"]\n  "]],"start1":2835,"start2":2835,"length1":32,"length2":14},{"diffs":[[0,"ext_"],[-1,"A"],[0,"x = "],[-1,"proposed_A"],[1,"n"],[0,"x\n  "]],"start1":2918,"start2":2918,"length1":23,"length2":13},{"diffs":[[0,"ext_"],[-1,"A"],[0,"y = "],[-1,"proposed_A"],[1,"n"],[0,"y\n  "]],"start1":2946,"start2":2946,"length1":23,"length2":13},{"diffs":[[0," = True\n"],[1,"    if not valid_found:  # If no valid move found (unlikely), fallback to random\n        print(\"Greedy found no valid move, falling back to random\")\n        return get_valid_random_move(Ax, Ay)\n"],[0,"    retu"]],"start1":2984,"start2":2984,"length1":16,"length2":210},{"diffs":[[0,"rn next_"],[-1,"A"],[0,"x, next_"],[-1,"A"],[0,"y\n\n# Run"]],"start1":3194,"start2":3194,"length1":26,"length2":24}]}]},{"timestamp":1740863225085,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"let rnum: number;\nlet temp = 0\nwhile (temp < 100) {\n    rnum = Math.floor(Math.random() * 4)\n    console.log(rnum)\n    temp += 1\n}\n"],[0,"//  "]],"start1":0,"start2":0,"length1":135,"length2":4},{"diffs":[[0,"r): any[] {\n"],[1,"    let i: number;\n"],[0,"    let prop"]],"start1":2234,"start2":2234,"length1":24,"length2":43},{"diffs":[[0,"= 0\n"],[-1,"    let i = 0\n"],[0,"    "]],"start1":2332,"start2":2332,"length1":22,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"usic"],[-1,"\ntemp = 0\nwhile temp < 100:\n    rnum = Math.floor(Math.random()*4)\n    print(rnum)\n    temp += 1"],[0,"\n\n# "]],"start1":31,"start2":31,"length1":104,"length2":8},{"diffs":[[0,"= 0\n"],[-1,"    i = 0\n"],[0,"    "]],"start1":2045,"start2":2045,"length1":18,"length2":8}]}]},{"timestamp":1740863631859,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"IL_DONE)"],[-1,"\n\n        "]],"start1":6842,"start2":6842,"length1":18,"length2":8}]}]},{"timestamp":1740865823624,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"_DONE)\n\n    "],[1,"    "]],"start1":6844,"start2":6844,"length1":12,"length2":16}]}]},{"timestamp":1740865824499,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"let rnum: number;\nlet temp = 0\nwhile (temp < 100) {\n    rnum = Math.floor(Math.random() * 4)\n    console.log(rnum)\n    temp += 1\n}\n"],[0,"//  Define Q-tab"]],"start1":0,"start2":0,"length1":16,"length2":147},{"diffs":[[0,"1]]\n"],[-1,"//  Global state for custom LCG\nlet rand_state = input.runningTimeMicros() % 65536\n//  Initial seed from runtime\n"],[0,"//  "]],"start1":540,"start2":540,"length1":121,"length2":8},{"diffs":[[0," Ay_0 = "],[-1,"custom_randint(1, 3)"],[1,"Math.floor(Math.random() * 3) + 1"],[0,"\nmaze[Ay"]],"start1":611,"start2":611,"length1":36,"length2":49},{"diffs":[[0,"on\nmaze["],[-1,"custom_randint(1, 3)"],[1,"Math.floor(Math.random() * 3) + 1"],[0,"][2] = 0"]],"start1":717,"start2":717,"length1":36,"length2":49},{"diffs":[[0,"y = "],[-1,"custom_randint(1, 3)"],[1,"Math.floor(Math.random() * 3) + 1"],[0,"\nmaz"]],"start1":857,"start2":857,"length1":28,"length2":41},{"diffs":[[0,"i = "],[-1,"custom_randint(0, 3"],[1,"Math.floor(Math.random() * 4"],[0,")\n  "]],"start1":2489,"start2":2489,"length1":27,"length2":36},{"diffs":[[0,"\n}\n\n"],[-1,"//  Custom pseudo-random number generator (LCG)\nfunction custom_randint(min: number, max: number): number {\n    \n    //  LCG: X_{n+1} = (a * X_n + c) mod m\n    rand_state = (69069 * rand_state + 1) % 65536\n    //  Scale to desired range\n    let range_size = max - min + 1\n    return min + rand_state % range_size\n}\n\n"]],"start1":8171,"start2":8171,"length1":320,"length2":4}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"rt music"],[1,"\ntemp = 0\nwhile temp < 100:\n    rnum = Math.floor(Math.random()*4)\n    print(rnum)\n    temp += 1"],[0,"\n\n# Defi"]],"start1":27,"start2":27,"length1":16,"length2":112},{"diffs":[[0,"\n]\n\n"],[-1,"# Global state for custom LCG\nrand_state = input.running_time_micros() % 65536  # Initial seed from runtime\n\n"],[0,"# Ag"]],"start1":566,"start2":566,"length1":117,"length2":8},{"diffs":[[0,"\nAy_0 = "],[-1,"custom_randint(1, 3)"],[1,"Math.floor(Math.random()*3)+1"],[0,"\nmaze[Ay"]],"start1":627,"start2":627,"length1":36,"length2":45},{"diffs":[[0,"on\nmaze["],[-1,"custom_randint(1, 3)"],[1,"Math.floor(Math.random()*3)+1"],[0,"][2] = 0"]],"start1":728,"start2":728,"length1":36,"length2":45},{"diffs":[[0,"y = "],[-1,"custom_randint(1, 3)"],[1,"Math.floor(Math.random()*3)+1"],[0,"\nmaz"]],"start1":854,"start2":854,"length1":28,"length2":37},{"diffs":[[0,"i = "],[-1,"custom_randint(0, 3"],[1,"Math.floor(Math.random()*4"],[0,")\n  "]],"start1":2181,"start2":2181,"length1":27,"length2":34},{"diffs":[[0,"E)\n\n"],[-1,"# Custom pseudo-random number generator (LCG)\ndef custom_randint(min, max):\n    global rand_state\n    # LCG: X_{n+1} = (a * X_n + c) mod m\n    rand_state = (69069 * rand_state + 1) % 65536\n    # Scale to desired range\n    range_size = max - min + 1\n    return min + (rand_state % range_size)\n"],[1,"    "]],"start1":6848,"start2":6848,"length1":296,"length2":8}]}]},{"timestamp":1740866210048,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"     if "],[-1,"True"],[1,"Math.random() < epsilon"],[0,":\n      "]],"start1":3814,"start2":3814,"length1":20,"length2":39}]}]},{"timestamp":1740868884328,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"    if ("],[-1,"true"],[1,"Math.random() < epsilon"],[0,") {\n    "]],"start1":4753,"start2":4753,"length1":20,"length2":39}]}]},{"timestamp":1740868885973,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"if ("],[-1,"Math.random() < epsilon"],[1,"true"],[0,") {\n"]],"start1":4757,"start2":4757,"length1":31,"length2":12},{"diffs":[[0,"Ay)\n"],[-1,"                console.log(\"\" + next_Ax + \" \" + (\"\" + next_Ay))\n"],[0,"    "]],"start1":4836,"start2":4836,"length1":73,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," if "],[-1,"Math.random() < epsilon"],[1,"True"],[0,":\n  "]],"start1":3818,"start2":3818,"length1":31,"length2":12},{"diffs":[[0,"Ay)\n"],[-1,"                print(str(next_Ax) + \" \" +str(next_Ay))\n"],[0,"    "]],"start1":3889,"start2":3889,"length1":64,"length2":8}]}]},{"timestamp":1740869162713,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\n\n# "],[-1,"Q-learning parameters\nalpha = 0.3  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon_0 = 1.0  # Start with full exploration\nepsilon_min = 0.01  # Lower minimum to favor exploitation later\nepsilon_decay = 0.95  # Slower decay for more exploration\ntraining_speed = 100   #set to control the speed that the agent moves through the maze during training\n\n# Define the maze on the microbit 5x5 LED array (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 1, 1, 1, 1]\n]\n\n# Define Q-table as a 5x5 grid with a single Q-value for each position in the maze\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\n\n# Define possible moves [Up, Down, Left, Right]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]"],[1,"Define Q-table as a 5x5 grid with a single Q-value per state\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\n\n# Define possible moves\nmove_x = [0, 0, -1, 1]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 1, 1, 1, 1]\n]\n\n# Global state for custom LCG\nrand_state = input.running_time_micros() % 65536  # Initial seed from runtime"],[0,"\n\n# "]],"start1":35,"start2":35,"length1":847,"length2":550},{"diffs":[[0,"\nAy_0 = "],[1,"custom_"],[0,"randint("]],"start1":640,"start2":640,"length1":16,"length2":23},{"diffs":[[0,"on\nmaze["],[1,"custom_"],[0,"randint("]],"start1":732,"start2":732,"length1":16,"length2":23},{"diffs":[[0,"oal_y = "],[1,"custom_"],[0,"randint("]],"start1":845,"start2":845,"length1":16,"length2":23},{"diffs":[[0,"ion\n"],[-1,"Qtable[goal_y][goal"],[1,"\n# Q-learning parameters\nalpha = 0.3  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.01  # Lower minimum to favor exploitation later\nepsilon_decay = 0.995  # Slower decay for more exploration\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old"],[0,"_x] ="],[1,"="],[0," 1"],[-1,"0 #set the reward for hitting the goal\n\n\n\n"],[1,":\n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(50)  # Brief delay to make movement visible\n    if (new_x == goal_x and new_y == goal_y):\n        play_ding()\n        for i in range(5):\n            led.toggle(goal_x, goal_y)\n            pause(50)\n    if (maze[new_y][new_x] == 1):\n        play_noise()"],[0,"\n\n# "]],"start1":936,"start2":936,"length1":76,"length2":981},{"diffs":[[0,"  i = 0\n"],[-1,"    next_state = [0,0]\n"],[0,"    whil"]],"start1":2133,"start2":2133,"length1":39,"length2":16},{"diffs":[[0,"le True:"],[1,"  "],[0,"\n       "]],"start1":2148,"start2":2148,"length1":16,"length2":18},{"diffs":[[0,"    i = "],[1,"custom_"],[0,"randint("]],"start1":2163,"start2":2163,"length1":16,"length2":23},{"diffs":[[0,"    "],[-1,"next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        #"],[1,"return proposed_x, proposed_y\n        "],[0,"prin"]],"start1":2332,"start2":2332,"length1":113,"length2":46},{"diffs":[[0,"s fail\n\n"],[1,"\n"],[0,"# Functi"]],"start1":2572,"start2":2572,"length1":16,"length2":17},{"diffs":[[0,"lse\n"],[-1,"    next_state = [0,0]\n"],[0,"    "]],"start1":2778,"start2":2778,"length1":31,"length2":8},{"diffs":[[0,"oposed_A"],[-1,"y"],[1,"x"],[0,"][propos"]],"start1":2964,"start2":2964,"length1":17,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"x"],[1,"y"],[0,"]\n      "]],"start1":2977,"start2":2977,"length1":17,"length2":17},{"diffs":[[0,"   next_"],[-1,"state[0]"],[1,"Ax"],[0," = propo"]],"start1":3055,"start2":3055,"length1":24,"length2":18},{"diffs":[[0,"   next_"],[-1,"state[1]"],[1,"Ay"],[0," = propo"]],"start1":3093,"start2":3093,"length1":24,"length2":18},{"diffs":[[0,"rn next_"],[-1,"state"],[1,"Ax, next_Ay"],[0,"\n\n# Run "]],"start1":3161,"start2":3161,"length1":21,"length2":27},{"diffs":[[0,"    "],[-1,"epsilon ="],[1,"global"],[0," epsilon"],[-1,"_0"],[0,"\n   "]],"start1":3254,"start2":3254,"length1":27,"length2":22},{"diffs":[[0,"n range("],[-1,"5"],[1,"10"],[0,"0):  # 1"]],"start1":3295,"start2":3295,"length1":17,"length2":18},{"diffs":[[0,"pause(10"],[-1,"0/training_speed"],[0,")\n      "]],"start1":3451,"start2":3451,"length1":32,"length2":16},{"diffs":[[0,"10)\n        "],[-1,"#"],[0,"print(\"Episo"]],"start1":3457,"start2":3457,"length1":25,"length2":24},{"diffs":[[0,"    "],[-1,"next"],[1,"old"],[0,"_Ax = "],[-1,"0  #initialize the nex"],[1,"Ax  # Store curren"],[0,"t ag"]],"start1":3650,"start2":3650,"length1":40,"length2":35},{"diffs":[[0,"    "],[-1,"next"],[1,"old"],[0,"_Ay = "],[-1,"0\n            \n            if maze[Ay][Ax] == 1:\n                next_state = [old_Ax, old_Ay]  #if hitting a wall force return to previous position\n            el"],[1,"Ay\n            next_Ax = 0  #initialize the next agent position\n            next_Ay = 0\n            "],[0,"if M"]],"start1":3706,"start2":3706,"length1":181,"length2":117},{"diffs":[[0,"           next_"],[-1,"state"],[1,"Ax, next_Ay"],[0," = get_valid_ran"]],"start1":3852,"start2":3852,"length1":37,"length2":43},{"diffs":[[0,"    "],[-1,"else:\n                next_state = get_valid_greedy_move(Ax, "],[1,"    print(str(next_Ax) + \" \" +str(next_"],[0,"Ay)"],[1,")"],[0,"\n   "]],"start1":3920,"start2":3920,"length1":72,"length2":51},{"diffs":[[0,"    "],[-1,"next_Ax = next_state[0]\n            next_Ay = next_state[1]\n            "],[1,"else:\n                next_Ax, next_Ay = get_valid_greedy_move(Ax, Ay)\n"],[0,"\n   "]],"start1":3976,"start2":3976,"length1":80,"length2":79},{"diffs":[[0,"_Ay)"],[-1," #reward for next step"],[0,"\n   "]],"start1":4097,"start2":4097,"length1":30,"length2":8},{"diffs":[[0,"ward"],[-1,"   # episode cumulative reward\n\n            old_Ax = Ax  # Store current agent position\n            old_Ay = Ay\n\n            #update Qvalue\n            old_q_value = Qtable[old_Ay][old_Ax]\n            if next_Ax == goal_x and next_Ay == goal_y:\n                new_q_value = old_q_value + alpha * (reward - old_q_value)  # Terminal state: no future reward\n                print(\"oldQ=\" + old_q_value + \" newQ=\" + new_q_value)\n            else:\n    "],[1,"\n\n            old_q_value = Qtable[old_Ay][old_Ax]\n"],[0,"    "]],"start1":4134,"start2":4134,"length1":456,"length2":59},{"diffs":[[0,"xt_Ax, next_Ay)\n"],[-1,"    "],[0,"            new_"]],"start1":4228,"start2":4228,"length1":36,"length2":32},{"diffs":[[0,"        "],[-1,"#"],[0,"print(\"M"]],"start1":4482,"start2":4482,"length1":17,"length2":16},{"diffs":[[0,"d))\n"],[-1,"        show_Qtable()"],[0,"\n   "]],"start1":4877,"start2":4877,"length1":29,"length2":8},{"diffs":[[0,"e\")\n    "],[1,"for row in Qtable:\n        print(str(row))\n"],[0,"\ninput.o"]],"start1":4909,"start2":4909,"length1":16,"length2":59},{"diffs":[[0,"pause(10"],[-1,"0/training_speed"],[0,")\n    st"]],"start1":5171,"start2":5171,"length1":32,"length2":16},{"diffs":[[0,"   next_"],[-1,"state"],[1,"Ax, next_Ay"],[0," = get_v"]],"start1":5306,"start2":5306,"length1":21,"length2":27},{"diffs":[[0,"Ay)\n"],[-1,"        next_Ax = next_state[0]\n        next_Ay = next_state[1]\n"],[0,"    "]],"start1":5354,"start2":5354,"length1":72,"length2":8},{"diffs":[[0,"    Ax = next_Ax"],[1,"  "],[0,"\n        Ay = ne"]],"start1":5571,"start2":5571,"length1":32,"length2":34},{"diffs":[[0,"b)\n\n"],[-1,"# Function to update the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1:\n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(500/training_speed)  # Brief delay to make movement visible\n    if (new_x == goal_x and new_y == goal_y):\n        play_ding()\n        for i in range(5):\n            led.toggle(goal_x, goal_y)\n            pause(500/training_speed)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n\n"],[0,"# Ge"]],"start1":5862,"start2":5862,"length1":564,"length2":8},{"diffs":[[0,"ward = -"],[-1,"10"],[1,"5"],[0,"\n    els"]],"start1":6036,"start2":6036,"length1":18,"length2":17},{"diffs":[[0,"_q\n\n"],[-1,"# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\n"],[0,"def "]],"start1":6413,"start2":6413,"length1":205,"length2":8},{"diffs":[[0,"E)\n\n"],[-1,"def format_number(num):\n    num_str = str(num)\n    if \".\" not in num_str:\n        num_str += \".\"  # Add \".\" if no decimal\n    num_str += \"00\" # pad with trailing 0s\n    parts = num_str.split(\".\")\n    int_part = parts[0]\n    frac_part = parts[1]\n    frac_part = frac_part[:2] #limit to 2 decimal places\n        \n    return int_part + \".\" + frac_part  # Raw number string, no padding here\n\n# Function to display the Q-table with aligned formatting\ndef show_Qtable():\n    print(\"Q-Table:\")\n    for row in Qtable:\n        row_str = \"[\"\n        for i in range(5):\n            num_str = format_number(row[i])\n            # Pad to 7 chars manually without len()\n            padded = \"       \" + num_str  # Start with 7 spaces + number\n            padded = padded[-7:]  # Take last 7 chars to trim excess\n            row_str += padded\n            if i < 4:\n                row_str += \",\"  # Add comma and space\n        row_str += \"]\"\n        print(row_str"],[1,"# Custom pseudo-random number generator (LCG)\ndef custom_randint(min, max):\n    global rand_state\n    # LCG: X_{n+1} = (a * X_n + c) mod m\n    rand_state = (69069 * rand_state + 1) % 65536\n    # Scale to desired range\n    range_size = max - min + 1\n    return min + (rand_state % range_size"],[0,")\n"]],"start1":6883,"start2":6883,"length1":953,"length2":296}]}]},{"timestamp":1740949825120,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"//  Q-learning parameters\nlet alpha = 0.3\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon_0 = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.01\n//  Lower minimum to favor exploitation later\nlet epsilon_decay = 0.95\n//  Slower decay for more exploration\nlet training_speed = 100\n// set to control the speed that the agent moves through the maze during training\n//  Define the maze on the microbit 5x5 LED array (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 1], [1, 1, 1, 1, 1]]\n"],[0,"//  "]],"start1":0,"start2":0,"length1":564,"length2":4},{"diffs":[[0,"lue "],[-1,"for each position in the maz"],[1,"per stat"],[0,"e\nle"]],"start1":51,"start2":51,"length1":36,"length2":16},{"diffs":[[0,"oves"],[-1," [Up, Down, Left, Right]"],[0,"\nlet"]],"start1":185,"start2":185,"length1":32,"length2":8},{"diffs":[[0," -1, 1]\n"],[1,"//  Up, Down, Left, Right\n"],[0,"let move"]],"start1":209,"start2":209,"length1":16,"length2":42},{"diffs":[[0,", 0, 0]\n"],[1,"//  Define the maze (1 = wall, 0 = open path)\nlet maze = [[1, 1, 1, 1, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 1], [1, 0, 1, 0, 1], [1, 1, 1, 1, 1]]\n//  Global state for custom LCG\nlet rand_state = input.runningTimeMicros() % 65536\n//  Initial seed from runtime\n"],[0,"//  Agen"]],"start1":262,"start2":262,"length1":16,"length2":272},{"diffs":[[0," Ay_0 = "],[1,"custom_"],[0,"randint("]],"start1":593,"start2":593,"length1":16,"length2":23},{"diffs":[[0,"on\nmaze["],[1,"custom_"],[0,"randint("]],"start1":686,"start2":686,"length1":16,"length2":23},{"diffs":[[0,"oal_y = "],[1,"custom_"],[0,"randint("]],"start1":809,"start2":809,"length1":16,"length2":23},{"diffs":[[0,"ion\n"],[-1,"Qtable[goal_y][goal_"],[1,"//  Q-learning parameters\nlet alpha = 0.3\n//  Learning rate\nlet gamma = 0.9\n//  Discount factor\nlet epsilon = 1.0\n//  Start with full exploration\nlet epsilon_min = 0.01\n//  Lower minimum to favor exploitation later\nlet epsilon_decay = 0.995\n//  Slower decay for more exploration\n//  Function to display the static maze (called once)\nfunction show_maze() {\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            if (maze[y]["],[0,"x] ="],[1,"="],[0," 1"],[-1,"0\n// set the reward for hitting the goal"],[1,") {\n                led.plotBrightness(x, y, 5)\n            }\n            \n        }\n    }\n}\n\n//  Function to update the agent's position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    if (maze[old_y][old_x] == 1) {\n        led.plotBrightness(old_x, old_y, 5)\n    } else {\n        led.unplot(old_x, old_y)\n    }\n    \n    led.plotBrightness(new_x, new_y, 255)\n    pause(50)\n    //  Brief delay to make movement visible\n    if (new_x == goal_x && new_y == goal_y) {\n        play_ding()\n        for (let i = 0; i < 5; i++) {\n            led.toggle(goal_x, goal_y)\n            pause(50)\n        }\n    }\n    \n    if (maze[new_y][new_x] == 1) {\n        play_noise()\n    }\n    \n}\n"],[0,"\n// "]],"start1":901,"start2":901,"length1":74,"length2":1190},{"diffs":[[0,"r, Ay: number): "],[-1,"number"],[1,"any"],[0,"[] {\n    let pro"]],"start1":2296,"start2":2296,"length1":38,"length2":35},{"diffs":[[0,"proposed_x: "],[-1,"number"],[1,"any"],[0,";\n    let pr"]],"start1":2328,"start2":2328,"length1":30,"length2":27},{"diffs":[[0,"proposed_y: "],[-1,"number"],[1,"any"],[0,";\n    let at"]],"start1":2353,"start2":2353,"length1":30,"length2":27},{"diffs":[[0,"= 0\n"],[-1,"    let next_state = [0, 0]\n"],[0,"    "]],"start1":2401,"start2":2401,"length1":36,"length2":8},{"diffs":[[0,"    i = "],[1,"custom_"],[0,"randint("]],"start1":2428,"start2":2428,"length1":16,"length2":23},{"diffs":[[0,"    "],[-1,"next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state"],[1,"return [proposed_x, proposed_y]"],[0,"\n   "]],"start1":2627,"start2":2627,"length1":103,"length2":39},{"diffs":[[0,"        "],[-1,"// print"],[1,"console.log"],[0,"(i + \": "]],"start1":2682,"start2":2682,"length1":24,"length2":27},{"diffs":[[0,"ected: [\" + "],[-1,"str("],[1,"(\"\" + "],[0,"Ax) + \"][\" +"]],"start1":2724,"start2":2724,"length1":28,"length2":30},{"diffs":[[0,"x) + \"][\" + "],[-1,"str("],[1,"(\"\" + "],[0,"Ay) + \"] to "]],"start1":2743,"start2":2743,"length1":28,"length2":30},{"diffs":[[0," \"] to [\" + "],[-1,"str("],[1,"(\"\" + "],[0,"proposed_x) "]],"start1":2766,"start2":2766,"length1":28,"length2":30},{"diffs":[[0," \"][\" + "],[-1,"str("],[1,"(\"\" + "],[0,"proposed"]],"start1":2797,"start2":2797,"length1":20,"length2":22},{"diffs":[[0,"lse\n"],[-1,"    let next_state = [0, 0]\n"],[0,"    "]],"start1":3245,"start2":3245,"length1":36,"length2":8},{"diffs":[[0,"oposed_A"],[-1,"y"],[1,"x"],[0,"][propos"]],"start1":3476,"start2":3476,"length1":17,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"x"],[1,"y"],[0,"]\n      "]],"start1":3489,"start2":3489,"length1":17,"length2":17},{"diffs":[[0,"   next_"],[-1,"state[0]"],[1,"Ax"],[0," = propo"]],"start1":3570,"start2":3570,"length1":24,"length2":18},{"diffs":[[0,"   next_"],[-1,"state[1]"],[1,"Ay"],[0," = propo"]],"start1":3608,"start2":3608,"length1":24,"length2":18},{"diffs":[[0," return "],[1,"["],[0,"next_"],[-1,"state"],[1,"Ax, next_Ay]"],[0,"\n}\n\n//  "]],"start1":3723,"start2":3723,"length1":26,"length2":34},{"diffs":[[0,"reward: number;\n"],[1,"    let old_Ax: number;\n    let old_Ay: number;\n"],[0,"    let next_Ax:"]],"start1":3921,"start2":3921,"length1":32,"length2":80},{"diffs":[[0,"er;\n"],[-1,"    let next_state: number[];\n"],[0,"    "]],"start1":4031,"start2":4031,"length1":38,"length2":8},{"diffs":[[0,"er;\n    let old_"],[-1,"Ax"],[1,"q_value"],[0,": number;\n    le"]],"start1":4055,"start2":4055,"length1":34,"length2":39},{"diffs":[[0,"let "],[-1,"old_Ay: number;\n    let old_q_value"],[1,"next_max_q"],[0,": nu"]],"start1":4092,"start2":4092,"length1":43,"length2":18},{"diffs":[[0,"    "],[-1,"let next_max_q: number;\n    let epsilon = epsilon_0"],[0,"\n   "]],"start1":4145,"start2":4145,"length1":59,"length2":8},{"diffs":[[0,"isode < "],[-1,"5"],[1,"10"],[0,"0; episo"]],"start1":4178,"start2":4178,"length1":17,"length2":18},{"diffs":[[0,"e(10"],[-1,"0 / training_speed)\n        // print"],[1,")\n        console.log"],[0,"(\"Ep"]],"start1":4353,"start2":4353,"length1":44,"length2":29},{"diffs":[[0,"ode \" + "],[-1,"str"],[1,"(\"\" + "],[0,"(episode"]],"start1":4384,"start2":4384,"length1":19,"length2":22},{"diffs":[[0,"ode + 1)"],[1,")"],[0," + \" Sta"]],"start1":4403,"start2":4403,"length1":16,"length2":17},{"diffs":[[0,"rt: \" + "],[-1,"str("],[1,"(\"\" + "],[0,"Ax) + \" "]],"start1":4420,"start2":4420,"length1":20,"length2":22},{"diffs":[[0,"+ \" \" + "],[-1,"str("],[1,"(\"\" + "],[0,"Ay) + \" "]],"start1":4438,"start2":4438,"length1":20,"length2":22},{"diffs":[[0,"\" Goal: \" + "],[-1,"str("],[1,"(\"\" + "],[0,"goal_x) + \" "]],"start1":4458,"start2":4458,"length1":28,"length2":30},{"diffs":[[0,"+ \" \" + "],[-1,"str("],[1,"(\"\" + "],[0,"goal_y))"]],"start1":4484,"start2":4484,"length1":20,"length2":22},{"diffs":[[0,"        "],[-1,"next"],[1,"old"],[0,"_Ax = "],[-1,"0"],[1,"Ax"],[0,"\n       "]],"start1":4558,"start2":4558,"length1":27,"length2":27},{"diffs":[[0,"     // "],[-1,"initialize the nex"],[1," Store curren"],[0,"t agent "]],"start1":4585,"start2":4585,"length1":34,"length2":29},{"diffs":[[0,"        "],[-1,"next"],[1,"old"],[0,"_Ay = "],[-1,"0"],[1,"Ay"],[0,"\n       "]],"start1":4627,"start2":4627,"length1":27,"length2":27},{"diffs":[[0,"    "],[-1,"if (maze[Ay]["],[1,"next_"],[0,"Ax"],[-1,"]"],[0," ="],[-1,"= 1) {"],[1," 0"],[0,"\n   "]],"start1":4655,"start2":4655,"length1":32,"length2":19},{"diffs":[[0,"    "],[-1,"    next_state = [old_Ax, old_Ay]"],[1,"// initialize the next agent position\n            next_Ay = 0"],[0,"\n   "]],"start1":4679,"start2":4679,"length1":41,"length2":69},{"diffs":[[0,"            "],[-1,"} else "],[0,"if (Math.ran"]],"start1":4745,"start2":4745,"length1":31,"length2":24},{"diffs":[[0,"    "],[-1,"// if hitting a wall force return to previous position\n                next_state = get_valid_random_move(Ax, "],[1,"let [next_Ax, next_Ay] = get_valid_random_move(Ax, Ay)\n                console.log(\"\" + next_Ax + \" \" + (\"\" + next_"],[0,"Ay)"],[1,")"],[0,"\n   "]],"start1":4800,"start2":4800,"length1":121,"length2":127},{"diffs":[[0,"            "],[-1,"next_state"],[1,"let [next_Ax, next_Ay]"],[0," = get_valid"]],"start1":4949,"start2":4949,"length1":34,"length2":46},{"diffs":[[0,"   \n"],[-1,"            next_Ax = next_state[0]\n            next_Ay = next_state[1]\n"],[0,"    "]],"start1":5039,"start2":5039,"length1":80,"length2":8},{"diffs":[[0,"Ay)\n"],[-1,"            // reward for next step\n"],[0,"    "]],"start1":5089,"start2":5089,"length1":44,"length2":8},{"diffs":[[0,"    "],[-1,"//  episode cumulative reward\n            old_Ax = Ax\n            //  Store current agent position\n            old_Ay = Ay\n            // update Qvalue\n            old_q_value = Qtable[old_Ay][old_Ax]\n            if (next_Ax == goal_x && next_Ay == goal_y) {\n                new_q_value = old_q_value + alpha * (reward - old_q_value)\n                //  Terminal state: no future reward\n                console.log(\"oldQ=\" + old_q_value + \" newQ=\" + new_q_value)\n            } else {\n    "],[1,"old_q_value = Qtable[old_Ay][old_Ax]\n"],[0,"    "]],"start1":5138,"start2":5138,"length1":496,"length2":45},{"diffs":[[0,"xt_Ax, next_Ay)\n"],[-1,"    "],[0,"            new_"]],"start1":5218,"start2":5218,"length1":36,"length2":32},{"diffs":[[0,"ue)\n"],[-1,"            }\n            \n"],[0,"    "]],"start1":5322,"start2":5322,"length1":35,"length2":8},{"diffs":[[0,"    "],[-1,"// print"],[1,"console.log"],[0,"(\"Mo"]],"start1":5475,"start2":5475,"length1":16,"length2":19},{"diffs":[[0,"e from: \" + "],[-1,"str("],[1,"(\"\" + "],[0,"old_Ax) + \" "]],"start1":5495,"start2":5495,"length1":28,"length2":30},{"diffs":[[0,"+ \" \" + "],[-1,"str("],[1,"(\"\" + "],[0,"old_Ay) "]],"start1":5521,"start2":5521,"length1":20,"length2":22},{"diffs":[[0,"+ \" to: \" + "],[-1,"str("],[1,"(\"\" + "],[0,"next_Ax) + \""]],"start1":5543,"start2":5543,"length1":28,"length2":30},{"diffs":[[0,"+ \" \" + "],[-1,"str("],[1,"(\"\" + "],[0,"next_Ay)"]],"start1":5570,"start2":5570,"length1":20,"length2":22},{"diffs":[[0,"\" + "],[-1,"str("],[1,"(\"\" + "],[0,"rewa"]],"start1":5605,"start2":5605,"length1":12,"length2":14},{"diffs":[[0,"d))\n"],[-1,"        show_Qtable()\n"],[0,"    "]],"start1":5897,"start2":5897,"length1":30,"length2":8},{"diffs":[[0,"plete\")\n"],[1,"    for (let row of Qtable) {\n        console.log(\"\" + row)\n    }\n"],[0,"})\n//  O"]],"start1":5936,"start2":5936,"length1":16,"length2":82},{"diffs":[[0,"er;\n"],[-1,"    let next_state: number[];\n    let next_Ax: number;\n    let next_Ay: number;\n"],[0,"    "]],"start1":6157,"start2":6157,"length1":88,"length2":8},{"diffs":[[0,"pause(10"],[-1,"0 / training_speed"],[0,")\n    le"]],"start1":6288,"start2":6288,"length1":34,"length2":16},{"diffs":[[0,"        "],[-1,"next_state"],[1,"let [next_Ax, next_Ay]"],[0," = get_v"]],"start1":6432,"start2":6432,"length1":26,"length2":38},{"diffs":[[0,"Ay)\n"],[-1,"        next_Ax = next_state[0]\n        next_Ay = next_state[1]\n"],[0,"    "]],"start1":6491,"start2":6491,"length1":72,"length2":8},{"diffs":[[0,"\n})\n"],[-1,"//  Function to update the agent's position\nfunction update_agent_position(old_x: number, old_y: number, new_x: number, new_y: number) {\n    if (maze[old_y][old_x] == 1) {\n        led.plotBrightness(old_x, old_y, 5)\n    } else {\n        led.unplot(old_x, old_y)\n    }\n    \n    led.plotBrightness(new_x, new_y, 255)\n    pause(500 / training_speed)\n    //  Brief delay to make movement visible\n    if (new_x == goal_x && new_y == goal_y) {\n        play_ding()\n        for (let i = 0; i < 5; i++) {\n            led.toggle(goal_x, goal_y)\n            pause(500 / training_speed)\n        }\n    }\n    \n    if (maze[new_y][new_x] == 1) {\n        play_noise()\n    }\n    \n}\n\n"],[0,"//  "]],"start1":6968,"start2":6968,"length1":674,"length2":8},{"diffs":[[0,"ward = -"],[-1,"10"],[1,"5"],[0,"\n    } e"]],"start1":7206,"start2":7206,"length1":18,"length2":17},{"diffs":[[0,"\n}\n\n"],[-1,"//  Function to display the static maze (called once)\nfunction show_maze() {\n    for (let y = 0; y < 5; y++) {\n        for (let x = 0; x < 5; x++) {\n            if (maze[y][x] == 1) {\n                led.plotBrightness(x, y, 5)\n            }\n            \n        }\n    }\n}\n\n"],[0,"func"]],"start1":7778,"start2":7778,"length1":282,"length2":8},{"diffs":[[0,"\n}\n\n"],[-1,"function format_number(num: number) {\n    let num_str = \"\" + num\n    if (num_str.indexOf(\".\") < 0) {\n        num_str += \".\"\n    }\n    \n    //  Add \".\" if no decimal\n    num_str += \"00\"\n    //  pad with trailing 0s\n    let parts = _py.py_string_split(num_str, \".\")\n    let int_part = parts[0]\n    let frac_part = parts[1]\n    frac_part = frac_part.slice(0, 2)\n    // limit to 2 decimal places\n    return int_part + \".\" + frac_part\n}\n\n//  Raw number string, no padding here\n//  Function to display the Q-table with aligned formatting\nfunction show_Qtable() {\n    let row_str: string;\n    let num_str: any;\n    let padded: any;\n    console.log(\"Q-Table:\")\n    for (let row of Qtable) {\n        row_str = \"[\"\n        for (let i = 0; i < 5; i++) {\n            num_str = format_number(row[i])\n            //  Pad to 7 chars manually without len()\n            padded = \"       \" + num_str\n            //  Start with 7 spaces + number\n            padded = padded.slice(-7)\n            //  Take last 7 chars to trim excess\n            row_str += padded\n            if (i < 4) {\n                row_str += \",\"\n            }\n            \n        }\n        //  Add comma and space\n        row_str += \"]\"\n        console.log(row_str)\n    }"],[1,"//  Custom pseudo-random number generator (LCG)\nfunction custom_randint(min: number, max: number): number {\n    \n    //  LCG: X_{n+1} = (a * X_n + c) mod m\n    rand_state = (69069 * rand_state + 1) % 65536\n    //  Scale to desired range\n    let range_size = max - min + 1\n    return min + rand_state % range_size"],[0,"\n}\n\n"]],"start1":8170,"start2":8170,"length1":1234,"length2":320}]}]},{"timestamp":1740949825260,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"arts[0]\n"],[-1,"    \n"],[0,"    frac"]],"start1":7913,"start2":7913,"length1":21,"length2":16}]}]},{"timestamp":1740963807463,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"[0]\n    "],[-1,"int_part = "],[0,"\n    fra"]],"start1":7917,"start2":7917,"length1":27,"length2":16}]}]},{"timestamp":1740963814106,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"gamma = "],[-1,"1"],[1,"0.9"],[0,"\n//  Dis"]],"start1":64,"start2":64,"length1":17,"length2":19},{"diffs":[[0,"peed = 1"],[1,"00"],[0,"\n// set "]],"start1":294,"start2":294,"length1":16,"length2":18},{"diffs":[[0,"on play_"],[-1,"ding"],[1,"noise"],[0,"() {\n   "]],"start1":8746,"start2":8746,"length1":20,"length2":21},{"diffs":[[0,"ape."],[-1,"Sine, 2469, 2996, 98"],[1,"Noise, 2526, 2351"],[0,", "],[1,"5"],[0,"0, 50"],[1,", 1"],[0,"0, S"]],"start1":8813,"start2":8813,"length1":35,"length2":36},{"diffs":[[0,"ve.L"],[-1,"ogarithmic"],[1,"inear"],[0,"), m"]],"start1":8892,"start2":8892,"length1":18,"length2":13},{"diffs":[[0,"\n\nfunction play_"],[-1,"noise"],[1,"ding"],[0,"() {\n    music.p"]],"start1":8935,"start2":8935,"length1":37,"length2":36},{"diffs":[[0,"veShape."],[-1,"Noise, 2526"],[1,"Sine, 1385"],[0,", 23"],[1,"9"],[0,"5"],[-1,"1"],[0,", 50, 50"]],"start1":9005,"start2":9005,"length1":33,"length2":32},{"diffs":[[0,", 2395, 50, 50, "],[-1,"1"],[1,"50"],[0,"0, SoundExpressi"]],"start1":9023,"start2":9023,"length1":33,"length2":34},{"diffs":[[0,"ect."],[-1,"Warbl"],[1,"Non"],[0,"e, I"]],"start1":9062,"start2":9062,"length1":13,"length2":11},{"diffs":[[0,"erpolationCurve."],[-1,"Linear"],[1,"Curve"],[0,"), music.Playbac"]],"start1":9075,"start2":9075,"length1":38,"length2":37},{"diffs":[[0,"decimal\n"],[1,"    num_str += \"00\"\n    //  pad with trailing 0s\n"],[0,"    let "]],"start1":9289,"start2":9289,"length1":16,"length2":65},{"diffs":[[0,"rt ="],[-1," \"      \" +"],[0," par"]],"start1":9410,"start2":9410,"length1":19,"length2":8},{"diffs":[[0,"    "],[-1,"// pad with leading whitespace\n    int_part = int_part.slice(-4)\n    // limit to 4 places left\n    let frac_part = parts[1] + \"00\"\n    //  pad with trailing 0s"],[1,"let frac_part = parts[1]"],[0,"\n   "]],"start1":9424,"start2":9424,"length1":167,"length2":32},{"diffs":[[0,"aces"],[-1," right"],[0,"\n   "]],"start1":9519,"start2":9519,"length1":14,"length2":8},{"diffs":[[0,"//  "],[-1,"recombine"],[1,"Raw"],[0," num"]],"start1":9565,"start2":9565,"length1":17,"length2":11},{"diffs":[[0,"r string"],[1,", no padding here"],[0,"\n//  Fun"]],"start1":9578,"start2":9578,"length1":16,"length2":33},{"diffs":[[0,"r: any;\n"],[1,"    let padded: any;\n"],[0,"    cons"]],"start1":9728,"start2":9728,"length1":16,"length2":37},{"diffs":[[0,"    "],[-1,"row_str"],[1,"//  Pad to 7 chars manually without len()\n            padded = \"       \" + num_str\n            //  Start with 7 spaces"],[0," +"],[-1,"="],[0," num"],[-1,"_str"],[1,"ber\n            padded = padded.slice(-7)\n            //  Take last 7 chars to trim excess\n            row_str += padded"],[0,"\n   "]],"start1":9927,"start2":9927,"length1":26,"length2":252}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"gamma = "],[-1,"1"],[1,"0.9"],[0,"  # Disc"]],"start1":90,"start2":90,"length1":17,"length2":19},{"diffs":[[0,"peed = 1"],[1,"00"],[0,"   #set "]],"start1":300,"start2":300,"length1":16,"length2":18},{"diffs":[[0,"ef play_"],[-1,"ding"],[1,"noise"],[0,"():\n    "]],"start1":7232,"start2":7232,"length1":20,"length2":21},{"diffs":[[0,"ape."],[-1,"SIN"],[1,"NOIS"],[0,"E,\n "]],"start1":7300,"start2":7300,"length1":11,"length2":12},{"diffs":[[0,"   2"],[-1,"469, 2996, 98"],[1,"526, 2351, 50"],[0,", "],[1,"5"],[0,"0, "],[-1,"50"],[1,"1"],[0,"0,\n "]],"start1":7320,"start2":7320,"length1":28,"length2":28},{"diffs":[[0,"ve.L"],[-1,"OGARITHMIC"],[1,"INEAR"],[0,"),\n "]],"start1":7415,"start2":7415,"length1":18,"length2":13},{"diffs":[[0,"L_DONE)\n"],[1,"\n"],[0,"def play"]],"start1":7458,"start2":7458,"length1":16,"length2":17},{"diffs":[[0,"DONE)\n\ndef play_"],[-1,"noise"],[1,"ding"],[0,"():\n    music.pl"]],"start1":7460,"start2":7460,"length1":37,"length2":36},{"diffs":[[0,"ssion(WaveShape."],[-1,"NOIS"],[1,"SIN"],[0,"E,\n            2"]],"start1":7523,"start2":7523,"length1":36,"length2":35},{"diffs":[[0,"    "],[-1,"2526,\n            2351,\n            50,\n            50,\n            1"],[1,"1385, 2395, 50, 50, 50"],[0,"0,\n "]],"start1":7553,"start2":7553,"length1":77,"length2":30},{"diffs":[[0,"ect."],[-1,"WARBL"],[1,"NON"],[0,"E,\n "]],"start1":7612,"start2":7612,"length1":13,"length2":11},{"diffs":[[0,"erpolationCurve."],[-1,"LINEAR"],[1,"CURVE"],[0,"),\n        music"]],"start1":7637,"start2":7637,"length1":38,"length2":37},{"diffs":[[0,"    "],[-1,"parts = num_str.split(\".\")\n    int_part = \"      \" + parts[0] #pad with leading whitespace\n    int_part = int_part[-4:] #limit to 4 places left\n    frac_part = parts[1] + \"00\" # pad with trailing 0s"],[1,"num_str += \"00\" # pad with trailing 0s\n    parts = num_str.split(\".\")\n    int_part = parts[0]\n    int_part = \n    frac_part = parts[1]"],[0,"\n   "]],"start1":7823,"start2":7823,"length1":206,"length2":142},{"diffs":[[0,"aces"],[-1," right"],[1,"\n        "],[0,"\n   "]],"start1":8014,"start2":8014,"length1":14,"length2":17},{"diffs":[[0,"  # "],[-1,"recombine"],[1,"Raw"],[0," num"]],"start1":8065,"start2":8065,"length1":17,"length2":11},{"diffs":[[0,"r string"],[1,", no padding here"],[0,"\n\n# Func"]],"start1":8078,"start2":8078,"length1":16,"length2":33},{"diffs":[[0,"    "],[-1,"row_str += num_str"],[1,"# Pad to 7 chars manually without len()\n            padded = \"       \" + num_str  # Start with 7 spaces + number\n            padded = padded[-7:]  # Take last 7 chars to trim excess\n            row_str += padded"],[0,"\n   "]],"start1":8328,"start2":8328,"length1":26,"length2":219}]}]},{"timestamp":1740964411414,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"55)\n"],[-1,"    if (maze[new_y][new_x] == 1) {\n        play_noise()\n    }\n    \n"],[0,"    "]],"start1":7296,"start2":7296,"length1":75,"length2":8},{"diffs":[[0," }\n    \n"],[1,"    if (maze[new_y][new_x] == 1) {\n        play_noise()\n    }\n    \n"],[0,"}\n\n//  G"]],"start1":7573,"start2":7573,"length1":16,"length2":83}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"55)\n"],[-1,"    if (maze[new_y][new_x] == 1):\n        play_noise()\n"],[0,"    "]],"start1":6178,"start2":6178,"length1":63,"length2":8},{"diffs":[[0,"ed)\n    "],[1,"if (maze[new_y][new_x] == 1):\n        play_noise()"],[0,"\n\n# Get "]],"start1":6418,"start2":6418,"length1":16,"length2":66},{"diffs":[[0,"526,"],[-1," 2351, 50, 50,"],[1,"\n            2351,\n            50,\n            50,\n           "],[0," 10,"]],"start1":7558,"start2":7558,"length1":22,"length2":70}]}]},{"timestamp":1740964668578,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"value = "],[-1,"10"],[1,"old_q_value + alpha * (reward - old_q_value)\n                //  Terminal state: no future reward\n                console.log(\"oldQ=\" + old_q_value + \" newQ=\" + new_q_value)"],[0,"\n       "]],"start1":4861,"start2":4861,"length1":18,"length2":189},{"diffs":[[0,"ld_q_value)\n    "],[1,"        }\n            \n"],[0,"            Qtab"]],"start1":5207,"start2":5207,"length1":32,"length2":55},{"diffs":[[0,"100\n"],[-1,"            }\n            \n"],[0,"    "]],"start1":5315,"start2":5315,"length1":35,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"value = "],[-1,"10"],[1,"old_q_value + alpha * (reward - old_q_value)  # Terminal state: no future reward\n                print(\"oldQ=\" + old_q_value + \" newQ=\" + new_q_value)"],[0,"\n       "]],"start1":4009,"start2":4009,"length1":18,"length2":166},{"diffs":[[0," - old_q_value)\n"],[-1,"    "],[0,"            Qtab"]],"start1":4325,"start2":4325,"length1":36,"length2":32}]}]},{"timestamp":1740965172957,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"ers\n"],[-1,"let training_speed = 10\n// set to control the speed that the agent moves through the maze during training\nlet num_episodes = 20\n// set to control the number of training episodes that will run \n"],[0,"let "]],"start1":22,"start2":22,"length1":201,"length2":8},{"diffs":[[0,"a = 0.3\n"],[1,""],[0,"//  Lear"]],"start1":34,"start2":34,"length1":16,"length2":16},{"diffs":[[0,"a = "],[-1,".09"],[1,"1"],[0,"\n// "]],"start1":68,"start2":68,"length1":11,"length2":9},{"diffs":[[0," = 0.95\n"],[-1,""],[0,"//  Slow"]],"start1":232,"start2":232,"length1":16,"length2":16},{"diffs":[[0,"oration\n"],[1,"let training_speed = 1\n// set to control the speed that the agent moves through the maze during training\n"],[0,"//  Defi"]],"start1":270,"start2":270,"length1":16,"length2":121},{"diffs":[[0,"e = "],[-1,"old_q_value + alpha * (reward - old_q_value)\n            } else {\n                //  Terminal state: no future reward"],[1,"10\n            } else {"],[0,"\n   "]],"start1":4865,"start2":4865,"length1":126,"length2":31},{"diffs":[[0,"ue)\n    "],[-1,"        }\n            \n"],[0,"        "]],"start1":5044,"start2":5044,"length1":39,"length2":16},{"diffs":[[0,") / 100\n"],[1,"            }\n            \n"],[0,"        "]],"start1":5117,"start2":5117,"length1":16,"length2":43},{"diffs":[[0," + \""],[-1,"  "],[0," Epsilon"],[-1,"="],[1,":"],[0," \" +"]],"start1":5543,"start2":5543,"length1":19,"length2":17},{"diffs":[[0," + \""],[-1,"  "],[0," Reward"],[-1,"="],[1,":"],[0," \" +"]],"start1":5601,"start2":5601,"length1":18,"length2":16}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ers\n"],[-1,"training_speed = 10   #set to control the speed that the agent moves through the maze during training\nnum_episodes = 20    #set to control the number of training episodes that will run \n"],[0,"alph"]],"start1":57,"start2":57,"length1":194,"length2":8},{"diffs":[[0,"a = "],[-1,".09"],[1,"1"],[0,"  # "]],"start1":94,"start2":94,"length1":11,"length2":9},{"diffs":[[0,"oration\n"],[1,"training_speed = 1   #set to control the speed that the agent moves through the maze during training"],[0,"\n\n# Defi"]],"start1":280,"start2":280,"length1":16,"length2":116},{"diffs":[[0,"e = "],[-1,"old_q_value + alpha * (reward - old_q_value)  # Terminal state: no future reward"],[1,"10"],[0,"\n   "]],"start1":4013,"start2":4013,"length1":88,"length2":10},{"diffs":[[0,"_value)\n"],[1,"    "],[0,"        "]],"start1":4185,"start2":4185,"length1":16,"length2":20},{"diffs":[[0," + \""],[-1,"  "],[0," Epsilon"],[-1,"="],[1,":"],[0," \" +"]],"start1":4649,"start2":4649,"length1":19,"length2":17},{"diffs":[[0," + \""],[-1,"  "],[0," Reward"],[-1,"="],[1,":"],[0," \" +"]],"start1":4705,"start2":4705,"length1":18,"length2":16}]}]},{"timestamp":1740965464318,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"speed = "],[-1,"5"],[1,"1"],[0,"0\n// set"]],"start1":39,"start2":39,"length1":17,"length2":17},{"diffs":[[0,"amma = ."],[1,"0"],[0,"9\n//  Di"]],"start1":258,"start2":258,"length1":16,"length2":17},{"diffs":[[0,"isode < "],[-1,"num_episodes"],[1,"50"],[0,"; episod"]],"start1":3617,"start2":3617,"length1":28,"length2":18},{"diffs":[[0,"    //  "],[1,"100 "],[0,"training"]],"start1":3646,"start2":3646,"length1":16,"length2":20}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"speed = "],[-1,"5"],[1,"1"],[0,"0   #set"]],"start1":70,"start2":70,"length1":17,"length2":17},{"diffs":[[0,"amma = ."],[1,"0"],[0,"9  # Dis"]],"start1":277,"start2":277,"length1":16,"length2":17},{"diffs":[[0,"nge("],[-1,"num_episodes"],[1,"50"],[0,"):  #"],[1," 100"],[0," tra"]],"start1":2868,"start2":2868,"length1":25,"length2":19}]}]},{"timestamp":1740965967213,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"\n}\n\n"],[-1,"input.onLogoEvent(TouchButtonEvent.Pressed, function on_logo_pressed() {\n    let training_speed = training_speed * 2\n})\n"]],"start1":10229,"start2":10229,"length1":124,"length2":4}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"tr)\n"],[-1,"\ndef on_logo_pressed():\n    training_speed = training_speed*2\ninput.on_logo_event(TouchButtonEvent.PRESSED, on_logo_pressed)"]],"start1":8553,"start2":8553,"length1":128,"length2":4}]}]},{"timestamp":1740966485921,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"speed = "],[-1,"1"],[1,"50"],[0,"\n// set "]],"start1":39,"start2":39,"length1":17,"length2":18},{"diffs":[[0,") {\n    "],[-1,"\n   "],[1,"let"],[0," trainin"]],"start1":10302,"start2":10302,"length1":20,"length2":19}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"speed = "],[-1,"1"],[1,"50"],[0,"   #set "]],"start1":70,"start2":70,"length1":17,"length2":18},{"diffs":[[0,"():\n"],[-1,"    global training_speed\n"],[0,"    "]],"start1":8577,"start2":8577,"length1":34,"length2":8}]}]},{"timestamp":1740966917916,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,")\npause("],[-1,"10"],[1,"5"],[0,"00)\n//  "]],"start1":1403,"start2":1403,"length1":18,"length2":17},{"diffs":[[0,"ars\n"],[-1,"led.plotBrightness(Ax_0, Ay_0, 255)"],[1,"//  Function to get a valid random move (allows walls)\nfunction get_valid_random_move(Ax: number, Ay: number): number[] {\n    let proposed_x: number;\n    let proposed_y: number;\n    let attempts = 0\n    let i = 0\n    let next_state = [0, 0]\n    while (true) {\n        i = randint(0, 3)\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if (0 <= proposed_x && proposed_x <= 4 && (0 <= proposed_y && proposed_y <= 4)) {\n            next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        }\n        \n        // print(i + \": Random move rejected: [\" + str(Ax) + \"][\" + str(Ay) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    }\n}\n\n//  Fallback: return a valid move if all attempts fail\n//  Function to get a valid greedy move (allows walls)\nfunction get_valid_greedy_move(Ax: number, Ay: number): number[] {\n    let proposed_Ax: number;\n    let proposed_Ay: number;\n    let q: number;\n    let best_q = -9999\n    let next_Ax = 0\n    //  initialize the next agent position\n    let next_Ay = 0\n    let valid_found = false\n    let next_state = [0, 0]\n    for (let i = 0; i < 4; i++) {\n        proposed_Ax = Ax + move_x[i]\n        proposed_Ay = Ay + move_y[i]\n        if (0 <= proposed_Ax && proposed_Ax <= 4 && (0 <= proposed_Ay && proposed_Ay <= 4)) {\n            q = Qtable[proposed_Ay][proposed_Ax]\n            if (q > best_q) {\n                best_q = q\n                next_state[0] = proposed_Ax\n                next_state[1] = proposed_Ay\n                valid_found = true\n            }\n            \n        }\n        \n    }\n    return next_state\n}\n"],[0,"\n// "]],"start1":1466,"start2":1466,"length1":43,"length2":1674},{"diffs":[[0,"\n})\n"],[-1,"//  Function to get a valid random move (allows walls)\nfunction get_valid_random_move(Ax: number, Ay: number): number[] {\n    let proposed_x: number;\n    let proposed_y: number;\n    let attempts = 0\n    let i = 0\n    let next_state = [0, 0]\n    while (true) {\n        i = randint(0, 3)\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if (0 <= proposed_x && proposed_x <= 4 && (0 <= proposed_y && proposed_y <= 4)) {\n            next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        }\n        \n        // print(i + \": Random move rejected: [\" + str(Ax) + \"][\" + str(Ay) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    }\n}\n\n//  Fallback: return a valid move if all attempts fail\n//  Function to get a valid greedy move (allows walls)\nfunction get_valid_greedy_move(Ax: number, Ay: number): number[] {\n    let proposed_Ax: number;\n    let proposed_Ay: number;\n    let q: number;\n    let best_q = -9999\n    let next_Ax = 0\n    //  initialize the next agent position\n    let next_Ay = 0\n    let valid_found = false\n    let next_state = [0, 0]\n    for (let i = 0; i < 4; i++) {\n        proposed_Ax = Ax + move_x[i]\n        proposed_Ay = Ay + move_y[i]\n        if (0 <= proposed_Ax && proposed_Ax <= 4 && (0 <= proposed_Ay && proposed_Ay <= 4)) {\n            q = Qtable[proposed_Ay][proposed_Ax]\n            if (q > best_q) {\n                best_q = q\n                next_state[0] = proposed_Ax\n                next_state[1] = proposed_Ay\n                valid_found = true\n            }\n            \n        }\n        \n    }\n    return next_state\n}\n\n"],[0,"//  "]],"start1":5894,"start2":5894,"length1":1675,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"e goal\n\n"],[1,"\n\n\n\n"],[0,"# Show t"]],"start1":1356,"start2":1356,"length1":16,"length2":20},{"diffs":[[0,")\npause("],[-1,"10"],[1,"5"],[0,"00)  # B"]],"start1":1412,"start2":1412,"length1":18,"length2":17},{"diffs":[[0,"ars\n"],[-1,"led.plot_brightness(Ax_0, Ay_0, 255)"],[1,"\n# Function to get a valid random move (allows walls)\ndef get_valid_random_move(Ax, Ay):\n    attempts = 0\n    i = 0\n    next_state = [0,0]\n    while True:\n        i = randint(0, 3)\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if (0 <= proposed_x <= 4 and 0 <= proposed_y <= 4):\n            next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        #print(i + \": Random move rejected: [\" + str(Ax) + \"][\" + str(Ay) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail\n\n# Function to get a valid greedy move (allows walls)\ndef get_valid_greedy_move(Ax, Ay):\n    best_q = -9999\n    next_Ax = 0  # initialize the next agent position\n    next_Ay = 0\n    valid_found = False\n    next_state = [0,0]\n    for i in range(4):\n        proposed_Ax = Ax + move_x[i]\n        proposed_Ay = Ay + move_y[i]\n        if 0 <= proposed_Ax <= 4 and 0 <= proposed_Ay <= 4:\n            q = Qtable[proposed_Ay][proposed_Ax]\n            if q > best_q:\n                best_q = q\n                next_state[0] = proposed_Ax\n                next_state[1] = proposed_Ay\n                valid_found = True\n    return next_state"],[0,"\n\n# "]],"start1":1474,"start2":1474,"length1":44,"length2":1272},{"diffs":[[0,"a)\n\n"],[-1,"# Function to get a valid random move (allows walls)\ndef get_valid_random_move(Ax, Ay):\n    attempts = 0\n    i = 0\n    next_state = [0,0]\n    while True:\n        i = randint(0, 3)\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if (0 <= proposed_x <= 4 and 0 <= proposed_y <= 4):\n            next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        #print(i + \": Random move rejected: [\" + str(Ax) + \"][\" + str(Ay) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail\n\n# Function to get a valid greedy move (allows walls)\ndef get_valid_greedy_move(Ax, Ay):\n    best_q = -9999\n    next_Ax = 0  # initialize the next agent position\n    next_Ay = 0\n    valid_found = False\n    next_state = [0,0]\n    for i in range(4):\n        proposed_Ax = Ax + move_x[i]\n        proposed_Ay = Ay + move_y[i]\n        if 0 <= proposed_Ax <= 4 and 0 <= proposed_Ay <= 4:\n            q = Qtable[proposed_Ay][proposed_Ax]\n            if q > best_q:\n                best_q = q\n                next_state[0] = proposed_Ax\n                next_state[1] = proposed_Ay\n                valid_found = True\n    return next_state\n\n\n\n"],[0,"# On"]],"start1":5023,"start2":5023,"length1":1275,"length2":8}]}]},{"timestamp":1740967469471,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"   pause(100"],[1,"/training_speed"],[0,")\n    steps "]],"start1":5214,"start2":5214,"length1":24,"length2":39}]}]},{"timestamp":1740968101231,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"   pause(100"],[1," / training_speed"],[0,")\n    let st"]],"start1":6287,"start2":6287,"length1":24,"length2":41}]}]},{"timestamp":1740968101415,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[-1,"function initialize() {\n    basic.showIcon(IconNames.Heart)\n}\n\n"],[0,"//  "]],"start1":0,"start2":0,"length1":67,"length2":4},{"diffs":[[0," 255)\n    pause("],[-1,"5"],[1,"1"],[0,"00)\n    let step"]],"start1":6280,"start2":6280,"length1":33,"length2":33},{"diffs":[[0,"  pause("],[-1,"5"],[1,"1"],[0,"00)\n    "]],"start1":6755,"start2":6755,"length1":17,"length2":17},{"diffs":[[0,"\n})\n"],[-1,"input.onButtonPressed(Button.AB, function on_button_pressed_ab() {\n    basic.showIcon(IconNames.Heart)\n})\n"]],"start1":10369,"start2":10369,"length1":110,"length2":4}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"t music\n"],[-1,"    "],[0,"\n# Q-lea"]],"start1":28,"start2":28,"length1":20,"length2":16},{"diffs":[[0,"on\n\n"],[-1,"initialize()\ndef initialize(): #initialize/reset the maze and Qtable\n    "],[1,"\n"],[0,"# De"]],"start1":471,"start2":471,"length1":81,"length2":9},{"diffs":[[0,"n path)\n"],[-1,"    "],[0,"maze = ["]],"start1":542,"start2":542,"length1":20,"length2":16},{"diffs":[[0,"h)\nmaze = [\n"],[-1,"    "],[0,"    [1, 1, 1"]],"start1":547,"start2":547,"length1":28,"length2":24},{"diffs":[[0,", 1, 1, 1],\n    "],[-1," "],[-1,"   "],[0,"[1, 0, 1, 0, 1],"]],"start1":568,"start2":568,"length1":36,"length2":32},{"diffs":[[0,", 1, 0, 1],\n    "],[-1,"   "],[-1," "],[0,"[1, 0, 1, 0, 1],"]],"start1":589,"start2":589,"length1":36,"length2":32},{"diffs":[[0," 0, 1],\n    "],[-1,"  "],[-1,"  "],[0,"[1, 0, 1, 0,"]],"start1":614,"start2":614,"length1":28,"length2":24},{"diffs":[[0,"1],\n    "],[-1," "],[-1,"   "],[0,"[1, 1, 1"]],"start1":639,"start2":639,"length1":20,"length2":16},{"diffs":[[0,", 1, 1]\n"],[-1,"    ]\n\n    "],[1,"]\n\n"],[0,"# Define"]],"start1":655,"start2":655,"length1":27,"length2":19},{"diffs":[[0,"he maze\n"],[-1,"    "],[0,"Qtable ="]],"start1":741,"start2":741,"length1":20,"length2":16},{"diffs":[[0,"ble = [\n"],[-1,"    "],[0,"    [0, "]],"start1":752,"start2":752,"length1":20,"length2":16},{"diffs":[[0,"0, 0, 0, 0, 0],\n"],[-1,"    "],[0,"    [0, 0, 0, 0,"]],"start1":765,"start2":765,"length1":36,"length2":32},{"diffs":[[0,", 0, 0, 0],\n    "],[-1,"  "],[-1,"  "],[0,"[0, 0, 0, 0, 0],"]],"start1":790,"start2":790,"length1":36,"length2":32},{"diffs":[[0,"0, 0, 0, 0, 0],\n"],[-1,"    "],[0,"    [0, 0, 0, 0,"]],"start1":807,"start2":807,"length1":36,"length2":32},{"diffs":[[0," 0, 0],\n"],[-1,"    "],[0,"    [0, "]],"start1":836,"start2":836,"length1":20,"length2":16},{"diffs":[[0," 0]\n"],[-1,"    ]\n\n    "],[1,"]\n\n"],[0,"# De"]],"start1":860,"start2":860,"length1":19,"length2":11},{"diffs":[[0," Right]\n"],[-1,"    "],[0,"move_x ="]],"start1":907,"start2":907,"length1":20,"length2":16},{"diffs":[[0," -1, 1]\n"],[-1,"    "],[0,"move_y ="]],"start1":930,"start2":930,"length1":20,"length2":16},{"diffs":[[0," 0, 0]\n\n"],[-1,"    "],[0,"# Agent "]],"start1":954,"start2":954,"length1":20,"length2":16},{"diffs":[[0,"ion\n"],[-1,"    "],[1,"A"],[0,"x_0 = 0\n"],[-1,"    "],[0,"Ay_0"]],"start1":1007,"start2":1007,"length1":24,"length2":17},{"diffs":[[0,"ndint(1, 3)\n"],[-1,"    "],[0,"maze[Ay_0][A"]],"start1":1029,"start2":1029,"length1":28,"length2":24},{"diffs":[[0,"osition\n"],[-1,"    "],[0,"maze[ran"]],"start1":1099,"start2":1099,"length1":20,"length2":16},{"diffs":[[0,"er wall\n"],[-1,"    "],[0,"goal_x ="]],"start1":1173,"start2":1173,"length1":20,"length2":16},{"diffs":[[0,"osition\n"],[-1,"    "],[0,"goal_y ="]],"start1":1201,"start2":1201,"length1":20,"length2":16},{"diffs":[[0,"t(1, 3)\n"],[-1,"    "],[0,"maze[goa"]],"start1":1224,"start2":1224,"length1":20,"length2":16},{"diffs":[[0,"osition\n"],[-1,"    "],[0,"Qtable[g"]],"start1":1290,"start2":1290,"length1":20,"length2":16},{"diffs":[[0,"e goal\n\n"],[-1,"    "],[0,"# Show t"]],"start1":1356,"start2":1356,"length1":20,"length2":16},{"diffs":[[0,"e start\n"],[-1,"    "],[0,"show_maz"]],"start1":1390,"start2":1390,"length1":20,"length2":16},{"diffs":[[0,"show_maze()\n"],[-1,"    "],[0,"pause(1000) "]],"start1":1398,"start2":1398,"length1":28,"length2":24},{"diffs":[[0,"appears\n"],[-1,"    "],[0,"led.plot"]],"start1":1467,"start2":1467,"length1":20,"length2":16},{"diffs":[[0," 255)\n    pause("],[-1,"5"],[1,"1"],[0,"00)\n    steps = "]],"start1":5207,"start2":5207,"length1":33,"length2":33},{"diffs":[[0,"  pause("],[-1,"5"],[1,"1"],[0,"00)\n    "]],"start1":5659,"start2":5659,"length1":17,"length2":17},{"diffs":[[0,"sed)"],[-1,"\n\ndef on_button_pressed_ab():\n    initialize()\ninput.on_button_pressed(Button.AB, on_button_pressed_ab)"]],"start1":8723,"start2":8723,"length1":107,"length2":4}]}]},{"timestamp":1740968609200,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[1,"function initialize() {\n    basic.showIcon(IconNames.Heart)\n}\n\n"],[0,"//  Q-learni"]],"start1":0,"start2":0,"length1":12,"length2":75},{"diffs":[[0,"peed * 2\n})\n"],[1,"input.onButtonPressed(Button.AB, function on_button_pressed_ab() {\n    basic.showIcon(IconNames.Heart)\n})\n"]],"start1":10424,"start2":10424,"length1":12,"length2":118}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ration\n\n"],[-1,"\n"],[1,"initialize()\ndef initialize(): #initialize/reset the maze and Qtable\n    "],[0,"# Define"]],"start1":471,"start2":471,"length1":17,"length2":89},{"diffs":[[0,"n path)\n"],[1,"    "],[0,"maze = ["]],"start1":618,"start2":618,"length1":16,"length2":20},{"diffs":[[0,"   maze = [\n"],[1,"    "],[0,"    [1, 1, 1"]],"start1":627,"start2":627,"length1":24,"length2":28},{"diffs":[[0,", 1, 1, 1],\n    "],[1," "],[1,"   "],[0,"[1, 0, 1, 0, 1],"]],"start1":652,"start2":652,"length1":32,"length2":36},{"diffs":[[0,", 1, 0, 1],\n    "],[1,"   "],[1," "],[0,"[1, 0, 1, 0, 1],"]],"start1":677,"start2":677,"length1":32,"length2":36},{"diffs":[[0," 0, 1],\n    "],[1,"  "],[1,"  "],[0,"[1, 0, 1, 0,"]],"start1":706,"start2":706,"length1":24,"length2":28},{"diffs":[[0," 0, 1],\n    "],[1," "],[1,"   "],[0,"[1, 1, 1, 1,"]],"start1":731,"start2":731,"length1":24,"length2":28},{"diffs":[[0,", 1, 1]\n"],[-1,"]\n\n"],[1,"    ]\n\n    "],[0,"# Define"]],"start1":755,"start2":755,"length1":19,"length2":27},{"diffs":[[0,"he maze\n"],[1,"    "],[0,"Qtable ="]],"start1":849,"start2":849,"length1":16,"length2":20},{"diffs":[[0," Qtable = [\n"],[1,"    "],[0,"    [0, 0, 0"]],"start1":860,"start2":860,"length1":24,"length2":28},{"diffs":[[0,"0, 0, 0, 0, 0],\n"],[1,"    "],[0,"    [0, 0, 0, 0,"]],"start1":881,"start2":881,"length1":32,"length2":36},{"diffs":[[0,", 0, 0, 0],\n    "],[1,"  "],[1,"  "],[0,"[0, 0, 0, 0, 0],"]],"start1":910,"start2":910,"length1":32,"length2":36},{"diffs":[[0,"0, 0, 0, 0, 0],\n"],[1,"    "],[0,"    [0, 0, 0, 0,"]],"start1":931,"start2":931,"length1":32,"length2":36},{"diffs":[[0,", 0, 0, 0],\n"],[1,"    "],[0,"    [0, 0, 0"]],"start1":960,"start2":960,"length1":24,"length2":28},{"diffs":[[0,", 0, 0]\n"],[-1,"]\n\n"],[1,"    ]\n\n    "],[0,"# Define"]],"start1":988,"start2":988,"length1":19,"length2":27},{"diffs":[[0," Right]\n"],[1,"    "],[0,"move_x ="]],"start1":1047,"start2":1047,"length1":16,"length2":20},{"diffs":[[0," -1, 1]\n"],[1,"    "],[0,"move_y ="]],"start1":1074,"start2":1074,"length1":16,"length2":20},{"diffs":[[0," 0, 0]\n\n"],[1,"    "],[0,"# Agent "]],"start1":1102,"start2":1102,"length1":16,"length2":20},{"diffs":[[0,"ion\n"],[-1,"A"],[1,"    "],[0,"x_0 = 0\n"],[1,"    "],[0,"Ay_0"]],"start1":1159,"start2":1159,"length1":17,"length2":24},{"diffs":[[0,"ndint(1, 3)\n"],[1,"    "],[0,"maze[Ay_0][A"]],"start1":1188,"start2":1188,"length1":24,"length2":28},{"diffs":[[0,"osition\n"],[1,"    "],[0,"maze[ran"]],"start1":1262,"start2":1262,"length1":16,"length2":20},{"diffs":[[0,"er wall\n"],[1,"    "],[0,"goal_x ="]],"start1":1340,"start2":1340,"length1":16,"length2":20},{"diffs":[[0,"osition\n"],[1,"    "],[0,"goal_y ="]],"start1":1372,"start2":1372,"length1":16,"length2":20},{"diffs":[[0,"t(1, 3)\n"],[1,"    "],[0,"maze[goa"]],"start1":1399,"start2":1399,"length1":16,"length2":20},{"diffs":[[0,"osition\n"],[1,"    "],[0,"Qtable[g"]],"start1":1469,"start2":1469,"length1":16,"length2":20},{"diffs":[[0,"e goal\n\n"],[1,"    "],[0,"# Show t"]],"start1":1539,"start2":1539,"length1":16,"length2":20},{"diffs":[[0,"e start\n"],[1,"    "],[0,"show_maz"]],"start1":1577,"start2":1577,"length1":16,"length2":20},{"diffs":[[0,"_maze()\n"],[1,"    "],[0,"pause(10"]],"start1":1593,"start2":1593,"length1":16,"length2":20},{"diffs":[[0,"appears\n"],[1,"    "],[0,"led.plot"]],"start1":1662,"start2":1662,"length1":16,"length2":20},{"diffs":[[0,"essed)\n\n"],[1,"def on_button_pressed_ab():\n    initialize()\ninput.on_button_pressed(Button.AB, on_button_pressed_ab)"]],"start1":8920,"start2":8920,"length1":8,"length2":109}]}]},{"timestamp":1740969197964,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"maze"],[-1,"_c"],[0," = ["],[1,"\n    "],[0,"[1, "],[-1,"0, 0, 0"],[1,"1, 1, 1"],[0,", 1]"]],"start1":554,"start2":554,"length1":25,"length2":28},{"diffs":[[0,"   ["],[-1,"0"],[1,"1"],[0,", 0, 1, "],[-1,"1"],[1,"0"],[0,", 1]"]],"start1":606,"start2":606,"length1":18,"length2":18},{"diffs":[[0,"1, 0, 1, 0, "],[-1,"0"],[1,"1"],[0,"],\n    [1, 0"]],"start1":631,"start2":631,"length1":25,"length2":25},{"diffs":[[0,"    [1, "],[-1,"0, 0, 0"],[1,"1, 1, 1"],[0,", 1]\n]\n\n"]],"start1":647,"start2":647,"length1":23,"length2":23}]}]},{"timestamp":1740974065368,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ze_c = ["],[-1,"\n    "],[0,"[1, 0, 0"]],"start1":556,"start2":556,"length1":21,"length2":16}]}]},{"timestamp":1740974073016,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"maze = [[1, "],[-1,"0, 0, 0"],[1,"1, 1, 1"],[0,", 1], [1, 0,"]],"start1":551,"start2":551,"length1":31,"length2":31},{"diffs":[[0,"], ["],[-1,"0"],[1,"1"],[0,", 0, 1, "],[-1,"1"],[1,"0"],[0,", 1]"]],"start1":590,"start2":590,"length1":18,"length2":18},{"diffs":[[0," 0, "],[-1,"0"],[1,"1"],[0,"], [1, "],[-1,"0, 0, 0"],[1,"1, 1, 1"],[0,", 1]]\n"],[-1,"// Goal position\nlet goal_row = 3\n//  row of the goal position (0-4)\nlet goal_col = 4\n//  column of the goal position (0-4)\n// Agent starting position\ngoal_row = 3\n//  row of the goal position (0-4)\ngoal_col = 4\n//  column of the goal position (0-4)\n"],[0,"//  "]],"start1":619,"start2":619,"length1":279,"length2":29},{"diffs":[[0,"er wall\n"],[1,"let goal_x = 4\n//  Goal position\nlet goal_y = randint(1, 3)\n"],[0,"maze[goa"]],"start1":1155,"start2":1155,"length1":16,"length2":76},{"diffs":[[0,")\nmaze[goal_"],[-1,"row"],[1,"y"],[0,"][goal_"],[-1,"col"],[1,"x"],[0,"] = 0\n//  Op"]],"start1":1221,"start2":1221,"length1":37,"length2":33},{"diffs":[[0,"oal_"],[-1,"row"],[1,"y"],[0,"][goal_"],[-1,"col"],[1,"x"],[0,"] = "]],"start1":1298,"start2":1298,"length1":21,"length2":17},{"diffs":[[0,"tr(goal_"],[-1,"col"],[1,"x"],[0,") + \" \" "]],"start1":2290,"start2":2290,"length1":19,"length2":17},{"diffs":[[0,"tr(goal_"],[-1,"row"],[1,"y"],[0,"))\n     "]],"start1":2310,"start2":2310,"length1":19,"length2":17},{"diffs":[[0,"ile (Ax != goal_"],[-1,"col"],[1,"x"],[0," || Ay != goal_r"]],"start1":2332,"start2":2332,"length1":35,"length2":33},{"diffs":[[0,"x || Ay != goal_"],[-1,"row"],[1,"y"],[0,") {\n            "]],"start1":2348,"start2":2348,"length1":35,"length2":33},{"diffs":[[0,"_Ax == goal_"],[-1,"col"],[1,"x"],[0," && next_Ay "]],"start1":3265,"start2":3265,"length1":27,"length2":25},{"diffs":[[0,"next_Ay == goal_"],[-1,"row"],[1,"y"],[0,") {\n            "]],"start1":3282,"start2":3282,"length1":35,"length2":33},{"diffs":[[0,"!= goal_"],[-1,"col"],[1,"x"],[0," || Ay !"]],"start1":6333,"start2":6333,"length1":19,"length2":17},{"diffs":[[0," Ay != goal_"],[-1,"row"],[1,"y"],[0,") {\n        "]],"start1":6345,"start2":6345,"length1":27,"length2":25},{"diffs":[[0,"toggle(goal_"],[-1,"col"],[1,"x"],[0,", goal_"],[-1,"row"],[1,"y"],[0,")\n    consol"]],"start1":6892,"start2":6892,"length1":37,"length2":33},{"diffs":[[0," (new_x == goal_"],[-1,"col"],[1,"x"],[0," && new_y == goa"]],"start1":7492,"start2":7492,"length1":35,"length2":33},{"diffs":[[0,"& new_y == goal_"],[-1,"row"],[1,"y"],[0,") {\n        play"]],"start1":7511,"start2":7511,"length1":35,"length2":33},{"diffs":[[0,"oal_"],[-1,"col"],[1,"x"],[0,", goal_"],[-1,"row"],[1,"y"],[0,")\n  "]],"start1":7614,"start2":7614,"length1":21,"length2":17},{"diffs":[[0,"oal_"],[-1,"col"],[1,"x"],[0," && "]],"start1":7827,"start2":7827,"length1":11,"length2":9},{"diffs":[[0,"== goal_"],[-1,"row"],[1,"y"],[0,") {\n    "]],"start1":7842,"start2":7842,"length1":19,"length2":17}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"th)\nmaze"],[1,"_c"],[0," = [\n   "]],"start1":550,"start2":550,"length1":16,"length2":18},{"diffs":[[0,", 0, 0],"],[-1," "],[0,"\n    [1,"]],"start1":640,"start2":640,"length1":17,"length2":16},{"diffs":[[0,"\n]\n\n"],[-1,"#Goal position\ngoal_row = 3  # row of the goal position (0-4)\ngoal_col = 4  # column of the goal position (0-4)\n\n#Agent starting position\nAgent_row_0 = 0  # row of the goal position (0-4)\nAgent_col_0 = randint(1, 3)\ngoal_row = 3  \ngoal_col = 4  \n\n"],[0,"# De"]],"start1":668,"start2":668,"length1":255,"length2":8},{"diffs":[[0,"ization\n"],[1,"Ax_0 = 0\nAy_0 = randint(1, 3)"],[0,"\nmaze[Ay"]],"start1":1009,"start2":1009,"length1":16,"length2":45},{"diffs":[[0,"er wall\n"],[1,"goal_x = 4  # Goal position\ngoal_y = randint(1, 3)"],[0,"\nmaze[go"]],"start1":1179,"start2":1179,"length1":16,"length2":66},{"diffs":[[0,")\nmaze[goal_"],[-1,"row"],[1,"y"],[0,"][goal_"],[-1,"col"],[1,"x"],[0,"] = 0  # Ope"]],"start1":1236,"start2":1236,"length1":37,"length2":33},{"diffs":[[0,"oal_"],[-1,"row"],[1,"y"],[0,"][goal_"],[-1,"col"],[1,"x"],[0,"] = "]],"start1":1312,"start2":1312,"length1":21,"length2":17},{"diffs":[[0,"tr(goal_"],[-1,"col"],[1,"x"],[0,") + \" \" "]],"start1":1923,"start2":1923,"length1":19,"length2":17},{"diffs":[[0,"tr(goal_"],[-1,"row"],[1,"y"],[0,"))\n    \n"]],"start1":1943,"start2":1943,"length1":19,"length2":17},{"diffs":[[0,"ile (Ax != goal_"],[-1,"col"],[1,"x"],[0," or Ay != goal_r"]],"start1":1970,"start2":1970,"length1":35,"length2":33},{"diffs":[[0,"x or Ay != goal_"],[-1,"row"],[1,"y"],[0,"):\n            n"]],"start1":1986,"start2":1986,"length1":35,"length2":33},{"diffs":[[0,"_Ax == goal_"],[-1,"col"],[1,"x"],[0," and next_Ay"]],"start1":2817,"start2":2817,"length1":27,"length2":25},{"diffs":[[0,"== goal_"],[-1,"row"],[1,"y"],[0,":\n      "]],"start1":2843,"start2":2843,"length1":19,"length2":17},{"diffs":[[0,"!= goal_"],[-1,"col"],[1,"x"],[0," or Ay !"]],"start1":5262,"start2":5262,"length1":19,"length2":17},{"diffs":[[0," Ay != goal_"],[-1,"row"],[1,"y"],[0,"):\n        o"]],"start1":5274,"start2":5274,"length1":27,"length2":25},{"diffs":[[0,"toggle(goal_"],[-1,"col"],[1,"x"],[0,", goal_"],[-1,"row"],[1,"y"],[0,")\n    print("]],"start1":5796,"start2":5796,"length1":37,"length2":33},{"diffs":[[0," (new_x == goal_"],[-1,"col"],[1,"x"],[0," and new_y == go"]],"start1":6366,"start2":6366,"length1":35,"length2":33},{"diffs":[[0,"d new_y == goal_"],[-1,"row"],[1,"y"],[0,"):\n        play_"]],"start1":6386,"start2":6386,"length1":35,"length2":33},{"diffs":[[0,"oal_"],[-1,"col"],[1,"x"],[0,", goal_"],[-1,"row"],[1,"y"],[0,")\n  "]],"start1":6477,"start2":6477,"length1":21,"length2":17},{"diffs":[[0,"oal_"],[-1,"col"],[1,"x"],[0," and"]],"start1":6614,"start2":6614,"length1":11,"length2":9},{"diffs":[[0,"== goal_"],[-1,"row"],[1,"y"],[0,"):\n     "]],"start1":6630,"start2":6630,"length1":19,"length2":17}]}]},{"timestamp":1740974671945,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"ion\n"],[-1,"let Agent"],[1,"goal"],[0,"_row"],[-1,"_0"],[0," = "],[-1,"2"],[1,"3"],[0,"\n// "]],"start1":791,"start2":791,"length1":27,"length2":20},{"diffs":[[0," row of the "],[-1,"agent starting"],[1,"goal"],[0," position (0"]],"start1":811,"start2":811,"length1":38,"length2":28},{"diffs":[[0,"-4)\n"],[-1,"let Agent"],[1,"goal"],[0,"_col"],[-1,"_0"],[0," = "],[-1,"0"],[1,"4"],[0,"\n// "]],"start1":839,"start2":839,"length1":27,"length2":20},{"diffs":[[0,"the "],[-1,"agent starting"],[1,"goal"],[0," pos"]],"start1":870,"start2":870,"length1":22,"length2":12},{"diffs":[[0,"ion\n"],[-1,"maze[Agent_col_0][Agent_row"],[1,"let Ax_0 = 0\nlet Ay_0 = randint(1, 3)\nmaze[Ay_0][Ax"],[0,"_0] "]],"start1":1229,"start2":1229,"length1":35,"length2":59},{"diffs":[[0,"ss(A"],[-1,"gent_row_0, Agent_col"],[1,"x_0, Ay"],[0,"_0, "]],"start1":1685,"start2":1685,"length1":29,"length2":15},{"diffs":[[0,"   let A"],[-1,"gent_row"],[1,"x"],[0,": number"]],"start1":1818,"start2":1818,"length1":24,"length2":17},{"diffs":[[0,"   let A"],[-1,"gent_col"],[1,"y"],[0,": number"]],"start1":1838,"start2":1838,"length1":24,"length2":17},{"diffs":[[0,"er;\n"],[-1,"    let Ax: number;\n    let Ay: number;\n"],[0,"    "]],"start1":2123,"start2":2123,"length1":48,"length2":8},{"diffs":[[0,"   A"],[-1,"gent_row = Agent_row_0\n        Agent_col = Agent_col"],[1,"x = Ax_0\n        Ay = Ay"],[0,"_0\n "]],"start1":2253,"start2":2253,"length1":60,"length2":32},{"diffs":[[0," = A"],[-1,"gent_row"],[1,"x"],[0,"_0\n "]],"start1":6407,"start2":6407,"length1":16,"length2":9},{"diffs":[[0," = A"],[-1,"gent_col"],[1,"y"],[0,"_0\n "]],"start1":6425,"start2":6425,"length1":16,"length2":9}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"row_0 = "],[-1,"2"],[1,"0"],[0,"  # row "]],"start1":815,"start2":815,"length1":17,"length2":17},{"diffs":[[0," row of the "],[-1,"agent starting"],[1,"goal"],[0," position (0"]],"start1":827,"start2":827,"length1":38,"length2":28},{"diffs":[[0,"0 = "],[-1,"0  # column of the agent starting position (0-4)"],[1,"randint(1, 3)\ngoal_row = 3  \ngoal_col = 4  "],[0,"\n\n# "]],"start1":869,"start2":869,"length1":56,"length2":51},{"diffs":[[0,"ze[A"],[-1,"gent_col_0][Agent_row"],[1,"y_0][Ax"],[0,"_0] "]],"start1":1266,"start2":1266,"length1":29,"length2":15},{"diffs":[[0,"ss(A"],[-1,"gent_row_0, Agent_col"],[1,"x_0, Ay"],[0,"_0, "]],"start1":1673,"start2":1673,"length1":29,"length2":15},{"diffs":[[0,"   A"],[-1,"gent_row = Agent_row_0\n        Agent_col = Agent_col"],[1,"x = Ax_0\n        Ay = Ay"],[0,"_0\n "]],"start1":1861,"start2":1861,"length1":60,"length2":32},{"diffs":[[0,"  Ax = A"],[-1,"gent_row"],[1,"x"],[0,"_0\n    A"]],"start1":5320,"start2":5320,"length1":24,"length2":17},{"diffs":[[0,"  Ay = A"],[-1,"gent_col"],[1,"y"],[0,"_0\n    e"]],"start1":5334,"start2":5334,"length1":24,"length2":17}]}]},{"timestamp":1740974898001,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"//  "],[-1,"Initializ"],[1,"Defin"],[0,"e Q-"]],"start1":928,"start2":928,"length1":17,"length2":13},{"diffs":[[0,"0]]\n"],[-1,"Qtable[goal_row][goal_col] = 10\n// set the reward for hitting the goal\n"],[0,"//  "]],"start1":1108,"start2":1108,"length1":79,"length2":8},{"diffs":[[0,", 0, 0]\n"],[1,"//  Agent starting position and maze randomization\nmaze[Agent_col_0][Agent_row_0] = 0\n//  Open up the maze at the starting position\nmaze[randint(1, 3)][2] = 0\n//  Open up a random passage in the center wall\nmaze[goal_row][goal_col] = 0\n//  Open up the maze at the goal position\nQtable[goal_row][goal_col] = 10\n// set the reward for hitting the goal\n"],[0,"//  Show"]],"start1":1208,"start2":1208,"length1":16,"length2":365},{"diffs":[[0,"s(Agent_"],[-1,"col"],[1,"row"],[0,"_0, Agen"]],"start1":1696,"start2":1696,"length1":19,"length2":19},{"diffs":[[0,"ow_0, Agent_"],[-1,"row"],[1,"col"],[0,"_0, 255)\n// "]],"start1":1705,"start2":1705,"length1":27,"length2":27},{"diffs":[[0,"htness(A"],[-1,"gent_col"],[1,"x"],[0,", Ay, 25"]],"start1":2437,"start2":2437,"length1":24,"length2":17}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"\n\n# "],[-1,"Initializ"],[1,"Defin"],[0,"e Q-"]],"start1":931,"start2":931,"length1":17,"length2":13},{"diffs":[[0,"0]\n]"],[-1,"\nQtable[goal_row][goal_col] = 10 #set the reward for hitting the goal"],[0,"\n\n# "]],"start1":1128,"start2":1128,"length1":77,"length2":8},{"diffs":[[0," 0, 0]\n\n"],[1,"# Agent starting position and maze randomization\n\nmaze[Agent_col_0][Agent_row_0] = 0  # Open up the maze at the starting position\nmaze[randint(1, 3)][2] = 0  # Open up a random passage in the center wall\n\nmaze[goal_row][goal_col] = 0  # Open up the maze at the goal position\nQtable[goal_row][goal_col] = 10 #set the reward for hitting the goal\n\n"],[0,"# Show t"]],"start1":1221,"start2":1221,"length1":16,"length2":361},{"diffs":[[0,"s(Agent_"],[-1,"col"],[1,"row"],[0,"_0, Agen"]],"start1":1703,"start2":1703,"length1":19,"length2":19},{"diffs":[[0,"ow_0, Agent_"],[-1,"row"],[1,"col"],[0,"_0, 255)\n\n# "]],"start1":1712,"start2":1712,"length1":27,"length2":27},{"diffs":[[0,"htness(A"],[-1,"gent_col"],[1,"x"],[0,", Ay, 25"]],"start1":2016,"start2":2016,"length1":24,"length2":17}]}]},{"timestamp":1740975490911,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"  let next_A"],[-1,"gent_col"],[1,"x"],[0,": number;\n  "]],"start1":1655,"start2":1655,"length1":32,"length2":25},{"diffs":[[0,"  let next_A"],[-1,"gent_row"],[1,"y"],[0,": number;\n  "]],"start1":1680,"start2":1680,"length1":32,"length2":25},{"diffs":[[0,"et old_A"],[-1,"gent_col"],[1,"x"],[0,": number"]],"start1":1762,"start2":1762,"length1":24,"length2":17},{"diffs":[[0,"et old_A"],[-1,"gent_row"],[1,"y"],[0,": number"]],"start1":1786,"start2":1786,"length1":24,"length2":17},{"diffs":[[0,"_max_q: number;\n"],[1,"    let Ax: number;\n    let Ay: number;\n"],[0,"    let epsilon "]],"start1":1875,"start2":1875,"length1":32,"length2":72},{"diffs":[[0,"Agent_col, A"],[-1,"gent_row"],[1,"y"],[0,", 255)\n     "]],"start1":2170,"start2":2170,"length1":32,"length2":25},{"diffs":[[0,"t: \" + str(A"],[-1,"gent_col"],[1,"x"],[0,") + \" \" + st"]],"start1":2281,"start2":2281,"length1":32,"length2":25},{"diffs":[[0," \" \" + str(A"],[-1,"gent_row"],[1,"y"],[0,") + \" Goal: "]],"start1":2297,"start2":2297,"length1":32,"length2":25},{"diffs":[[0,"    while (A"],[-1,"gent_col"],[1,"x"],[0," != goal_col"]],"start1":2367,"start2":2367,"length1":32,"length2":25},{"diffs":[[0,"oal_col || A"],[-1,"gent_row"],[1,"y"],[0," != goal_row"]],"start1":2385,"start2":2385,"length1":32,"length2":25},{"diffs":[[0,"      next_A"],[-1,"gent_col"],[1,"x"],[0," = 0\n       "]],"start1":2420,"start2":2420,"length1":32,"length2":25},{"diffs":[[0,"      next_A"],[-1,"gent_row"],[1,"y"],[0," = 0\n       "]],"start1":2494,"start2":2494,"length1":32,"length2":25},{"diffs":[[0,"ze[A"],[-1,"gent_row][Agent_col"],[1,"y][Ax"],[0,"] =="]],"start1":2530,"start2":2530,"length1":27,"length2":13},{"diffs":[[0,"= [old_A"],[-1,"gent_col, old_Agent_row"],[1,"x, old_Ay"],[0,"]\n      "]],"start1":2576,"start2":2576,"length1":39,"length2":25},{"diffs":[[0,"m_move(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,")\n      "]],"start1":2760,"start2":2760,"length1":35,"length2":21},{"diffs":[[0,"y_move(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,")\n      "]],"start1":2840,"start2":2840,"length1":35,"length2":21},{"diffs":[[0,"      next_A"],[-1,"gent_col"],[1,"x"],[0," = next_stat"]],"start1":2888,"start2":2888,"length1":32,"length2":25},{"diffs":[[0,"      next_A"],[-1,"gent_row"],[1,"y"],[0," = next_stat"]],"start1":2924,"start2":2924,"length1":32,"length2":25},{"diffs":[[0,"xt_A"],[-1,"gent_col, next_Agent_row"],[1,"x, next_Ay"],[0,")\n  "]],"start1":2988,"start2":2988,"length1":32,"length2":18},{"diffs":[[0,"   old_A"],[-1,"gent_col = Agent_col"],[1,"x = Ax"],[0,"\n       "]],"start1":3128,"start2":3128,"length1":36,"length2":22},{"diffs":[[0,"   old_A"],[-1,"gent_row = Agent_row"],[1,"y = Ay"],[0,"\n       "]],"start1":3197,"start2":3197,"length1":36,"length2":22},{"diffs":[[0,"le[old_A"],[-1,"gent_row][old_Agent_col"],[1,"y][old_Ax"],[0,"]\n      "]],"start1":3271,"start2":3271,"length1":39,"length2":25},{"diffs":[[0," (next_A"],[-1,"gent_col"],[1,"x"],[0," == goal"]],"start1":3304,"start2":3304,"length1":24,"length2":17},{"diffs":[[0,"& next_A"],[-1,"gent_row"],[1,"y"],[0," == goal"]],"start1":3327,"start2":3327,"length1":24,"length2":17},{"diffs":[[0,"xt_A"],[-1,"gent_col, next_Agent_row"],[1,"x, next_Ay"],[0,")\n  "]],"start1":3544,"start2":3544,"length1":32,"length2":18},{"diffs":[[0,"ld_A"],[-1,"gent_row][old_Agent_col"],[1,"y][old_Ax"],[0,"] = "]],"start1":3703,"start2":3703,"length1":31,"length2":17},{"diffs":[[0,"ld_A"],[-1,"gent_col, old_Agent_row, next_Agent_col, next_Agent_row"],[1,"x, old_Ay, next_Ax, next_Ay"],[0,")\n  "]],"start1":3791,"start2":3791,"length1":63,"length2":35},{"diffs":[[0,"tr(old_A"],[-1,"gent_col"],[1,"x"],[0,") + \" \" "]],"start1":3862,"start2":3862,"length1":24,"length2":17},{"diffs":[[0,"tr(old_A"],[-1,"gent_row"],[1,"y"],[0,") + \" to"]],"start1":3882,"start2":3882,"length1":24,"length2":17},{"diffs":[[0,"r(next_A"],[-1,"gent_col"],[1,"x"],[0,") + \" \" "]],"start1":3907,"start2":3907,"length1":24,"length2":17},{"diffs":[[0,"r(next_A"],[-1,"gent_row"],[1,"y"],[0,") + \" Re"]],"start1":3928,"start2":3928,"length1":24,"length2":17},{"diffs":[[0,"           A"],[-1,"gent_col"],[1,"x"],[0," = next_Agen"]],"start1":3969,"start2":3969,"length1":32,"length2":25},{"diffs":[[0," Ax = next_A"],[-1,"gent_col"],[1,"x"],[0,"\n           "]],"start1":3979,"start2":3979,"length1":32,"length2":25},{"diffs":[[0,"           A"],[-1,"gent_row"],[1,"y"],[0," = next_Agen"]],"start1":3994,"start2":3994,"length1":32,"length2":25},{"diffs":[[0," Ay = next_A"],[-1,"gent_row"],[1,"y"],[0,"\n        }\n "]],"start1":4004,"start2":4004,"length1":32,"length2":25},{"diffs":[[0,"andom_move(A"],[-1,"gent_col"],[1,"x"],[0,": number, Ag"]],"start1":4392,"start2":4392,"length1":32,"length2":25},{"diffs":[[0,"x: number, A"],[-1,"gent_row"],[1,"y"],[0,": number): n"]],"start1":4404,"start2":4404,"length1":32,"length2":25},{"diffs":[[0,"oposed_x = A"],[-1,"gent_col"],[1,"x"],[0," + move_x[i]"]],"start1":4613,"start2":4613,"length1":32,"length2":25},{"diffs":[[0,"oposed_y = A"],[-1,"gent_row"],[1,"y"],[0," + move_y[i]"]],"start1":4649,"start2":4649,"length1":32,"length2":25},{"diffs":[[0," + str(A"],[-1,"gent_col"],[1,"x"],[0,") + \"][\""]],"start1":4940,"start2":4940,"length1":24,"length2":17},{"diffs":[[0," + str(A"],[-1,"gent_row"],[1,"y"],[0,") + \"] t"]],"start1":4957,"start2":4957,"length1":24,"length2":17},{"diffs":[[0,"y_move(A"],[-1,"gent_col"],[1,"x"],[0,": number"]],"start1":5194,"start2":5194,"length1":24,"length2":17},{"diffs":[[0,"x: number, A"],[-1,"gent_row"],[1,"y"],[0,": number): n"]],"start1":5202,"start2":5202,"length1":32,"length2":25},{"diffs":[[0,"oposed_A"],[-1,"gent_col"],[1,"x"],[0,": number"]],"start1":5247,"start2":5247,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_row"],[1,"y"],[0,": number"]],"start1":5276,"start2":5276,"length1":24,"length2":17},{"diffs":[[0,"t next_A"],[-1,"gent_col"],[1,"x"],[0," = 0\n   "]],"start1":5343,"start2":5343,"length1":24,"length2":17},{"diffs":[[0,"t next_A"],[-1,"gent_row"],[1,"y"],[0," = 0\n   "]],"start1":5406,"start2":5406,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_col = Agent_col"],[1,"x = Ax"],[0," + move_"]],"start1":5520,"start2":5520,"length1":36,"length2":22},{"diffs":[[0,"oposed_A"],[-1,"gent_row = Agent_row"],[1,"y = Ay"],[0," + move_"]],"start1":5557,"start2":5557,"length1":36,"length2":22},{"diffs":[[0,"oposed_A"],[-1,"gent_col"],[1,"x"],[0," && prop"]],"start1":5603,"start2":5603,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_col"],[1,"x"],[0," <= 4 &&"]],"start1":5618,"start2":5618,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_row"],[1,"y"],[0," && prop"]],"start1":5644,"start2":5644,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_row"],[1,"y"],[0," <= 4)) "]],"start1":5659,"start2":5659,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_row"],[1,"y"],[0,"][propos"]],"start1":5703,"start2":5703,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_col"],[1,"x"],[0,"]\n      "]],"start1":5716,"start2":5716,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_col"],[1,"x"],[0,"\n       "]],"start1":5818,"start2":5818,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_row"],[1,"y"],[0,"\n       "]],"start1":5862,"start2":5862,"length1":24,"length2":17},{"diffs":[[0,"t next_A"],[-1,"gent_col"],[1,"x"],[0,": number"]],"start1":6168,"start2":6168,"length1":24,"length2":17},{"diffs":[[0,"t next_A"],[-1,"gent_row"],[1,"y"],[0,": number"]],"start1":6193,"start2":6193,"length1":24,"length2":17},{"diffs":[[0,"et A"],[-1,"gent_row: number;\n    let Agent_col"],[1,"x"],[0," = A"]],"start1":6241,"start2":6241,"length1":43,"length2":9},{"diffs":[[0,"   let A"],[-1,"Agent_row"],[1,"y"],[0," = Agent"]],"start1":6262,"start2":6262,"length1":25,"length2":17},{"diffs":[[0,"htness(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,", 255)\n "]],"start1":6329,"start2":6329,"length1":35,"length2":21},{"diffs":[[0,"while (A"],[-1,"gent_col"],[1,"x"],[0," != goal"]],"start1":6386,"start2":6386,"length1":24,"length2":17},{"diffs":[[0,"col || A"],[-1,"gent_row"],[1,"y"],[0," != goal"]],"start1":6404,"start2":6404,"length1":24,"length2":17},{"diffs":[[0,"ld_x = A"],[-1,"gent_col"],[1,"x"],[0,"\n       "]],"start1":6438,"start2":6438,"length1":24,"length2":17},{"diffs":[[0,"ld_y = A"],[-1,"Agent_row"],[1,"y"],[0,"\n       "]],"start1":6492,"start2":6492,"length1":25,"length2":17},{"diffs":[[0,"ve(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,")\n  "]],"start1":6542,"start2":6542,"length1":27,"length2":13},{"diffs":[[0,"  next_A"],[-1,"gent_col"],[1,"x"],[0," = next_"]],"start1":6559,"start2":6559,"length1":24,"length2":17},{"diffs":[[0,"  next_A"],[-1,"gent_row"],[1,"y"],[0," = next_"]],"start1":6591,"start2":6591,"length1":24,"length2":17},{"diffs":[[0,"xt_A"],[-1,"gent_col, next_Agent_row"],[1,"x, next_Ay"],[0,")\n  "]],"start1":6647,"start2":6647,"length1":32,"length2":18},{"diffs":[[0,"xt_A"],[-1,"gent_col, next_Agent_row"],[1,"x, next_Ay"],[0,")\n  "]],"start1":6800,"start2":6800,"length1":32,"length2":18},{"diffs":[[0,"       A"],[-1,"gent_col"],[1,"x"],[0," = next_"]],"start1":6836,"start2":6836,"length1":24,"length2":17},{"diffs":[[0,"= next_A"],[-1,"gent_col"],[1,"x"],[0,"\n       "]],"start1":6846,"start2":6846,"length1":24,"length2":17},{"diffs":[[0,"       A"],[-1,"gent_row"],[1,"y"],[0," = next_"]],"start1":6857,"start2":6857,"length1":24,"length2":17},{"diffs":[[0,"= next_A"],[-1,"gent_row"],[1,"y"],[0,"\n       "]],"start1":6867,"start2":6867,"length1":24,"length2":17},{"diffs":[[0,"_value(A"],[-1,"gent_col"],[1,"x"],[0,": number"]],"start1":8159,"start2":8159,"length1":24,"length2":17},{"diffs":[[0,"umber, A"],[-1,"gent_row"],[1,"y"],[0,": number"]],"start1":8171,"start2":8171,"length1":24,"length2":17},{"diffs":[[0,"  nx = A"],[-1,"gent_col"],[1,"x"],[0," + move_"]],"start1":8321,"start2":8321,"length1":24,"length2":17},{"diffs":[[0,"  ny = A"],[-1,"gent_row"],[1,"y"],[0," + move_"]],"start1":8349,"start2":8349,"length1":24,"length2":17}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"l] = 10 "],[-1,"  "],[0,"#set the"]],"start1":1161,"start2":1161,"length1":18,"length2":16},{"diffs":[[0,"Agent_col, A"],[-1,"gent_row"],[1,"y"],[0,", 255)\n     "]],"start1":1751,"start2":1751,"length1":32,"length2":25},{"diffs":[[0,"t: \" + str(A"],[-1,"gent_col"],[1,"x"],[0,") + \" \" + st"]],"start1":1858,"start2":1858,"length1":32,"length2":25},{"diffs":[[0," \" \" + str(A"],[-1,"gent_row"],[1,"y"],[0,") + \" Goal: "]],"start1":1874,"start2":1874,"length1":32,"length2":25},{"diffs":[[0,"    while (A"],[-1,"gent_col"],[1,"x"],[0," != goal_col"]],"start1":1949,"start2":1949,"length1":32,"length2":25},{"diffs":[[0,"oal_col or A"],[-1,"gent_row"],[1,"y"],[0," != goal_row"]],"start1":1967,"start2":1967,"length1":32,"length2":25},{"diffs":[[0,"      next_A"],[-1,"gent_col"],[1,"x"],[0," = 0  #initi"]],"start1":2001,"start2":2001,"length1":32,"length2":25},{"diffs":[[0,"      next_A"],[-1,"gent_row"],[1,"y"],[0," = 0\n       "]],"start1":2062,"start2":2062,"length1":32,"length2":25},{"diffs":[[0,"ze[A"],[-1,"gent_row][Agent_col"],[1,"y][Ax"],[0,"] =="]],"start1":2110,"start2":2110,"length1":27,"length2":13},{"diffs":[[0,"= [old_A"],[-1,"gent_col, old_Agent_row"],[1,"x, old_Ay"],[0,"]  #if h"]],"start1":2154,"start2":2154,"length1":39,"length2":25},{"diffs":[[0,"m_move(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,")\n      "]],"start1":2313,"start2":2313,"length1":35,"length2":21},{"diffs":[[0,"y_move(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,")\n      "]],"start1":2390,"start2":2390,"length1":35,"length2":21},{"diffs":[[0,"      next_A"],[-1,"gent_col"],[1,"x"],[0," = next_stat"]],"start1":2411,"start2":2411,"length1":32,"length2":25},{"diffs":[[0,"      next_A"],[-1,"gent_row"],[1,"y"],[0," = next_stat"]],"start1":2447,"start2":2447,"length1":32,"length2":25},{"diffs":[[0,"xt_A"],[-1,"gent_col, next_Agent_row"],[1,"x, next_Ay"],[0,") #r"]],"start1":2524,"start2":2524,"length1":32,"length2":18},{"diffs":[[0,"   old_A"],[-1,"gent_col = Agent_col"],[1,"x = Ax"],[0,"  # Stor"]],"start1":2639,"start2":2639,"length1":36,"length2":22},{"diffs":[[0,"   old_A"],[-1,"gent_row = Agent_row"],[1,"y = Ay"],[0,"\n\n      "]],"start1":2695,"start2":2695,"length1":36,"length2":22},{"diffs":[[0,"le[old_A"],[-1,"gent_row][old_Agent_col"],[1,"y][old_Ax"],[0,"]\n      "]],"start1":2768,"start2":2768,"length1":39,"length2":25},{"diffs":[[0,"f next_A"],[-1,"gent_col"],[1,"x"],[0," == goal"]],"start1":2800,"start2":2800,"length1":24,"length2":17},{"diffs":[[0,"d next_A"],[-1,"gent_row"],[1,"y"],[0," == goal"]],"start1":2824,"start2":2824,"length1":24,"length2":17},{"diffs":[[0,"xt_A"],[-1,"gent_col, next_Agent_row"],[1,"x, next_Ay"],[0,")\n  "]],"start1":3019,"start2":3019,"length1":32,"length2":18},{"diffs":[[0,"ld_A"],[-1,"gent_row][old_Agent_col"],[1,"y][old_Ax"],[0,"] = "]],"start1":3151,"start2":3151,"length1":31,"length2":17},{"diffs":[[0,"ld_A"],[-1,"gent_col, old_Agent_row, next_Agent_col, next_Agent_row"],[1,"x, old_Ay, next_Ax, next_Ay"],[0,")\n  "]],"start1":3240,"start2":3240,"length1":63,"length2":35},{"diffs":[[0,"tr(old_A"],[-1,"gent_col"],[1,"x"],[0,") + \" \" "]],"start1":3309,"start2":3309,"length1":24,"length2":17},{"diffs":[[0,"tr(old_A"],[-1,"gent_row"],[1,"y"],[0,") + \" to"]],"start1":3329,"start2":3329,"length1":24,"length2":17},{"diffs":[[0,"r(next_A"],[-1,"gent_col"],[1,"x"],[0,") + \" \" "]],"start1":3354,"start2":3354,"length1":24,"length2":17},{"diffs":[[0,"r(next_A"],[-1,"gent_row"],[1,"y"],[0,") + \" Re"]],"start1":3375,"start2":3375,"length1":24,"length2":17},{"diffs":[[0,"           A"],[-1,"gent_col"],[1,"x"],[0," = next_Agen"]],"start1":3429,"start2":3429,"length1":32,"length2":25},{"diffs":[[0," Ax = next_A"],[-1,"gent_col"],[1,"x"],[0,"\n           "]],"start1":3439,"start2":3439,"length1":32,"length2":25},{"diffs":[[0,"           A"],[-1,"gent_row"],[1,"y"],[0," = next_Agen"]],"start1":3454,"start2":3454,"length1":32,"length2":25},{"diffs":[[0,"= next_A"],[-1,"gent_row"],[1,"y"],[0,"\n\n      "]],"start1":3468,"start2":3468,"length1":24,"length2":17},{"diffs":[[0,"m_move(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,"):\n    a"]],"start1":3867,"start2":3867,"length1":35,"length2":21},{"diffs":[[0,"oposed_x = A"],[-1,"gent_col"],[1,"x"],[0," + move_x[i]"]],"start1":3985,"start2":3985,"length1":32,"length2":25},{"diffs":[[0,"oposed_y = A"],[-1,"gent_row"],[1,"y"],[0," + move_y[i]"]],"start1":4021,"start2":4021,"length1":32,"length2":25},{"diffs":[[0," + str(A"],[-1,"gent_col"],[1,"x"],[0,") + \"][\""]],"start1":4261,"start2":4261,"length1":24,"length2":17},{"diffs":[[0," + str(A"],[-1,"gent_row"],[1,"y"],[0,") + \"] t"]],"start1":4278,"start2":4278,"length1":24,"length2":17},{"diffs":[[0,"y_move(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,"):\n    b"]],"start1":4502,"start2":4502,"length1":35,"length2":21},{"diffs":[[0,"  next_A"],[-1,"gent_col"],[1,"x"],[0," = 0  # "]],"start1":4539,"start2":4539,"length1":24,"length2":17},{"diffs":[[0,"  next_A"],[-1,"gent_row"],[1,"y"],[0," = 0\n   "]],"start1":4593,"start2":4593,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_col = Agent_col"],[1,"x = Ax"],[0," + move_"]],"start1":4687,"start2":4687,"length1":36,"length2":22},{"diffs":[[0,"oposed_A"],[-1,"gent_row = Agent_row"],[1,"y = Ay"],[0," + move_"]],"start1":4724,"start2":4724,"length1":36,"length2":22},{"diffs":[[0,"oposed_A"],[-1,"gent_col"],[1,"x"],[0," <= 4 an"]],"start1":4769,"start2":4769,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_row"],[1,"y"],[0," <= 4:\n "]],"start1":4795,"start2":4795,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_row"],[1,"y"],[0,"][propos"]],"start1":4836,"start2":4836,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_col"],[1,"x"],[0,"]\n      "]],"start1":4849,"start2":4849,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_col"],[1,"x"],[0,"\n       "]],"start1":4948,"start2":4948,"length1":24,"length2":17},{"diffs":[[0,"oposed_A"],[-1,"gent_row"],[1,"y"],[0,"\n       "]],"start1":4992,"start2":4992,"length1":24,"length2":17},{"diffs":[[0,"d_b():\n    A"],[-1,"gent_col"],[1,"x"],[0," = Agent_row"]],"start1":5117,"start2":5117,"length1":32,"length2":25},{"diffs":[[0,"_0\n    A"],[-1,"Agent_row"],[1,"y"],[0," = Agent"]],"start1":5142,"start2":5142,"length1":25,"length2":17},{"diffs":[[0,"htness(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,", 255)\n "]],"start1":5206,"start2":5206,"length1":35,"length2":21},{"diffs":[[0,"while (A"],[-1,"gent_col"],[1,"x"],[0," != goal"]],"start1":5259,"start2":5259,"length1":24,"length2":17},{"diffs":[[0,"col or A"],[-1,"gent_row"],[1,"y"],[0," != goal"]],"start1":5277,"start2":5277,"length1":24,"length2":17},{"diffs":[[0,"ld_x = A"],[-1,"gent_col"],[1,"x "],[0," # Store"]],"start1":5310,"start2":5310,"length1":24,"length2":18},{"diffs":[[0,"ld_y = A"],[-1,"Agent_row"],[1,"y"],[0,"\n       "]],"start1":5355,"start2":5355,"length1":25,"length2":17},{"diffs":[[0,"y_move(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,")\n      "]],"start1":5401,"start2":5401,"length1":35,"length2":21},{"diffs":[[0,"  next_A"],[-1,"gent_col"],[1,"x"],[0," = next_"]],"start1":5422,"start2":5422,"length1":24,"length2":17},{"diffs":[[0,"  next_A"],[-1,"gent_row"],[1,"y"],[0," = next_"]],"start1":5454,"start2":5454,"length1":24,"length2":17},{"diffs":[[0,"xt_A"],[-1,"gent_col, next_Agent_row"],[1,"x, next_Ay"],[0,")\n  "]],"start1":5510,"start2":5510,"length1":32,"length2":18},{"diffs":[[0,"xt_A"],[-1,"gent_col, next_Agent_row"],[1,"x, next_Ay"],[0,")\n  "]],"start1":5654,"start2":5654,"length1":32,"length2":18},{"diffs":[[0,"       A"],[-1,"gent_col"],[1,"x"],[0," = next_"]],"start1":5690,"start2":5690,"length1":24,"length2":17},{"diffs":[[0,"= next_A"],[-1,"gent_col"],[1,"x"],[0,"\n       "]],"start1":5700,"start2":5700,"length1":24,"length2":17},{"diffs":[[0,"       A"],[-1,"gent_row"],[1,"y"],[0," = next_"]],"start1":5711,"start2":5711,"length1":24,"length2":17},{"diffs":[[0,"= next_A"],[-1,"gent_row"],[1,"y"],[0,"\n       "]],"start1":5721,"start2":5721,"length1":24,"length2":17},{"diffs":[[0,"ue(A"],[-1,"gent_col, Agent_row"],[1,"x, Ay"],[0,"):\n "]],"start1":6863,"start2":6863,"length1":27,"length2":13},{"diffs":[[0,"  nx = A"],[-1,"gent_col"],[1,"x"],[0," + move_"]],"start1":6922,"start2":6922,"length1":24,"length2":17},{"diffs":[[0,"  ny = A"],[-1,"gent_row"],[1,"y"],[0," + move_"]],"start1":6950,"start2":6950,"length1":24,"length2":17}]}]},{"timestamp":1740975995767,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"et maze = [["],[-1,"0"],[1,"1"],[0,", 0, 0, 0, 0"]],"start1":548,"start2":548,"length1":25,"length2":25},{"diffs":[[0," 0, "],[-1,"0"],[1,"1"],[0,"], [1, "],[-1,"1"],[1,"0"],[0,", 1, "],[-1,"1, 0"],[1,"0, 1"],[0,"], ["]],"start1":568,"start2":568,"length1":26,"length2":26},{"diffs":[[0," 0, "],[-1,"0"],[1,"1"],[0,", 1, "],[-1,"0"],[1,"1"],[0,"], ["],[-1,"0, 1"],[1,"1, 0"],[0,", 1, "],[-1,"1"],[1,"0"],[0,", 0], ["],[-1,"0"],[1,"1"],[0,", 0,"]],"start1":596,"start2":596,"length1":37,"length2":37},{"diffs":[[0,", 0, 0, "],[-1,"0"],[1,"1"],[0,"]]\n// Go"]],"start1":632,"start2":632,"length1":17,"length2":17},{"diffs":[[0,"l_row = "],[-1,"0"],[1,"3"],[0,"\n//  row"]],"start1":668,"start2":668,"length1":17,"length2":17},{"diffs":[[0,"l_col = "],[-1,"0"],[1,"4"],[0,"\n//  col"]],"start1":720,"start2":720,"length1":17,"length2":17},{"diffs":[[0,"ent_col_0 = "],[-1,"2"],[1,"0"],[0,"\n//  column "]],"start1":866,"start2":866,"length1":25,"length2":25},{"diffs":[[0,"reward: number;\n"],[1,"    let Agent_row: number;\n"],[0,"    let Agent_co"]],"start1":6691,"start2":6691,"length1":32,"length2":59},{"diffs":[[0,"col = Agent_"],[-1,"col"],[1,"row"],[0,"_0\n    let A"]],"start1":6748,"start2":6748,"length1":27,"length2":27},{"diffs":[[0,"t_row_0\n    let "],[1,"A"],[0,"Agent_row = Agen"]],"start1":6758,"start2":6758,"length1":32,"length2":33},{"diffs":[[0,"ent_row = Agent_"],[-1,"row"],[1,"col"],[0,"_0\n    let episo"]],"start1":6777,"start2":6777,"length1":35,"length2":35},{"diffs":[[0,"    old_y = "],[1,"A"],[0,"Agent_row\n  "]],"start1":7035,"start2":7035,"length1":24,"length2":25}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,")\nmaze = [\n    ["],[-1,"0"],[1,"1"],[0,", 0, 0, 0, 0],\n "]],"start1":552,"start2":552,"length1":33,"length2":33},{"diffs":[[0,"   [1, 0, 0, 0, "],[-1,"0"],[1,"1"],[0,"],\n    [1, 1, 1,"]],"start1":564,"start2":564,"length1":33,"length2":33},{"diffs":[[0,"    [1, "],[-1,"1"],[1,"0"],[0,", 1, "],[-1,"1, 0"],[1,"0, 1"],[0,"],\n    ["]],"start1":584,"start2":584,"length1":26,"length2":26},{"diffs":[[0," 0, "],[-1,"0"],[1,"1"],[0,", 1, "],[-1,"0"],[1,"1"],[0,"],\n    ["],[1,"1, "],[0,"0, 1, "],[-1,"1, 1"],[1,"0"],[0,", 0]"]],"start1":612,"start2":612,"length1":33,"length2":33},{"diffs":[[0,", 0], \n    ["],[-1,"0"],[1,"1"],[0,", 0, 0, 0, 0"]],"start1":641,"start2":641,"length1":25,"length2":25},{"diffs":[[0,"1, 0, 0, 0, "],[-1,"0"],[1,"1"],[0,"]\n]\n\n#Goal p"]],"start1":653,"start2":653,"length1":25,"length2":25},{"diffs":[[0,"l_row = "],[-1,"0"],[1,"3"],[0,"  # row "]],"start1":689,"start2":689,"length1":17,"length2":17},{"diffs":[[0,"\ngoal_col = "],[-1,"0"],[1,"4"],[0,"  # column o"]],"start1":732,"start2":732,"length1":25,"length2":25},{"diffs":[[0,"ent_col_0 = "],[-1,"2"],[1,"0"],[0,"  # column o"]],"start1":871,"start2":871,"length1":25,"length2":25},{"diffs":[[0,"_state\n\n"],[1,"\n\n"],[0,"# On but"]],"start1":5495,"start2":5495,"length1":16,"length2":18},{"diffs":[[0,"= Agent_"],[-1,"col"],[1,"row"],[0,"_0\n    "],[1,"A"],[0,"Agent_ro"]],"start1":5581,"start2":5581,"length1":26,"length2":27},{"diffs":[[0,"row = Agent_"],[-1,"row"],[1,"col"],[0,"_0\n    episo"]],"start1":5606,"start2":5606,"length1":27,"length2":27},{"diffs":[[0,"    old_y = "],[1,"A"],[0,"Agent_row\n  "]],"start1":5842,"start2":5842,"length1":24,"length2":25}]}]},{"timestamp":1740976424549,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," = [[0, "],[-1,"1"],[1,"0"],[0,", 0, 0, "]],"start1":555,"start2":555,"length1":17,"length2":17},{"diffs":[[0,", 0, 0, 0, 0], ["],[-1,"0"],[1,"1"],[0,", 1, 1, 1, 0], ["]],"start1":561,"start2":561,"length1":33,"length2":33},{"diffs":[[0,", 0], [0, 0, 0, "],[-1,"0"],[1,"1"],[0,", 0], [0, 1, 1, "]],"start1":587,"start2":587,"length1":33,"length2":33},{"diffs":[[0,", [0, 0, 0, "],[-1,"1"],[1,"0"],[0,", 0]]\n// Goa"]],"start1":625,"start2":625,"length1":25,"length2":25},{"diffs":[[0,"l_row = "],[-1,"4"],[1,"0"],[0,"\n//  row"]],"start1":668,"start2":668,"length1":17,"length2":17},{"diffs":[[0,"l_col = "],[-1,"4"],[1,"0"],[0,"\n//  col"]],"start1":720,"start2":720,"length1":17,"length2":17},{"diffs":[[0,"ent_row_0 = "],[-1,"0"],[1,"2"],[0,"\n//  row of "]],"start1":801,"start2":801,"length1":25,"length2":25},{"diffs":[[0,"col_0 = "],[-1,"0"],[1,"2"],[0,"\n//  col"]],"start1":870,"start2":870,"length1":17,"length2":17}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"    [0, "],[-1,"1"],[1,"0"],[0,", 0, 0, "]],"start1":563,"start2":563,"length1":17,"length2":17},{"diffs":[[0," 0, 0, 0],\n    ["],[-1,"0"],[1,"1"],[0,", 1, 1, 1, 0],\n "]],"start1":573,"start2":573,"length1":33,"length2":33},{"diffs":[[0,",\n    [0, 0, 0, "],[-1,"0"],[1,"1"],[0,", 0],\n    [0, 1,"]],"start1":603,"start2":603,"length1":33,"length2":33},{"diffs":[[0,"  [0, 0, 0, "],[-1,"1"],[1,"0"],[0,", 0]\n]\n\n#Goa"]],"start1":650,"start2":650,"length1":25,"length2":25},{"diffs":[[0,"l_row = "],[-1,"4"],[1,"0"],[0,"  # row "]],"start1":689,"start2":689,"length1":17,"length2":17},{"diffs":[[0,"l_col = "],[-1,"4"],[1,"0"],[0,"  # colu"]],"start1":736,"start2":736,"length1":17,"length2":17},{"diffs":[[0,"ent_row_0 = "],[-1,"0"],[1,"2"],[0,"  # row of t"]],"start1":811,"start2":811,"length1":25,"length2":25},{"diffs":[[0,"col_0 = "],[-1,"0"],[1,"2"],[0,"  # colu"]],"start1":875,"start2":875,"length1":17,"length2":17}]}]},{"timestamp":1740976670320,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," 1, 0],\n    [0, "],[-1,"1"],[1,"0"],[0,", 0, 0, 0],\n    "]],"start1":597,"start2":597,"length1":33,"length2":33}]}]},{"timestamp":1740977847036,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," 1, 0], [0, "],[-1,"1"],[1,"0"],[0,", 0, 0, 0], "]],"start1":585,"start2":585,"length1":25,"length2":25}]}]},{"timestamp":1740977847866,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"gamma = "],[-1,"1"],[1,".9"],[0,"\n//  Dis"]],"start1":256,"start2":256,"length1":17,"length2":18},{"diffs":[[0,", 0, 0], [0, 1, "],[-1,"0"],[1,"1"],[0,", 1, 0], [0, 0, "]],"start1":567,"start2":567,"length1":33,"length2":33},{"diffs":[[0,", 1, 1, 0], [0, "],[-1,"0"],[1,"1"],[0,", 0, 0, 0], [0, "]],"start1":581,"start2":581,"length1":33,"length2":33},{"diffs":[[0," [0, 1, "],[-1,"0"],[1,"1"],[0,", 1, 0],"]],"start1":609,"start2":609,"length1":17,"length2":17},{"diffs":[[0,"next_Agent_row)\n"],[1,"            // print(\"Move from: \" + str(old_Agent_col) + \" \" + str(old_Agent_row) + \" to: \" + str(next_Agent_col) + \" \" + str(next_Agent_row) + \" Reward: \" + str(reward))\n"],[0,"            Agen"]],"start1":4041,"start2":4041,"length1":32,"length2":204},{"diffs":[[0,"in)\n"],[-1,"        //  decay epsilon for next episode\n"],[0,"    "]],"start1":4378,"start2":4378,"length1":51,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"gamma = "],[-1,"1"],[1,".9"],[0,"  # Disc"]],"start1":279,"start2":279,"length1":17,"length2":18},{"diffs":[[0,"ration\n\n"],[1,"\n"],[0,"# Define"]],"start1":471,"start2":471,"length1":16,"length2":17},{"diffs":[[0," 0],\n    [0, 1, "],[-1,"0"],[1,"1"],[0,", 1, 0],\n    [0,"]],"start1":579,"start2":579,"length1":33,"length2":33},{"diffs":[[0," 1, 0],\n    [0, "],[-1,"0"],[1,"1"],[0,", 0, 0, 0],\n    "]],"start1":597,"start2":597,"length1":33,"length2":33},{"diffs":[[0," [0, 1, "],[-1,"0"],[1,"1"],[0,", 1, 0],"]],"start1":629,"start2":629,"length1":17,"length2":17},{"diffs":[[0,") / 100\n"],[1,"\n"],[0,"        "]],"start1":3415,"start2":3415,"length1":16,"length2":17},{"diffs":[[0,"ent_row)"],[1,"\n            #print(\"Move from: \" + str(old_Agent_col) + \" \" + str(old_Agent_row) + \" to: \" + str(next_Agent_col) + \" \" + str(next_Agent_row) + \" Reward: \" + str(reward))\n    "],[0,"        "]],"start1":3511,"start2":3511,"length1":16,"length2":191},{"diffs":[[0,"min)"],[-1,"  # decay epsilon for next episode"],[0,"\n   "]],"start1":3837,"start2":3837,"length1":42,"length2":8}]}]},{"timestamp":1740978392054,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"ion\n"],[-1,"//  Settings for showing Qtables during Training\nlet show_Qtable_after_each_STEP = true\n// start with this True to see how the Qvalues are updated for each step\nlet show_Qtable_after_each_EPISODE = false\nlet show_Qtable_after_TRAINING = false\n"],[0,"//  "]],"start1":466,"start2":466,"length1":251,"length2":8},{"diffs":[[0,"100\n"],[-1,"            if (show_Qtable_after_each_STEP == true) {\n                console.log(\"Qtable:\")\n                show_Qtable()\n            }\n            \n"],[0,"    "]],"start1":3956,"start2":3956,"length1":159,"length2":8},{"diffs":[[0,"t_row\n        }\n"],[1,"        epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n        //  decay epsilon for next episode\n"],[0,"        console."]],"start1":4128,"start2":4128,"length1":32,"length2":140},{"diffs":[[0,"d))\n        "],[-1,"if ("],[0,"show_Qtable_"]],"start1":4404,"start2":4404,"length1":28,"length2":24},{"diffs":[[0,"able"],[-1,"_after_each_EPISODE == true) {\n            console.log(\"Qtable:\")\n            show_Qtable()\n        }\n        \n        epsilon = Math.max(epsilon * epsilon_decay, epsilon_min)\n    }\n    //  decay epsilon for next episode\n    console.log(\"Training Complete\")\n    if (show_Qtable_after_TRAINING == true) {\n        console.log(\"Qtable:\")\n        show_Qtable()\n    }\n    "],[1,"()\n    }\n    console.log(\"Training Complete\")"],[0,"\n})\n"]],"start1":4423,"start2":4423,"length1":375,"length2":53}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"on\n\n"],[-1,"# Settings for showing Qtables during Training\nshow_Qtable_after_each_STEP = True    #start with this True to see how the Qvalues are updated for each step\nshow_Qtable_after_each_EPISODE = False\nshow_Qtable_after_TRAINING = False\n\n"],[0,"# De"]],"start1":474,"start2":474,"length1":239,"length2":8},{"diffs":[[0,"100\n"],[-1,"            if show_Qtable_after_each_STEP == True:\n                print(\"Qtable:\")\n                show_Qtable()\n"],[0,"    "]],"start1":3417,"start2":3417,"length1":123,"length2":8},{"diffs":[[0,"next_Agent_row\n\n"],[1,"        epsilon = max(epsilon * epsilon_decay, epsilon_min)  # decay epsilon for next episode\n"],[0,"        print(\"E"]],"start1":3588,"start2":3588,"length1":32,"length2":126},{"diffs":[[0,"d))\n        "],[-1,"if "],[0,"show_Qtable_"]],"start1":3836,"start2":3836,"length1":27,"length2":24},{"diffs":[[0,"able"],[-1,"_after_each_EPISODE == True:\n            print(\"Qtable:\")\n            show_Qtable()\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)  # decay epsilon for next episode\n        \n    print(\"Training Complete\")\n    if show_Qtable_after_TRAINING == True:\n            print(\"Qtable:\")\n            show_Qtable()"],[1,"()\n    print(\"Training Complete\")\n    "],[0,"\ninp"]],"start1":3855,"start2":3855,"length1":323,"length2":46}]}]},{"timestamp":1740979018460,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"e()\n"],[-1,"                \n"],[0,"    "]],"start1":3763,"start2":3763,"length1":25,"length2":8}]}]},{"timestamp":1740979019784,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0," [0, 1, "],[-1,"1"],[1,"0"],[0,", 1, 0],"]],"start1":817,"start2":817,"length1":17,"length2":17},{"diffs":[[0," 1, 0], [0, "],[-1,"1"],[1,"0"],[0,", 0, 0, 0], "]],"start1":827,"start2":827,"length1":25,"length2":25},{"diffs":[[0,"e()\n"],[-1,"                pause(2000)\n"],[0,"    "]],"start1":4323,"start2":4323,"length1":36,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," [0, 1, "],[-1,"1"],[1,"0"],[0,", 1, 0],"]],"start1":816,"start2":816,"length1":17,"length2":17},{"diffs":[[0," 1, 0],\n    [0, "],[-1,"1"],[1,"0"],[0,", 0, 0, 0],\n    "]],"start1":826,"start2":826,"length1":33,"length2":33},{"diffs":[[0,"    "],[-1,"pause(2000)"],[0,"\n   "]],"start1":3779,"start2":3779,"length1":19,"length2":8}]}]},{"timestamp":1740979171884,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,") {\n"],[-1,"    if (show_Qtable_after_each_STEP == true) {\n        console.log(\"Qtable:\")\n        show_Qtable()\n    }\n    \n"],[0,"    "]],"start1":11553,"start2":11553,"length1":119,"length2":8}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"():\n"],[-1,"    if show_Qtable_after_each_STEP == True:\n        global is_paused = False          \n    else:\n    "],[0,"    "]],"start1":9654,"start2":9654,"length1":109,"length2":8},{"diffs":[[0,"g_speed\n"],[-1,"    "],[0,"    trai"]],"start1":9676,"start2":9676,"length1":20,"length2":16}]}]},{"timestamp":1740979770027,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"lse\n"],[-1,"let is_paused = false\n"],[0,"//  "]],"start1":709,"start2":709,"length1":30,"length2":8},{"diffs":[[0,"            "],[-1,"show_"],[1,"console.log(\""],[0,"Qtable"],[-1,"("],[1,":\""],[0,")\n          "]],"start1":4262,"start2":4262,"length1":36,"length2":45},{"diffs":[[0,"    "],[-1,"\n                is_paused = true"],[1,"show_Qtable()"],[0,"\n   "]],"start1":4309,"start2":4309,"length1":41,"length2":21},{"diffs":[[0,"    "],[-1,"    }\n"],[0,"        "],[-1,"    \n            while (is_paused == true) {\n                pause(1000)"],[1,"pause(2000)\n            }"],[0,"\n   "]],"start1":4331,"start2":4331,"length1":94,"length2":41},{"diffs":[[0,"  }\n            "],[-1,"}"],[0,"\n            upd"]],"start1":4365,"start2":4365,"length1":33,"length2":32},{"diffs":[[0,"SODE == true) {\n"],[1,"            console.log(\"Qtable:\")\n"],[0,"            show"]],"start1":4760,"start2":4760,"length1":32,"length2":67},{"diffs":[[0,"NING == true) {\n"],[1,"        console.log(\"Qtable:\")\n"],[0,"        show_Qta"]],"start1":5033,"start2":5033,"length1":32,"length2":63},{"diffs":[[0,"    "],[-1,"\n        is_paused = false\n    } else {"],[1,"console.log(\"Qtable:\")\n        show_Qtable()\n    }"],[0,"\n    "],[-1,"    "],[0,"\n    "],[1,"\n"],[0,"    "]],"start1":11608,"start2":11608,"length1":61,"length2":69},{"diffs":[[0,"eed * 2\n"],[-1,"    }\n    \n"],[0,"})\n"]],"start1":11705,"start2":11705,"length1":22,"length2":11}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"alse"],[-1,"\nis_paused = False"],[0,"\n\n# "]],"start1":703,"start2":703,"length1":26,"length2":8},{"diffs":[[0,"            "],[-1,"show_"],[1,"print(\""],[0,"Qtable"],[-1,"("],[1,":\""],[0,")\n          "]],"start1":3708,"start2":3708,"length1":36,"length2":39},{"diffs":[[0,"    "],[-1,"global is_paused\n                is_paused = True\n            while is_paused == True:"],[1,"show_Qtable()"],[0,"\n   "]],"start1":3749,"start2":3749,"length1":94,"length2":21},{"diffs":[[0,"      pause("],[-1,"1"],[1,"2"],[0,"000)\n       "]],"start1":3777,"start2":3777,"length1":25,"length2":25},{"diffs":[[0,"PISODE == True:\n"],[1,"            print(\"Qtable:\")\n"],[0,"            show"]],"start1":4155,"start2":4155,"length1":32,"length2":61},{"diffs":[[0,"AINING == True:\n"],[1,"            print(\"Qtable:\")\n"],[0,"            show"]],"start1":4387,"start2":4387,"length1":32,"length2":61},{"diffs":[[0,"used"],[-1,"\n        is_paused"],[0," = F"]],"start1":9722,"start2":9722,"length1":26,"length2":8}]}]},{"timestamp":1740980109212,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"_STEP = "],[-1,"fals"],[1,"tru"],[0,"e\n// sta"]],"start1":545,"start2":545,"length1":20,"length2":19},{"diffs":[[0,"ISODE = "],[-1,"tru"],[1,"fals"],[0,"e\nlet sh"]],"start1":660,"start2":660,"length1":19,"length2":20}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"_STEP = "],[-1,"False"],[1,"True "],[0,"   #star"]],"start1":547,"start2":547,"length1":21,"length2":21},{"diffs":[[0,"ISODE = "],[-1,"Tru"],[1,"Fals"],[0,"e\nshow_Q"]],"start1":659,"start2":659,"length1":19,"length2":20}]}]},{"timestamp":1740980495951,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"pha = 0."],[1,"3"],[0,"  # Lear"]],"start1":252,"start2":252,"length1":16,"length2":17}]}]},{"timestamp":1740981285345,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"pha = 0."],[1,"3"],[0,"\n//  Lea"]],"start1":224,"start2":224,"length1":16,"length2":17}]}]},{"timestamp":1740981285486,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.ts","patch":[{"diffs":[[0,"sodes = "],[-1,"5"],[1,"2"],[0,"0\n// set"]],"start1":142,"start2":142,"length1":17,"length2":17},{"diffs":[[0,"pha = 0."],[-1,"4"],[0,"\n//  Lea"]],"start1":224,"start2":224,"length1":17,"length2":16},{"diffs":[[0,"gamma = "],[-1,".9"],[1,"1"],[0,"\n//  Dis"]],"start1":255,"start2":255,"length1":18,"length2":17}]},{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"sodes = "],[-1,"5"],[1,"2"],[0,"0    #se"]],"start1":173,"start2":173,"length1":17,"length2":17},{"diffs":[[0,"pha = 0."],[-1,"4"],[0,"  # Lear"]],"start1":252,"start2":252,"length1":17,"length2":16},{"diffs":[[0,"gamma = "],[-1,".9"],[1,"1"],[0,"  # Disc"]],"start1":278,"start2":278,"length1":18,"length2":17}]}]},{"timestamp":1740981376044,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"E = True"],[-1,"   "],[0,"\nshow_Qt"]],"start1":664,"start2":664,"length1":19,"length2":16}]}]},{"timestamp":1741050078571,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," [\n    ["],[-1,"S"],[1,"0"],[0,", 1, 0, "]],"start1":810,"start2":810,"length1":17,"length2":17}]}]},{"timestamp":1741065217106,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0," [\n    ["],[-1,"\""],[0,"S, 1, 0,"]],"start1":810,"start2":810,"length1":17,"length2":16}]}]},{"timestamp":1741065349558,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.py","patch":[{"diffs":[[0,"ze = [\n    ["],[-1,"0"],[1,"\"S"],[0,", 1, 0, 0, 0"]],"start1":806,"start2":806,"length1":25,"length2":26}]}]},{"timestamp":1741065406000,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"main.blocks","patch":[{"diffs":[[0,"iables><"],[1,"block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement></block><"],[0,"/xml>"]],"start1":124,"start2":124,"length1":13,"length2":3928}]}]},{"timestamp":1741067490021,"editorVersion":"7.0.57","changes":[{"type":"edited","filename":"pxt.json","patch":[{"diffs":[[0," ],\n"],[-1,"    \"testFiles\": [\n        \"test.ts\"\n    ],\n"],[0,"    "]],"start1":252,"start2":252,"length1":52,"length2":8}]},{"type":"added","filename":"test.ts","value":"// tests go here; this will not be compiled when this package is used as an extension.\n"}]}],"snapshots":[{"timestamp":1740253156123,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"http://www.w3.org/1999/xhtml\">\n  <block type=\"pxt-on-start\"></block>\n  <block type=\"device_forever\"></block>\n</xml>","main.ts":"\n","README.md":"","pxt.json":"{\n    \"name\": \"MazeRunner\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\"\n    ],\n    \"additionalFilePaths\": []\n}\n"}},{"timestamp":1740266945227,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"20\" y=\"20\"></block><block type=\"device_forever\" x=\"225\" y=\"20\"></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\nimport Math  # Use Micro:bit's Math module for randomness\n\n# Maze representation (1 = wall, 0 = path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [0, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\ndef display_maze():\n    basic.clear_screen()  # Clear previous display\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:  # If it's a wall, turn on the LED\n                led.plot_brightness(x, y, 10)\n\n# Display the maze\ndisplay_maze()\n\n# Player start position\nplayer_x, player_y = 1, 1\ngoal_x, goal_y = 3, 4\n\n# Learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 0.3  # Exploration chance\n\n# Possible moves (Up, Down, Left, Right)\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Function to choose an action\ndef choose_action():\n    if Math.random() < epsilon:  # Random move for exploration\n        action_index = int(Math.random() * 4)  # Pick a random index between 0-3\n        return action_index\n    else:\n        return 0  # Placeholder for now, can be improved with learned Q-values\n\n# Training loop\nfor episode in range(10):  # Run multiple learning episodes\n    player_x, player_y = 1, 1  # Reset position\n    while (player_x, player_y) != (goal_x, goal_y):  # Until goal is reached\n        action = choose_action()\n        new_x = player_x + move_x[action]\n        new_y = player_y + move_y[action]\n\n        # Check if the move is valid\n        if maze[new_y][new_x] == 1:  # Hit a wall\n            reward = -5\n        elif (new_x, new_y) == (goal_x, goal_y):  # Reached goal\n            reward = 10\n        else:\n            reward = -1  # Normal movement\n\n        # Move the player only if it's a valid move\n        if maze[new_y][new_x] == 0:\n            player_x, player_y = new_x, new_y\n\n        # Update display to show movement\n        basic.clear_screen()  # Clear previous display\n        led.plot(player_x, player_y)  # Show player position\n        basic.pause(500)  # Wait for a short time\n\n# Display success message when done\nbasic.show_icon(IconNames.HAPPY)","pxt.json":"{\n    \"name\": \"MazeRunner\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1740343063784,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"20\" y=\"20\"></block><block type=\"device_forever\" x=\"225\" y=\"20\"></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\nimport random\n\n# 5x5 Q-table initialized to 0\nQ_table = {}\nfor y in range(5):\n    for x in range(5):\n        Q_table[str(x) + \",\" + str(y)] = [0, 0, 0, 0]  # [Up, Down, Left, Right]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Q-learning parameters\nalpha = 0.1   # Learning rate\ngamma = 0.9   # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Epsilon decay after each episode\n\ngoal_x, goal_y = 3, 4  # Goal position\n\n# Run multiple training episodes\nfor episode in range(100):\n    robot_x, robot_y = 1, 1  # Reset each episode\n\n    while (robot_x, robot_y) != (goal_x, goal_y):\n        # Exploration or Exploitation\n        if random.random() < epsilon:\n            action_index = random.randint(0, 3)  # explore\n        else:\n            # exploit (choose best action manually)\n            q_values = Q_table[str(robot_x) + \",\" + str(robot_y)]\n            action_index = 0\n            max_q = q_values[0]\n            for i in range(1, 4):\n                if q_values[i] > max_q:\n                    max_q = q_values[i]\n                    action_index = i\n\n        # Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        # Check if move is valid\n        if maze[new_y][new_x] == 1:\n            reward = -5\n            new_x, new_y = robot_x, robot_y\n        elif (new_x, new_y) == (goal_x, goal_y):\n            reward = 10\n        else:\n            reward = -1\n\n        # Q-value update\n        old_q_value = Q_table[str(robot_x) + \",\" + str(robot_y)][action_index]\n\n        next_q_values = Q_table[str(new_x) + \",\" + str(new_y)]\n        max_future_q = next_q_values[0]\n        for i in range(1, 4):\n            if next_q_values[i] > max_future_q:\n                max_future_q = next_q_values[i]\n\n        Q_table[str(robot_x) + \",\" + str(robot_y)][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n\n        # Move to new position\n        robot_x, robot_y = new_x, new_y\n\n    # Decay epsilon after each episode\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    print(\"Episode:\", episode+1, \"Epsilon:\", round(epsilon, 4))\n","pxt.json":"{\n    \"name\": \"MazeRunner\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1740377161692,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"20\" y=\"20\"></block><block type=\"device_forever\" x=\"225\" y=\"20\"></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\n\n# 5x5 Q-table initialized to 0\nQ_table = {}\nfor y in range(5):\n    for x in range(5):\n        Q_table[str(x) + \",\" + str(y)] = [0, 0, 0, 0]  # [Up, Down, Left, Right]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Q-learning parameters\nalpha = 0.1   # Learning rate\ngamma = 0.9   # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Epsilon decay after each episode\n\ngoal_x, goal_y = 3, 4  # Goal position\n\n# Run multiple training episodes\nfor episode in range(100):\n    robot_x, robot_y = 1, 1  # Reset each episode\n\n    while (robot_x, robot_y) != (goal_x, goal_y):\n        # Exploration or Exploitation\n        if Math.random() < epsilon:\n            action_index = randint(0, 3)  # explore\n        else:\n            # exploit (choose best action manually)\n            q_values = Q_table[str(robot_x) + \",\" + str(robot_y)]\n            action_index = 0\n            max_q = q_values[0]\n            for i in range(1, 4):\n                if q_values[i] > max_q:\n                    max_q = q_values[i]\n                    action_index = i\n\n        # Determine new position\n        new_x = robot_x + move_x[action_index]\n        new_y = robot_y + move_y[action_index]\n\n        # Check if move is valid\n        if maze[new_y][new_x] == 1:\n            reward = -5\n            new_x, new_y = robot_x, robot_y\n        elif (new_x, new_y) == (goal_x, goal_y):\n            reward = 10\n        else:\n            reward = -1\n\n        # Q-value update\n        old_q_value = Q_table[str(robot_x) + \",\" + str(robot_y)][action_index]\n\n        next_q_values = Q_table[str(new_x) + \",\" + str(new_y)]\n        max_future_q = next_q_values[0]\n        for i in range(1, 4):\n            if next_q_values[i] > max_future_q:\n                max_future_q = next_q_values[i]\n\n        Q_table[str(robot_x) + \",\" + str(robot_y)][action_index] += alpha * (reward + gamma * max_future_q - old_q_value)\n\n        # Move to new position\n        robot_x, robot_y = new_x, new_y\n\n    # Decay epsilon after each episode\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n","pxt.json":"{\n    \"name\": \"MazeRunner\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1740699491733,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables></variables><block type=\"pxt-on-start\" x=\"20\" y=\"20\"></block><block type=\"device_forever\" x=\"225\" y=\"20\"></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\n\n\n# Define Q-table as a 5x5 grid where each entry is a list of 4 Q-values: [Up, Down, Left, Right]\nQtable = [\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]],\n    [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]]\n]\n\n# Define possible moves\nactions = [\"Up\", \"Down\", \"Left\", \"Right\"]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 0, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n\n# Agent starting position\nAx_0 = 0\nAy_0 = randint(1, 3)\n\n# Q-learning parameters\nalpha = 0.1  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.98  # Multiply epsilon by this after each episode\n\ngoal_x, goal_y = 3, 4  # Goal position\n\n# Run multiple training episodes\nfor episode in range(100):  # 100 training episodes\n    Ax = Ax_0 \n    Ay = Ay_0  # Reset agent starting position each episode\n\n    while (Ax, Ay) != (goal_x, goal_y):  # Until goal is reached\n        # Exploration vs Exploitation\n        if Math.random() < epsilon:\n            action_index = randint(0, 3)  # Explore (0,1,2,3)\n        else:\n\n            Qindex = Qvalues.index(maxQvalue)\n\n        # Determine new position\n        new_Ax = Ax + move_x[Qindex]\n        new_Ay = Ay + move_y[Qindex]\n\n        # Check if move is valid\n        if maze[new_Ay][new_Ax] == 1:  # Hit a wall\n            reward = -5\n            new_Ax, new_A = Ax, Ay  # Stay in place\n        elif (new_Ax, new_Ay) == (goal_x, goal_y):  # Reached goal\n            reward = 10\n        else:\n            reward = -1  # Normal movement\n\n        # Q-value update\n        old_q_value = Qtable[Ax][Ay][Qindex]\n        #action_index = Qvalues.index(max(Qvalues))  # Choose best action\n        max_future_q = Qtable[0][0][0]\n        for x in Qvalues:\n            if x > maxQvalue:\n                maxQvalue = x\n        Qindex = Qvalues.index(maxQvalue)\n        #max_future_q =     max(Qtable[new_Ay][new_Ax])  # Best future Q-value\n        new_q_value = old_q_value + alpha * (reward + gamma * max_future_q - old_q_value)\n\n        # Update Q-table\n        Qtable[robot_y][robot_x][action_index] = new_q_value  # Store rounded Q-value\n\n        # Move to new position\n        robot_x, robot_y = new_x, new_y\n\n    # Reduce epsilon after each episode (exponential decay)\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    #print(\"Episode:\", episode+1, \"Epsilon:\", round(epsilon, 4))\n    \n    # function to find max Qvalue within a single element of the Qtable\n    def Find_MaxQvalue(x,y):\n        Qvalues = Qtable[x][y]\n        #action_index = Qvalues.index(max(Qvalues))  # Choose best action\n        maxQ = Qvalues[0]\n        for v in Qvalues:\n            if v > maxQvalue:\n                maxQ = v\n        return maxQ","pxt.json":"{\n    \"name\": \"MazeRunner\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1740759239856,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables><variable id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</variable></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\n\n# Define Q-table as a 5x5 grid with a single Q-value per state\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\n\n# Define possible moves\nmove_x = [0, 0, -1, 1]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 1, 1, 1, 1]\n]\n\n# Agent starting position and maze randomization\nAx_0 = 0\nAy_0 = randint(1, 3)\nmaze[Ay_0][Ax_0] = 0  # Open up the maze at the starting position\nmaze[randint(1, 3)][2] = 0  # Open up a random passage in the center wall\ngoal_x = 4  # Goal position\ngoal_y = randint(1, 3)\nmaze[goal_y][goal_x] = 0  # Open up the maze at the goal position\n\n# Q-learning parameters\nalpha = 0.3  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.05  # Minimum exploration rate\nepsilon_decay = 0.97  # Multiply epsilon by this after each episode\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update only the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1: \n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(50)  # Brief delay to make movement visible\n    if (new_x == goal_x and new_y == goal_y):\n        play_ding()\n        for i in range(7):\n            led.toggle(goal_x, goal_y)\n            pause(50)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n\n\n# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears\n\n# Run multiple training episodes\nfor episode in range(10):  # 100 training episodes\n    Ax = Ax_0  # Declare Ax at the start of the episode\n    Ay = Ay_0  # Declare Ay at the start of the episode\n    episode_reward = 0\n    \n    led.plot_brightness(Ax, Ay, 255)\n    pause(10)  # Brief delay to show starting position\n    \n    while (Ax != goal_x or Ay != goal_y):  # Run until goal is reached\n        new_Ax = 0  # Declare new_Ax before use\n        new_Ay = 0  # Declare new_Ay before use\n        reward = 0  # Declare reward before use\n\n        # Exploration vs Exploitation\n        if Math.random() < epsilon:\n            # Random move (no valid move check)\n            i = randint(0, 3)  # Pick a random direction\n            new_Ax = Ax + move_x[i]\n            new_Ay = Ay + move_y[i]\n            # Ensure move stays within bounds\n            if new_Ax < 0 or new_Ax > 4 or new_Ay < 0 or new_Ay > 4:\n                new_Ax = Ax  # Revert to current position if out of bounds\n                new_Ay = Ay\n        else:\n            # Inline greedy selection (only valid moves)\n            best_q = -1000\n            next_x = Ax\n            next_y = Ay\n            for i in range(4):\n                nx = Ax + move_x[i]\n                ny = Ay + move_y[i]\n                if (0 <= nx <= 4 and 0 <= ny <= 4):\n                    q = Qtable[ny][nx]\n                    if q > best_q:\n                        best_q = q\n                        next_x = nx\n                        next_y = ny\n            new_Ax = next_x\n            new_Ay = next_y\n\n        # Determine reward\n        if (new_Ax == goal_x and new_Ay == goal_y):\n            reward = 10\n        elif maze[new_Ay][new_Ax] == 1:\n            reward = -5\n        else:\n            reward = -1\n\n        episode_reward = episode_reward + reward\n\n        # Q-value update\n        old_q_value = Qtable[Ay][Ax]\n        next_q_value = Qtable[new_Ay][new_Ax]\n        new_q_value = old_q_value + alpha * (reward + gamma * next_q_value - old_q_value)\n\n        # Update Q-table\n        Qtable[Ay][Ax] = Math.round(new_q_value*100)/100 #update Qtable element rounded to 100th\n\n        # Update agent position on display\n        update_agent_position(Ax, Ay, new_Ax, new_Ay)\n\n        # Move to new position\n        Ax = new_Ax\n        Ay = new_Ay\n\n    # Reduce epsilon after each episode (exponential decay)\n    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n\n    # Debug episode progress\n    print(\"Episode: \" + str(episode + 1) + \"    Epsilon: \" + str(Math.round(epsilon * 1000) / 1000) + \"    Total Reward: \" + episode_reward)\n\n# Optional: Show completion\nprint(\"Done\")\nfor row in Qtable:\n        print(row)\n\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526,\n            2351,\n            50,\n            50,\n            10,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            1385,\n            2395,\n            50,\n            50,\n            500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.CURVE),\n        music.PlaybackMode.UNTIL_DONE)","pxt.json":"{\n    \"name\": \"MazeRunner(RL)\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1740868884328,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables><variable id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</variable></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\nimport music\n\n# Define Q-table as a 5x5 grid with a single Q-value per state\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\n\n# Define possible moves\nmove_x = [0, 0, -1, 1]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 1, 1, 1, 1]\n]\n\n# Global state for custom LCG\nrand_state = input.running_time_micros() % 65536  # Initial seed from runtime\n\n# Agent starting position and maze randomization\nAx_0 = 0\nAy_0 = custom_randint(1, 3)\nmaze[Ay_0][Ax_0] = 0  # Open up the maze at the starting position\nmaze[custom_randint(1, 3)][2] = 0  # Open up a random passage in the center wall\ngoal_x = 4  # Goal position\ngoal_y = custom_randint(1, 3)\nmaze[goal_y][goal_x] = 0  # Open up the maze at the goal position\n\n# Q-learning parameters\nalpha = 0.3  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.01  # Lower minimum to favor exploitation later\nepsilon_decay = 0.995  # Slower decay for more exploration\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1:\n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(50)  # Brief delay to make movement visible\n    if (new_x == goal_x and new_y == goal_y):\n        play_ding()\n        for i in range(5):\n            led.toggle(goal_x, goal_y)\n            pause(50)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n\n# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears\n\n# Function to get a valid random move (allows walls)\ndef get_valid_random_move(Ax, Ay):\n    attempts = 0\n    i = 0\n    while True:  \n        i = custom_randint(0, 3)\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if (0 <= proposed_x <= 4 and 0 <= proposed_y <= 4):\n            return proposed_x, proposed_y\n        print(i + \": Random move rejected: [\" + str(Ax) + \"][\" + str(Ay) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail\n\n\n# Function to get a valid greedy move (allows walls)\ndef get_valid_greedy_move(Ax, Ay):\n    best_q = -9999\n    next_Ax = 0  # initialize the next agent position\n    next_Ay = 0\n    valid_found = False\n    for i in range(4):\n        proposed_Ax = Ax + move_x[i]\n        proposed_Ay = Ay + move_y[i]\n        if 0 <= proposed_Ax <= 4 and 0 <= proposed_Ay <= 4:\n            q = Qtable[proposed_Ax][proposed_Ay]\n            if q > best_q:\n                best_q = q\n                next_Ax = proposed_Ax\n                next_Ay = proposed_Ay\n                valid_found = True\n    return next_Ax, next_Ay\n\n# Run multiple training episodes on button A\ndef on_button_pressed_a():\n    global epsilon\n    \n    for episode in range(100):  # 100 training episodes\n        Ax = Ax_0\n        Ay = Ay_0\n        episode_reward = 0\n    \n        led.plot_brightness(Ax, Ay, 255)\n        pause(10)\n        print(\"Episode \" + str(episode + 1) + \" Start: \" + str(Ax) + \" \" + str(Ay) + \" Goal: \" + str(goal_x) + \" \" + str(goal_y))\n    \n        while (Ax != goal_x or Ay != goal_y):\n            old_Ax = Ax  # Store current agent position\n            old_Ay = Ay\n            next_Ax = 0  #initialize the next agent position\n            next_Ay = 0\n            if Math.random() < epsilon:\n                next_Ax, next_Ay = get_valid_random_move(Ax, Ay)\n            else:\n                next_Ax, next_Ay = get_valid_greedy_move(Ax, Ay)\n\n            reward = get_reward(next_Ax, next_Ay)\n            episode_reward += reward\n\n            old_q_value = Qtable[old_Ay][old_Ax]\n            next_max_q = max_q_value(next_Ax, next_Ay)\n            new_q_value = old_q_value + alpha * (reward + gamma * next_max_q - old_q_value)\n            Qtable[old_Ay][old_Ax] = Math.round(new_q_value * 100) / 100\n\n            update_agent_position(old_Ax, old_Ay, next_Ax, next_Ay)\n            print(\"Move from: \" + str(old_Ax) + \" \" + str(old_Ay) + \" to: \" + str(next_Ax) + \" \" + str(next_Ay) + \" Reward: \" + str(reward))\n            \n            Ax = next_Ax\n            Ay = next_Ay\n\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n        print(\"Episode: \" + str(episode + 1) + \" Epsilon: \" + str(Math.round(epsilon * 1000) / 1000) + \" Reward: \" + str(episode_reward))\n\n    print(\"Training Complete\")\n    for row in Qtable:\n        print(str(row))\n\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n# On button B run the optimal path\ndef on_button_pressed_b():\n    Ax = Ax_0\n    Ay = Ay_0\n    episode_reward = 0\n    led.plot_brightness(Ax, Ay, 255)\n    pause(10)\n    steps = 0\n    while (Ax != goal_x or Ay != goal_y):\n        old_x = Ax  # Store current position\n        old_y = Ay\n        next_Ax, next_Ay = get_valid_greedy_move(Ax, Ay)\n        reward = get_reward(next_Ax, next_Ay)\n        episode_reward += reward  #count the total reward for the intelligent run\n        update_agent_position(old_x, old_y, next_Ax, next_Ay)\n        pause(100)\n        Ax = next_Ax  \n        Ay = next_Ay\n        steps += 1  #count the steps taken for intelligent run\n    \n    led.toggle(goal_x, goal_y)\n    print(\"Intelligent run complete in \" + str(steps) + \" steps. Reward = \" + str(episode_reward))\n\ninput.on_button_pressed(Button.B, on_button_pressed_b)\n\n# Get reward of the next move\ndef get_reward(new_x, new_y):\n    if (new_x == goal_x and new_y == goal_y):\n        reward = 10\n    elif maze[new_y][new_x] == 1:\n        reward = -5\n    else:\n        reward = -1\n    return reward\n\n# Get maximum Q-value from next state's possible actions\ndef max_q_value(Ax, Ay):\n    max_q = -1000\n    for i in range(4):\n        nx = Ax + move_x[i]\n        ny = Ay + move_y[i]\n        if 0 <= nx <= 4 and 0 <= ny <= 4:\n            q = Qtable[ny][nx]\n            if q > max_q:\n                max_q = q\n    return max_q\n\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526, 2351, 50, 50, 10,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            1385, 2395, 50, 50, 500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.CURVE),\n        music.PlaybackMode.UNTIL_DONE)\n\n# Custom pseudo-random number generator (LCG)\ndef custom_randint(min, max):\n    global rand_state\n    # LCG: X_{n+1} = (a * X_n + c) mod m\n    rand_state = (69069 * rand_state + 1) % 65536\n    # Scale to desired range\n    range_size = max - min + 1\n    return min + (rand_state % range_size)\n","pxt.json":"{\n    \"name\": \"MazeRunner(RL)\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1740949825120,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables><variable id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</variable></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\nimport music\n\n# Define Q-table as a 5x5 grid with a single Q-value per state\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\n\n# Define possible moves\nmove_x = [0, 0, -1, 1]  # Up, Down, Left, Right\nmove_y = [-1, 1, 0, 0]\n\n# Define the maze (1 = wall, 0 = open path)\nmaze = [\n    [1, 1, 1, 1, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 0, 1, 0, 1],\n    [1, 1, 1, 1, 1]\n]\n\n# Global state for custom LCG\nrand_state = input.running_time_micros() % 65536  # Initial seed from runtime\n\n# Agent starting position and maze randomization\nAx_0 = 0\nAy_0 = custom_randint(1, 3)\nmaze[Ay_0][Ax_0] = 0  # Open up the maze at the starting position\nmaze[custom_randint(1, 3)][2] = 0  # Open up a random passage in the center wall\ngoal_x = 4  # Goal position\ngoal_y = custom_randint(1, 3)\nmaze[goal_y][goal_x] = 0  # Open up the maze at the goal position\n\n# Q-learning parameters\nalpha = 0.3  # Learning rate\ngamma = 0.9  # Discount factor\nepsilon = 1.0  # Start with full exploration\nepsilon_min = 0.01  # Lower minimum to favor exploitation later\nepsilon_decay = 0.995  # Slower decay for more exploration\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\n# Function to update the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1:\n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    pause(50)  # Brief delay to make movement visible\n    if (new_x == goal_x and new_y == goal_y):\n        play_ding()\n        for i in range(5):\n            led.toggle(goal_x, goal_y)\n            pause(50)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n\n# Show the maze once at the start\nshow_maze()\npause(500)  # Brief pause to view the maze before agent appears\n\n# Function to get a valid random move (allows walls)\ndef get_valid_random_move(Ax, Ay):\n    attempts = 0\n    i = 0\n    while True:  \n        i = custom_randint(0, 3)\n        proposed_x = Ax + move_x[i]\n        proposed_y = Ay + move_y[i]\n        if (0 <= proposed_x <= 4 and 0 <= proposed_y <= 4):\n            return proposed_x, proposed_y\n        print(i + \": Random move rejected: [\" + str(Ax) + \"][\" + str(Ay) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail\n\n\n# Function to get a valid greedy move (allows walls)\ndef get_valid_greedy_move(Ax, Ay):\n    best_q = -9999\n    next_Ax = 0  # initialize the next agent position\n    next_Ay = 0\n    valid_found = False\n    for i in range(4):\n        proposed_Ax = Ax + move_x[i]\n        proposed_Ay = Ay + move_y[i]\n        if 0 <= proposed_Ax <= 4 and 0 <= proposed_Ay <= 4:\n            q = Qtable[proposed_Ax][proposed_Ay]\n            if q > best_q:\n                best_q = q\n                next_Ax = proposed_Ax\n                next_Ay = proposed_Ay\n                valid_found = True\n    return next_Ax, next_Ay\n\n# Run multiple training episodes on button A\ndef on_button_pressed_a():\n    global epsilon\n    \n    for episode in range(100):  # 100 training episodes\n        Ax = Ax_0\n        Ay = Ay_0\n        episode_reward = 0\n    \n        led.plot_brightness(Ax, Ay, 255)\n        pause(10)\n        print(\"Episode \" + str(episode + 1) + \" Start: \" + str(Ax) + \" \" + str(Ay) + \" Goal: \" + str(goal_x) + \" \" + str(goal_y))\n    \n        while (Ax != goal_x or Ay != goal_y):\n            old_Ax = Ax  # Store current agent position\n            old_Ay = Ay\n            next_Ax = 0  #initialize the next agent position\n            next_Ay = 0\n            if Math.random() < epsilon:\n                next_Ax, next_Ay = get_valid_random_move(Ax, Ay)\n                print(str(next_Ax) + \" \" +str(next_Ay))\n            else:\n                next_Ax, next_Ay = get_valid_greedy_move(Ax, Ay)\n\n            reward = get_reward(next_Ax, next_Ay)\n            episode_reward += reward\n\n            old_q_value = Qtable[old_Ay][old_Ax]\n            next_max_q = max_q_value(next_Ax, next_Ay)\n            new_q_value = old_q_value + alpha * (reward + gamma * next_max_q - old_q_value)\n            Qtable[old_Ay][old_Ax] = Math.round(new_q_value * 100) / 100\n\n            update_agent_position(old_Ax, old_Ay, next_Ax, next_Ay)\n            print(\"Move from: \" + str(old_Ax) + \" \" + str(old_Ay) + \" to: \" + str(next_Ax) + \" \" + str(next_Ay) + \" Reward: \" + str(reward))\n            \n            Ax = next_Ax\n            Ay = next_Ay\n\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)\n        print(\"Episode: \" + str(episode + 1) + \" Epsilon: \" + str(Math.round(epsilon * 1000) / 1000) + \" Reward: \" + str(episode_reward))\n\n    print(\"Training Complete\")\n    for row in Qtable:\n        print(str(row))\n\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n# On button B run the optimal path\ndef on_button_pressed_b():\n    Ax = Ax_0\n    Ay = Ay_0\n    episode_reward = 0\n    led.plot_brightness(Ax, Ay, 255)\n    pause(10)\n    steps = 0\n    while (Ax != goal_x or Ay != goal_y):\n        old_x = Ax  # Store current position\n        old_y = Ay\n        next_Ax, next_Ay = get_valid_greedy_move(Ax, Ay)\n        reward = get_reward(next_Ax, next_Ay)\n        episode_reward += reward  #count the total reward for the intelligent run\n        update_agent_position(old_x, old_y, next_Ax, next_Ay)\n        pause(100)\n        Ax = next_Ax  \n        Ay = next_Ay\n        steps += 1  #count the steps taken for intelligent run\n    \n    led.toggle(goal_x, goal_y)\n    print(\"Intelligent run complete in \" + str(steps) + \" steps. Reward = \" + str(episode_reward))\n\ninput.on_button_pressed(Button.B, on_button_pressed_b)\n\n# Get reward of the next move\ndef get_reward(new_x, new_y):\n    if (new_x == goal_x and new_y == goal_y):\n        reward = 10\n    elif maze[new_y][new_x] == 1:\n        reward = -5\n    else:\n        reward = -1\n    return reward\n\n# Get maximum Q-value from next state's possible actions\ndef max_q_value(Ax, Ay):\n    max_q = -1000\n    for i in range(4):\n        nx = Ax + move_x[i]\n        ny = Ay + move_y[i]\n        if 0 <= nx <= 4 and 0 <= ny <= 4:\n            q = Qtable[ny][nx]\n            if q > max_q:\n                max_q = q\n    return max_q\n\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526, 2351, 50, 50, 10,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            1385, 2395, 50, 50, 500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.CURVE),\n        music.PlaybackMode.UNTIL_DONE)\n\n# Custom pseudo-random number generator (LCG)\ndef custom_randint(min, max):\n    global rand_state\n    # LCG: X_{n+1} = (a * X_n + c) mod m\n    rand_state = (69069 * rand_state + 1) % 65536\n    # Scale to desired range\n    range_size = max - min + 1\n    return min + (rand_state % range_size)\n","pxt.json":"{\n    \"name\": \"MazeRunner(RL)\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1740979706662,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables><variable id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</variable></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\nimport music\n    \n# Q-learning parameters\ntraining_speed = 1   #set to control the speed that the agent moves through the maze during training\nnum_episodes = 20    #set to control the number of training episodes that will run \nalpha = 0.3  # Learning rate\ngamma = 1  # Discount factor\nepsilon_0 = 1.0  # Start with full exploration\nepsilon_min = 0.01  # Lower minimum to favor exploitation later\nepsilon_decay = 0.95  # Slower decay for more exploration\n\n# Settings for showing Qtables during Training\nshow_Qtable_after_each_STEP = True    #start with this True to see how the Qvalues are updated for each step\nshow_Qtable_after_each_EPISODE = False\nshow_Qtable_after_TRAINING = False\n\n# Define the maze on the microbit 5x5 LED array (1 = wall, 0 = open path)\nmaze = [\n    [0, 1, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 1, 0, 0, 0],\n    [0, 1, 0, 1, 0], \n    [0, 0, 0, 1, 0]\n]\n\n#Goal position\ngoal_row = 4  # row of the goal position (0-4)\ngoal_col = 4  # column of the goal position (0-4)\n\n#Agent starting position\nAgent_row_0 = 0  # row of the agent starting position (0-4)\nAgent_col_0 = 0  # column of the agent starting position (0-4)\n\n# Initialize Q-table as a 5x5 grid with a single Q-value for each position in the maze\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\nQtable[goal_row][goal_col] = 10   #set the reward for hitting the goal\n\n# Define possible moves [Up, Down, Left, Right]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Show the maze once at the start\nshow_maze()\npause(1000)  # Brief pause to view the maze before agent appears\nled.plot_brightness(Agent_col_0, Agent_row_0, 255)\n\n# Run multiple training episodes on button A\ndef on_button_pressed_a():\n    epsilon = epsilon_0\n    \n    for episode in range(num_episodes):  # training episodes\n        Agent_row = Agent_row_0\n        Agent_col = Agent_col_0\n        episode_reward = 0\n    \n        led.plot_brightness(Agent_col, Agent_row, 255)\n        pause(100/training_speed)\n        #print(\"Episode \" + str(episode + 1) + \" Start: \" + str(Agent_col) + \" \" + str(Agent_row) + \" Goal: \" + str(goal_col) + \" \" + str(goal_row))\n    \n        while (Agent_col != goal_col or Agent_row != goal_row):\n            next_Agent_col = 0  #initialize the next agent position\n            next_Agent_row = 0\n            \n            if maze[Agent_row][Agent_col] == 1:\n                next_state = [old_Agent_col, old_Agent_row]  #if hitting a wall force return to previous position\n            elif Math.random() < epsilon:\n                next_state = get_valid_random_move(Agent_col, Agent_row)\n            else:\n                next_state = get_valid_greedy_move(Agent_col, Agent_row)\n            next_Agent_col = next_state[0]\n            next_Agent_row = next_state[1]\n            \n            reward = get_reward(next_Agent_col, next_Agent_row) #reward for next step\n            episode_reward += reward   # episode cumulative reward\n\n            old_Agent_col = Agent_col  # Store current agent position\n            old_Agent_row = Agent_row\n\n            #update Qvalue\n            old_q_value = Qtable[old_Agent_row][old_Agent_col]\n            if next_Agent_col == goal_col and next_Agent_row == goal_row:\n                new_q_value = old_q_value + alpha * (reward - old_q_value)  # Terminal state: no future reward\n            else:\n                next_max_q = max_q_value(next_Agent_col, next_Agent_row)\n                new_q_value = old_q_value + alpha * (reward + gamma * next_max_q - old_q_value)\n            Qtable[old_Agent_row][old_Agent_col] = Math.round(new_q_value * 100) / 100\n            if show_Qtable_after_each_STEP == True:\n                print(\"Qtable:\")\n                show_Qtable()\n                pause(2000)\n            update_agent_position(old_Agent_col, old_Agent_row, next_Agent_col, next_Agent_row)        \n            Agent_col = next_Agent_col\n            Agent_row = next_Agent_row\n\n        print(\"Episode: \" + str(episode + 1) + \"   Epsilon= \" + str(Math.round(epsilon * 1000) / 1000) + \"   Reward= \" + str(episode_reward))\n        if show_Qtable_after_each_EPISODE == True:\n            print(\"Qtable:\")\n            show_Qtable()\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)  # decay epsilon for next episode\n        \n    print(\"Training Complete\")\n    if show_Qtable_after_TRAINING == True:\n            print(\"Qtable:\")\n            show_Qtable()\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n# Function to get a valid random move (allows walls)\ndef get_valid_random_move(Agent_col, Agent_row):\n    attempts = 0\n    i = 0\n    next_state = [0,0]\n    while True:\n        i = randint(0, 3)\n        proposed_x = Agent_col + move_x[i]\n        proposed_y = Agent_row + move_y[i]\n        if (0 <= proposed_x <= 4 and 0 <= proposed_y <= 4):\n            next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        #print(i + \": Random move rejected: [\" + str(Agent_col) + \"][\" + str(Agent_row) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail\n\n# Function to get a valid greedy move (allows walls)\ndef get_valid_greedy_move(Agent_col, Agent_row):\n    best_q = -9999\n    next_Agent_col = 0  # initialize the next agent position\n    next_Agent_row = 0\n    valid_found = False\n    next_state = [0,0]\n    for i in range(4):\n        proposed_Agent_col = Agent_col + move_x[i]\n        proposed_Agent_row = Agent_row + move_y[i]\n        if 0 <= proposed_Agent_col <= 4 and 0 <= proposed_Agent_row <= 4:\n            q = Qtable[proposed_Agent_row][proposed_Agent_col]\n            if q > best_q:\n                best_q = q\n                next_state[0] = proposed_Agent_col\n                next_state[1] = proposed_Agent_row\n                valid_found = True\n    return next_state\n\n# On button B run the optimal path\ndef on_button_pressed_b():\n    Agent_col = Agent_col_0\n    Agent_row = Agent_row_0\n    episode_reward = 0\n    led.plot_brightness(Agent_col, Agent_row, 255)\n    pause(500)\n    steps = 0\n    while (Agent_col != goal_col or Agent_row != goal_row):\n        old_x = Agent_col # Store current position\n        old_y = Agent_row\n        next_state = get_valid_greedy_move(Agent_col, Agent_row)\n        next_Agent_col = next_state[0]\n        next_Agent_row = next_state[1]\n        reward = get_reward(next_Agent_col, next_Agent_row)\n        episode_reward += reward  #count the total reward for the intelligent run\n        update_agent_position(old_x, old_y, next_Agent_col, next_Agent_row)\n        pause(500)\n        Agent_col = next_Agent_col\n        Agent_row = next_Agent_row\n        steps += 1  #count the steps taken for intelligent run\n    \n    led.toggle(goal_col, goal_row)\n    print(\"Intelligent run complete in \" + str(steps) + \" steps. Reward = \" + str(episode_reward))\n\ninput.on_button_pressed(Button.B, on_button_pressed_b)\n\n# Function to update the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1:\n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n    pause(500/training_speed)  # Brief delay to make movement visible\n    if (new_x == goal_col and new_y == goal_row):\n        play_ding()\n        for i in range(5):\n            led.toggle(goal_col, goal_row)\n            pause(500/training_speed)\n    \n\n# Get reward of the next move\ndef get_reward(new_x, new_y):\n    if (new_x == goal_col and new_y == goal_row):\n        reward = 10\n    elif maze[new_y][new_x] == 1:\n        reward = -10\n    else:\n        reward = -1\n    return reward\n\n# Get maximum Q-value from next state's possible actions\ndef max_q_value(Agent_col, Agent_row):\n    max_q = -1000\n    for i in range(4):\n        nx = Agent_col + move_x[i]\n        ny = Agent_row + move_y[i]\n        if 0 <= nx <= 4 and 0 <= ny <= 4:\n            q = Qtable[ny][nx]\n            if q > max_q:\n                max_q = q\n    return max_q\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            2469, 2996, 98, 0, 500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LOGARITHMIC),\n        music.PlaybackMode.UNTIL_DONE)\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526, 2351, 50, 50, 10,\n            SoundExpressionEffect.WARBLE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef format_number(num):\n    num_str = str(num)\n    if \".\" not in num_str:\n        num_str += \".\"  # Add \".\" if no decimal\n    parts = num_str.split(\".\")\n    int_part = \"      \" + parts[0] #pad with leading whitespace\n    int_part = int_part[-4:] #limit to 4 places left\n    frac_part = parts[1] + \"00\" # pad with trailing 0s\n    frac_part = frac_part[:2] #limit to 2 decimal places right\n    return int_part + \".\" + frac_part  # recombine number string\n\n# Function to display the Q-table with aligned formatting\ndef show_Qtable():\n    print(\"Q-Table:\")\n    for row in Qtable:\n        row_str = \"[\"\n        for i in range(5):\n            num_str = format_number(row[i])\n            row_str += num_str\n            if i < 4:\n                row_str += \",\"  # Add comma and space\n        row_str += \"]\"\n        print(row_str)\n\ndef on_logo_pressed():\n    global training_speed\n    training_speed = training_speed*2\ninput.on_logo_event(TouchButtonEvent.PRESSED, on_logo_pressed)\n\n","pxt.json":"{\n    \"name\": \"MazeRunner(RL)\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1741050078571,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables><variable id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</variable></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\nimport music\n    \n# Q-learning parameters\ntraining_speed = 1   #set to control the speed that the agent moves through the maze during training\nnum_episodes = 50    #set to control the number of training episodes that will run \nalpha = 0.4  # Learning rate\ngamma = .9  # Discount factor\nepsilon_0 = 1.0  # Start with full exploration\nepsilon_min = 0.01  # Lower minimum to favor exploitation later\nepsilon_decay = 0.95  # Slower decay for more exploration\n\n# Settings for showing Qtables during Training\nshow_Qtable_after_each_STEP = False   #start with this True to see how the Qvalues are updated for each step\nshow_Qtable_after_each_EPISODE = True\nshow_Qtable_after_TRAINING = False\nis_paused = False\n\n# Define the maze on the microbit 5x5 LED array (1 = wall, 0 = open path)\nmaze = [\n    [0, 1, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 1, 0, 0, 0],\n    [0, 1, 0, 1, 0], \n    [0, 0, 0, 1, 0]\n]\n\n#Goal position\ngoal_row = 4  # row of the goal position (0-4)\ngoal_col = 4  # column of the goal position (0-4)\n\n#Agent starting position\nAgent_row_0 = 0  # row of the agent starting position (0-4)\nAgent_col_0 = 0  # column of the agent starting position (0-4)\n\n# Initialize Q-table as a 5x5 grid with a single Q-value for each position in the maze\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\nQtable[goal_row][goal_col] = 10   #set the reward for hitting the goal\n\n# Define possible moves [Up, Down, Left, Right]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Show the maze once at the start\nshow_maze()\npause(1000)  # Brief pause to view the maze before agent appears\nled.plot_brightness(Agent_col_0, Agent_row_0, 255)\n\n# Run multiple training episodes on button A\ndef on_button_pressed_a():\n    epsilon = epsilon_0\n    \n    for episode in range(num_episodes):  # training episodes\n        Agent_row = Agent_row_0\n        Agent_col = Agent_col_0\n        episode_reward = 0\n    \n        led.plot_brightness(Agent_col, Agent_row, 255)\n        pause(100/training_speed)\n        #print(\"Episode \" + str(episode + 1) + \" Start: \" + str(Agent_col) + \" \" + str(Agent_row) + \" Goal: \" + str(goal_col) + \" \" + str(goal_row))\n    \n        while (Agent_col != goal_col or Agent_row != goal_row):\n            next_Agent_col = 0  #initialize the next agent position\n            next_Agent_row = 0\n            \n            if maze[Agent_row][Agent_col] == 1:\n                next_state = [old_Agent_col, old_Agent_row]  #if hitting a wall force return to previous position\n            elif Math.random() < epsilon:\n                next_state = get_valid_random_move(Agent_col, Agent_row)\n            else:\n                next_state = get_valid_greedy_move(Agent_col, Agent_row)\n            next_Agent_col = next_state[0]\n            next_Agent_row = next_state[1]\n            \n            reward = get_reward(next_Agent_col, next_Agent_row) #reward for next step\n            episode_reward += reward   # episode cumulative reward\n\n            old_Agent_col = Agent_col  # Store current agent position\n            old_Agent_row = Agent_row\n\n            #update Qvalue\n            old_q_value = Qtable[old_Agent_row][old_Agent_col]\n            if next_Agent_col == goal_col and next_Agent_row == goal_row:\n                new_q_value = old_q_value + alpha * (reward - old_q_value)  # Terminal state: no future reward\n            else:\n                next_max_q = max_q_value(next_Agent_col, next_Agent_row)\n                new_q_value = old_q_value + alpha * (reward + gamma * next_max_q - old_q_value)\n            Qtable[old_Agent_row][old_Agent_col] = Math.round(new_q_value * 100) / 100\n            if show_Qtable_after_each_STEP == True:\n                show_Qtable()\n                global is_paused\n                is_paused = True\n            while is_paused == True:\n                pause(1000)\n            update_agent_position(old_Agent_col, old_Agent_row, next_Agent_col, next_Agent_row)        \n            Agent_col = next_Agent_col\n            Agent_row = next_Agent_row\n\n        print(\"Episode: \" + str(episode + 1) + \"   Epsilon= \" + str(Math.round(epsilon * 1000) / 1000) + \"   Reward= \" + str(episode_reward))\n        if show_Qtable_after_each_EPISODE == True:\n            show_Qtable()\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)  # decay epsilon for next episode\n        \n    print(\"Training Complete\")\n    if show_Qtable_after_TRAINING == True:\n            show_Qtable()\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n# Function to get a valid random move (allows walls)\ndef get_valid_random_move(Agent_col, Agent_row):\n    attempts = 0\n    i = 0\n    next_state = [0,0]\n    while True:\n        i = randint(0, 3)\n        proposed_x = Agent_col + move_x[i]\n        proposed_y = Agent_row + move_y[i]\n        if (0 <= proposed_x <= 4 and 0 <= proposed_y <= 4):\n            next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        #print(i + \": Random move rejected: [\" + str(Agent_col) + \"][\" + str(Agent_row) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail\n\n# Function to get a valid greedy move (allows walls)\ndef get_valid_greedy_move(Agent_col, Agent_row):\n    best_q = -9999\n    next_Agent_col = 0  # initialize the next agent position\n    next_Agent_row = 0\n    valid_found = False\n    next_state = [0,0]\n    for i in range(4):\n        proposed_Agent_col = Agent_col + move_x[i]\n        proposed_Agent_row = Agent_row + move_y[i]\n        if 0 <= proposed_Agent_col <= 4 and 0 <= proposed_Agent_row <= 4:\n            q = Qtable[proposed_Agent_row][proposed_Agent_col]\n            if q > best_q:\n                best_q = q\n                next_state[0] = proposed_Agent_col\n                next_state[1] = proposed_Agent_row\n                valid_found = True\n    return next_state\n\n# On button B run the optimal path\ndef on_button_pressed_b():\n    Agent_col = Agent_col_0\n    Agent_row = Agent_row_0\n    episode_reward = 0\n    led.plot_brightness(Agent_col, Agent_row, 255)\n    pause(500)\n    steps = 0\n    while (Agent_col != goal_col or Agent_row != goal_row):\n        old_x = Agent_col # Store current position\n        old_y = Agent_row\n        next_state = get_valid_greedy_move(Agent_col, Agent_row)\n        next_Agent_col = next_state[0]\n        next_Agent_row = next_state[1]\n        reward = get_reward(next_Agent_col, next_Agent_row)\n        episode_reward += reward  #count the total reward for the intelligent run\n        update_agent_position(old_x, old_y, next_Agent_col, next_Agent_row)\n        pause(500)\n        Agent_col = next_Agent_col\n        Agent_row = next_Agent_row\n        steps += 1  #count the steps taken for intelligent run\n    \n    led.toggle(goal_col, goal_row)\n    print(\"Intelligent run complete in \" + str(steps) + \" steps. Reward = \" + str(episode_reward))\n\ninput.on_button_pressed(Button.B, on_button_pressed_b)\n\n# Function to update the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1:\n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n    pause(500/training_speed)  # Brief delay to make movement visible\n    if (new_x == goal_col and new_y == goal_row):\n        play_ding()\n        for i in range(5):\n            led.toggle(goal_col, goal_row)\n            pause(500/training_speed)\n    \n\n# Get reward of the next move\ndef get_reward(new_x, new_y):\n    if (new_x == goal_col and new_y == goal_row):\n        reward = 10\n    elif maze[new_y][new_x] == 1:\n        reward = -10\n    else:\n        reward = -1\n    return reward\n\n# Get maximum Q-value from next state's possible actions\ndef max_q_value(Agent_col, Agent_row):\n    max_q = -1000\n    for i in range(4):\n        nx = Agent_col + move_x[i]\n        ny = Agent_row + move_y[i]\n        if 0 <= nx <= 4 and 0 <= ny <= 4:\n            q = Qtable[ny][nx]\n            if q > max_q:\n                max_q = q\n    return max_q\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            2469, 2996, 98, 0, 500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LOGARITHMIC),\n        music.PlaybackMode.UNTIL_DONE)\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526, 2351, 50, 50, 10,\n            SoundExpressionEffect.WARBLE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef format_number(num):\n    num_str = str(num)\n    if \".\" not in num_str:\n        num_str += \".\"  # Add \".\" if no decimal\n    parts = num_str.split(\".\")\n    int_part = \"      \" + parts[0] #pad with leading whitespace\n    int_part = int_part[-4:] #limit to 4 places left\n    frac_part = parts[1] + \"00\" # pad with trailing 0s\n    frac_part = frac_part[:2] #limit to 2 decimal places right\n    return int_part + \".\" + frac_part  # recombine number string\n\n# Function to display the Q-table with aligned formatting\ndef show_Qtable():\n    print(\"Q-Table:\")\n    for row in Qtable:\n        row_str = \"[\"\n        for i in range(5):\n            num_str = format_number(row[i])\n            row_str += num_str\n            if i < 4:\n                row_str += \",\"  # Add comma and space\n        row_str += \"]\"\n        print(row_str)\n\ndef on_logo_pressed():\n    if show_Qtable_after_each_STEP == True:\n        global is_paused\n        is_paused = False          \n    else:\n        global training_speed\n        training_speed = training_speed*2\ninput.on_logo_event(TouchButtonEvent.PRESSED, on_logo_pressed)\n\n","pxt.json":"{\n    \"name\": \"MazeRunner(RL)\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1741065217106,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables><variable id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</variable></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\nimport music\n    \n# Q-learning parameters\ntraining_speed = 1   #set to control the speed that the agent moves through the maze during training\nnum_episodes = 50    #set to control the number of training episodes that will run \nalpha = 0.4  # Learning rate\ngamma = .9  # Discount factor\nepsilon_0 = 1.0  # Start with full exploration\nepsilon_min = 0.01  # Lower minimum to favor exploitation later\nepsilon_decay = 0.95  # Slower decay for more exploration\n\n# Settings for showing Qtables during Training\nshow_Qtable_after_each_STEP = False   #start with this True to see how the Qvalues are updated for each step\nshow_Qtable_after_each_EPISODE = True   \nshow_Qtable_after_TRAINING = False\nis_paused = False\n\n# Define the maze on the microbit 5x5 LED array (1 = wall, 0 = open path)\nmaze = [\n    [0, 1, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 1, 0, 0, 0],\n    [0, 1, 0, 1, 0], \n    [0, 0, 0, 1, 0]\n]\n\n#Goal position\ngoal_row = 4  # row of the goal position (0-4)\ngoal_col = 4  # column of the goal position (0-4)\n\n#Agent starting position\nAgent_row_0 = 0  # row of the agent starting position (0-4)\nAgent_col_0 = 0  # column of the agent starting position (0-4)\n\n# Initialize Q-table as a 5x5 grid with a single Q-value for each position in the maze\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\nQtable[goal_row][goal_col] = 10   #set the reward for hitting the goal\n\n# Define possible moves [Up, Down, Left, Right]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Show the maze once at the start\nshow_maze()\npause(1000)  # Brief pause to view the maze before agent appears\nled.plot_brightness(Agent_col_0, Agent_row_0, 255)\n\n# Run multiple training episodes on button A\ndef on_button_pressed_a():\n    epsilon = epsilon_0\n    \n    for episode in range(num_episodes):  # training episodes\n        Agent_row = Agent_row_0\n        Agent_col = Agent_col_0\n        episode_reward = 0\n    \n        led.plot_brightness(Agent_col, Agent_row, 255)\n        pause(100/training_speed)\n        #print(\"Episode \" + str(episode + 1) + \" Start: \" + str(Agent_col) + \" \" + str(Agent_row) + \" Goal: \" + str(goal_col) + \" \" + str(goal_row))\n    \n        while (Agent_col != goal_col or Agent_row != goal_row):\n            next_Agent_col = 0  #initialize the next agent position\n            next_Agent_row = 0\n            \n            if maze[Agent_row][Agent_col] == 1:\n                next_state = [old_Agent_col, old_Agent_row]  #if hitting a wall force return to previous position\n            elif Math.random() < epsilon:\n                next_state = get_valid_random_move(Agent_col, Agent_row)\n            else:\n                next_state = get_valid_greedy_move(Agent_col, Agent_row)\n            next_Agent_col = next_state[0]\n            next_Agent_row = next_state[1]\n            \n            reward = get_reward(next_Agent_col, next_Agent_row) #reward for next step\n            episode_reward += reward   # episode cumulative reward\n\n            old_Agent_col = Agent_col  # Store current agent position\n            old_Agent_row = Agent_row\n\n            #update Qvalue\n            old_q_value = Qtable[old_Agent_row][old_Agent_col]\n            if next_Agent_col == goal_col and next_Agent_row == goal_row:\n                new_q_value = old_q_value + alpha * (reward - old_q_value)  # Terminal state: no future reward\n            else:\n                next_max_q = max_q_value(next_Agent_col, next_Agent_row)\n                new_q_value = old_q_value + alpha * (reward + gamma * next_max_q - old_q_value)\n            Qtable[old_Agent_row][old_Agent_col] = Math.round(new_q_value * 100) / 100\n            if show_Qtable_after_each_STEP == True:\n                show_Qtable()\n                global is_paused\n                is_paused = True\n            while is_paused == True:\n                pause(1000)\n            update_agent_position(old_Agent_col, old_Agent_row, next_Agent_col, next_Agent_row)        \n            Agent_col = next_Agent_col\n            Agent_row = next_Agent_row\n\n        print(\"Episode: \" + str(episode + 1) + \"   Epsilon= \" + str(Math.round(epsilon * 1000) / 1000) + \"   Reward= \" + str(episode_reward))\n        if show_Qtable_after_each_EPISODE == True:\n            show_Qtable()\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)  # decay epsilon for next episode\n        \n    print(\"Training Complete\")\n    if show_Qtable_after_TRAINING == True:\n            show_Qtable()\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n# Function to get a valid random move (allows walls)\ndef get_valid_random_move(Agent_col, Agent_row):\n    attempts = 0\n    i = 0\n    next_state = [0,0]\n    while True:\n        i = randint(0, 3)\n        proposed_x = Agent_col + move_x[i]\n        proposed_y = Agent_row + move_y[i]\n        if (0 <= proposed_x <= 4 and 0 <= proposed_y <= 4):\n            next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        #print(i + \": Random move rejected: [\" + str(Agent_col) + \"][\" + str(Agent_row) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail\n\n# Function to get a valid greedy move (allows walls)\ndef get_valid_greedy_move(Agent_col, Agent_row):\n    best_q = -9999\n    next_Agent_col = 0  # initialize the next agent position\n    next_Agent_row = 0\n    valid_found = False\n    next_state = [0,0]\n    for i in range(4):\n        proposed_Agent_col = Agent_col + move_x[i]\n        proposed_Agent_row = Agent_row + move_y[i]\n        if 0 <= proposed_Agent_col <= 4 and 0 <= proposed_Agent_row <= 4:\n            q = Qtable[proposed_Agent_row][proposed_Agent_col]\n            if q > best_q:\n                best_q = q\n                next_state[0] = proposed_Agent_col\n                next_state[1] = proposed_Agent_row\n                valid_found = True\n    return next_state\n\n# On button B run the optimal path\ndef on_button_pressed_b():\n    Agent_col = Agent_col_0\n    Agent_row = Agent_row_0\n    episode_reward = 0\n    led.plot_brightness(Agent_col, Agent_row, 255)\n    pause(500)\n    steps = 0\n    while (Agent_col != goal_col or Agent_row != goal_row):\n        old_x = Agent_col # Store current position\n        old_y = Agent_row\n        next_state = get_valid_greedy_move(Agent_col, Agent_row)\n        next_Agent_col = next_state[0]\n        next_Agent_row = next_state[1]\n        reward = get_reward(next_Agent_col, next_Agent_row)\n        episode_reward += reward  #count the total reward for the intelligent run\n        update_agent_position(old_x, old_y, next_Agent_col, next_Agent_row)\n        pause(500)\n        Agent_col = next_Agent_col\n        Agent_row = next_Agent_row\n        steps += 1  #count the steps taken for intelligent run\n    \n    led.toggle(goal_col, goal_row)\n    print(\"Intelligent run complete in \" + str(steps) + \" steps. Reward = \" + str(episode_reward))\n\ninput.on_button_pressed(Button.B, on_button_pressed_b)\n\n# Function to update the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1:\n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n    pause(500/training_speed)  # Brief delay to make movement visible\n    if (new_x == goal_col and new_y == goal_row):\n        play_ding()\n        for i in range(5):\n            led.toggle(goal_col, goal_row)\n            pause(500/training_speed)\n    \n\n# Get reward of the next move\ndef get_reward(new_x, new_y):\n    if (new_x == goal_col and new_y == goal_row):\n        reward = 10\n    elif maze[new_y][new_x] == 1:\n        reward = -10\n    else:\n        reward = -1\n    return reward\n\n# Get maximum Q-value from next state's possible actions\ndef max_q_value(Agent_col, Agent_row):\n    max_q = -1000\n    for i in range(4):\n        nx = Agent_col + move_x[i]\n        ny = Agent_row + move_y[i]\n        if 0 <= nx <= 4 and 0 <= ny <= 4:\n            q = Qtable[ny][nx]\n            if q > max_q:\n                max_q = q\n    return max_q\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            2469, 2996, 98, 0, 500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LOGARITHMIC),\n        music.PlaybackMode.UNTIL_DONE)\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526, 2351, 50, 50, 10,\n            SoundExpressionEffect.WARBLE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef format_number(num):\n    num_str = str(num)\n    if \".\" not in num_str:\n        num_str += \".\"  # Add \".\" if no decimal\n    parts = num_str.split(\".\")\n    int_part = \"      \" + parts[0] #pad with leading whitespace\n    int_part = int_part[-4:] #limit to 4 places left\n    frac_part = parts[1] + \"00\" # pad with trailing 0s\n    frac_part = frac_part[:2] #limit to 2 decimal places right\n    return int_part + \".\" + frac_part  # recombine number string\n\n# Function to display the Q-table with aligned formatting\ndef show_Qtable():\n    print(\"Q-Table:\")\n    for row in Qtable:\n        row_str = \"[\"\n        for i in range(5):\n            num_str = format_number(row[i])\n            row_str += num_str\n            if i < 4:\n                row_str += \",\"  # Add comma and space\n        row_str += \"]\"\n        print(row_str)\n\ndef on_logo_pressed():\n    if show_Qtable_after_each_STEP == True:\n        global is_paused\n        is_paused = False          \n    else:\n        global training_speed\n        training_speed = training_speed*2\ninput.on_logo_event(TouchButtonEvent.PRESSED, on_logo_pressed)\n\n","pxt.json":"{\n    \"name\": \"MazeRunner(RL)\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}},{"timestamp":1741067490021,"editorVersion":"7.0.57","text":{"main.blocks":"<xml xmlns=\"https://developers.google.com/blockly/xml\"><variables><variable id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</variable></variables><block type=\"pxt-on-start\" x=\"0\" y=\"0\"><statement name=\"HANDLER\"><block type=\"variables_set\"><field name=\"VAR\" id=\":hBNFOS=|JRA7sz]q1gu\">Qtable</field><comment pinned=\"false\" h=\"80\" w=\"160\">Define Q-table as a 5x5 grid with a single Q-value per state</comment><value name=\"VALUE\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD1\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD2\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD3\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value><value name=\"ADD4\"><block type=\"lists_create_with\" inline=\"false\"><mutation items=\"5\" horizontalafter=\"3\"></mutation><value name=\"ADD0\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD1\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD2\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD3\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value><value name=\"ADD4\"><shadow type=\"math_number\"><field name=\"NUM\">0</field></shadow></value></block></value></block></value><next><block type=\"device_plot_brightness\"><value name=\"x\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"y\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"4\" precision=\"0\"></mutation><field name=\"SLIDER\">0</field></shadow></value><value name=\"brightness\"><shadow type=\"math_number_minmax\"><mutation min=\"0\" max=\"255\" precision=\"0\"></mutation><field name=\"SLIDER\">255</field></shadow></value></block></next></block></statement></block></xml>","main.ts":"","README.md":"","main.py":"from microbit import *\nimport music\n    \n# Q-learning parameters\ntraining_speed = 1   #set to control the speed that the agent moves through the maze during training\nnum_episodes = 50    #set to control the number of training episodes that will run \nalpha = 0.4  # Learning rate\ngamma = .9  # Discount factor\nepsilon_0 = 1.0  # Start with full exploration\nepsilon_min = 0.01  # Lower minimum to favor exploitation later\nepsilon_decay = 0.95  # Slower decay for more exploration\n\n# Settings for showing Qtables during Training\nshow_Qtable_after_each_STEP = False   #start with this True to see how the Qvalues are updated for each step\nshow_Qtable_after_each_EPISODE = True   \nshow_Qtable_after_TRAINING = False\nis_paused = False\n\n# Define the maze on the microbit 5x5 LED array (1 = wall, 0 = open path)\nmaze = [\n    [0, 1, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 1, 0, 0, 0],\n    [0, 1, 0, 1, 0], \n    [0, 0, 0, 1, 0]\n]\n\n#Goal position\ngoal_row = 4  # row of the goal position (0-4)\ngoal_col = 4  # column of the goal position (0-4)\n\n#Agent starting position\nAgent_row_0 = 0  # row of the agent starting position (0-4)\nAgent_col_0 = 0  # column of the agent starting position (0-4)\n\n# Initialize Q-table as a 5x5 grid with a single Q-value for each position in the maze\nQtable = [\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0],\n    [0, 0, 0, 0, 0]\n]\nQtable[goal_row][goal_col] = 10   #set the reward for hitting the goal\n\n# Define possible moves [Up, Down, Left, Right]\nmove_x = [0, 0, -1, 1]\nmove_y = [-1, 1, 0, 0]\n\n# Show the maze once at the start\nshow_maze()\npause(1000)  # Brief pause to view the maze before agent appears\nled.plot_brightness(Agent_col_0, Agent_row_0, 255)\n\n# Run multiple training episodes on button A\ndef on_button_pressed_a():\n    epsilon = epsilon_0\n    \n    for episode in range(num_episodes):  # training episodes\n        Agent_row = Agent_row_0\n        Agent_col = Agent_col_0\n        episode_reward = 0\n    \n        led.plot_brightness(Agent_col, Agent_row, 255)\n        pause(100/training_speed)\n        #print(\"Episode \" + str(episode + 1) + \" Start: \" + str(Agent_col) + \" \" + str(Agent_row) + \" Goal: \" + str(goal_col) + \" \" + str(goal_row))\n    \n        while (Agent_col != goal_col or Agent_row != goal_row):\n            next_Agent_col = 0  #initialize the next agent position\n            next_Agent_row = 0\n            \n            if maze[Agent_row][Agent_col] == 1:\n                next_state = [old_Agent_col, old_Agent_row]  #if hitting a wall force return to previous position\n            elif Math.random() < epsilon:\n                next_state = get_valid_random_move(Agent_col, Agent_row)\n            else:\n                next_state = get_valid_greedy_move(Agent_col, Agent_row)\n            next_Agent_col = next_state[0]\n            next_Agent_row = next_state[1]\n            \n            reward = get_reward(next_Agent_col, next_Agent_row) #reward for next step\n            episode_reward += reward   # episode cumulative reward\n\n            old_Agent_col = Agent_col  # Store current agent position\n            old_Agent_row = Agent_row\n\n            #update Qvalue\n            old_q_value = Qtable[old_Agent_row][old_Agent_col]\n            if next_Agent_col == goal_col and next_Agent_row == goal_row:\n                new_q_value = old_q_value + alpha * (reward - old_q_value)  # Terminal state: no future reward\n            else:\n                next_max_q = max_q_value(next_Agent_col, next_Agent_row)\n                new_q_value = old_q_value + alpha * (reward + gamma * next_max_q - old_q_value)\n            Qtable[old_Agent_row][old_Agent_col] = Math.round(new_q_value * 100) / 100\n            if show_Qtable_after_each_STEP == True:\n                show_Qtable()\n                global is_paused\n                is_paused = True\n            while is_paused == True:\n                pause(1000)\n            update_agent_position(old_Agent_col, old_Agent_row, next_Agent_col, next_Agent_row)        \n            Agent_col = next_Agent_col\n            Agent_row = next_Agent_row\n\n        print(\"Episode: \" + str(episode + 1) + \"   Epsilon= \" + str(Math.round(epsilon * 1000) / 1000) + \"   Reward= \" + str(episode_reward))\n        if show_Qtable_after_each_EPISODE == True:\n            show_Qtable()\n        epsilon = max(epsilon * epsilon_decay, epsilon_min)  # decay epsilon for next episode\n        \n    print(\"Training Complete\")\n    if show_Qtable_after_TRAINING == True:\n            show_Qtable()\ninput.on_button_pressed(Button.A, on_button_pressed_a)\n\n# Function to get a valid random move (allows walls)\ndef get_valid_random_move(Agent_col, Agent_row):\n    attempts = 0\n    i = 0\n    next_state = [0,0]\n    while True:\n        i = randint(0, 3)\n        proposed_x = Agent_col + move_x[i]\n        proposed_y = Agent_row + move_y[i]\n        if (0 <= proposed_x <= 4 and 0 <= proposed_y <= 4):\n            next_state[0] = proposed_x\n            next_state[1] = proposed_y\n            return next_state\n        #print(i + \": Random move rejected: [\" + str(Agent_col) + \"][\" + str(Agent_row) + \"] to [\" + str(proposed_x) + \"][\" + str(proposed_y) + \"]\")\n        attempts += 1\n    # Fallback: return a valid move if all attempts fail\n\n# Function to get a valid greedy move (allows walls)\ndef get_valid_greedy_move(Agent_col, Agent_row):\n    best_q = -9999\n    next_Agent_col = 0  # initialize the next agent position\n    next_Agent_row = 0\n    valid_found = False\n    next_state = [0,0]\n    for i in range(4):\n        proposed_Agent_col = Agent_col + move_x[i]\n        proposed_Agent_row = Agent_row + move_y[i]\n        if 0 <= proposed_Agent_col <= 4 and 0 <= proposed_Agent_row <= 4:\n            q = Qtable[proposed_Agent_row][proposed_Agent_col]\n            if q > best_q:\n                best_q = q\n                next_state[0] = proposed_Agent_col\n                next_state[1] = proposed_Agent_row\n                valid_found = True\n    return next_state\n\n# On button B run the optimal path\ndef on_button_pressed_b():\n    Agent_col = Agent_col_0\n    Agent_row = Agent_row_0\n    episode_reward = 0\n    led.plot_brightness(Agent_col, Agent_row, 255)\n    pause(500)\n    steps = 0\n    while (Agent_col != goal_col or Agent_row != goal_row):\n        old_x = Agent_col # Store current position\n        old_y = Agent_row\n        next_state = get_valid_greedy_move(Agent_col, Agent_row)\n        next_Agent_col = next_state[0]\n        next_Agent_row = next_state[1]\n        reward = get_reward(next_Agent_col, next_Agent_row)\n        episode_reward += reward  #count the total reward for the intelligent run\n        update_agent_position(old_x, old_y, next_Agent_col, next_Agent_row)\n        pause(500)\n        Agent_col = next_Agent_col\n        Agent_row = next_Agent_row\n        steps += 1  #count the steps taken for intelligent run\n    \n    led.toggle(goal_col, goal_row)\n    print(\"Intelligent run complete in \" + str(steps) + \" steps. Reward = \" + str(episode_reward))\n\ninput.on_button_pressed(Button.B, on_button_pressed_b)\n\n# Function to update the agent's position\ndef update_agent_position(old_x, old_y, new_x, new_y):\n    if maze[old_y][old_x] == 1:\n        led.plot_brightness(old_x, old_y, 5)\n    else:\n        led.unplot(old_x, old_y)\n    led.plot_brightness(new_x, new_y, 255)\n    if (maze[new_y][new_x] == 1):\n        play_noise()\n    pause(500/training_speed)  # Brief delay to make movement visible\n    if (new_x == goal_col and new_y == goal_row):\n        play_ding()\n        for i in range(5):\n            led.toggle(goal_col, goal_row)\n            pause(500/training_speed)\n    \n\n# Get reward of the next move\ndef get_reward(new_x, new_y):\n    if (new_x == goal_col and new_y == goal_row):\n        reward = 10\n    elif maze[new_y][new_x] == 1:\n        reward = -10\n    else:\n        reward = -1\n    return reward\n\n# Get maximum Q-value from next state's possible actions\ndef max_q_value(Agent_col, Agent_row):\n    max_q = -1000\n    for i in range(4):\n        nx = Agent_col + move_x[i]\n        ny = Agent_row + move_y[i]\n        if 0 <= nx <= 4 and 0 <= ny <= 4:\n            q = Qtable[ny][nx]\n            if q > max_q:\n                max_q = q\n    return max_q\n\n# Function to display the static maze (called once)\ndef show_maze():\n    for y in range(5):\n        for x in range(5):\n            if maze[y][x] == 1:\n                led.plot_brightness(x, y, 5)\n\ndef play_ding():\n    music.play(music.create_sound_expression(WaveShape.SINE,\n            2469, 2996, 98, 0, 500,\n            SoundExpressionEffect.NONE,\n            InterpolationCurve.LOGARITHMIC),\n        music.PlaybackMode.UNTIL_DONE)\ndef play_noise():\n    music.play(music.create_sound_expression(WaveShape.NOISE,\n            2526, 2351, 50, 50, 10,\n            SoundExpressionEffect.WARBLE,\n            InterpolationCurve.LINEAR),\n        music.PlaybackMode.UNTIL_DONE)\n\ndef format_number(num):\n    num_str = str(num)\n    if \".\" not in num_str:\n        num_str += \".\"  # Add \".\" if no decimal\n    parts = num_str.split(\".\")\n    int_part = \"      \" + parts[0] #pad with leading whitespace\n    int_part = int_part[-4:] #limit to 4 places left\n    frac_part = parts[1] + \"00\" # pad with trailing 0s\n    frac_part = frac_part[:2] #limit to 2 decimal places right\n    return int_part + \".\" + frac_part  # recombine number string\n\n# Function to display the Q-table with aligned formatting\ndef show_Qtable():\n    print(\"Q-Table:\")\n    for row in Qtable:\n        row_str = \"[\"\n        for i in range(5):\n            num_str = format_number(row[i])\n            row_str += num_str\n            if i < 4:\n                row_str += \",\"  # Add comma and space\n        row_str += \"]\"\n        print(row_str)\n\ndef on_logo_pressed():\n    if show_Qtable_after_each_STEP == True:\n        global is_paused\n        is_paused = False          \n    else:\n        global training_speed\n        training_speed = training_speed*2\ninput.on_logo_event(TouchButtonEvent.PRESSED, on_logo_pressed)\n\n","pxt.json":"{\n    \"name\": \"MazeRunner(RL)\",\n    \"description\": \"\",\n    \"dependencies\": {\n        \"core\": \"*\",\n        \"radio\": \"*\",\n        \"microphone\": \"*\"\n    },\n    \"files\": [\n        \"main.blocks\",\n        \"main.ts\",\n        \"README.md\",\n        \"main.py\"\n    ],\n    \"preferredEditor\": \"pyprj\"\n}\n"}}],"shares":[],"lastSaveTime":1741067490109}